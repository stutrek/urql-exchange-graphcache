{"version":3,"file":"urql-exchange-graphcache.js","sources":["../src/ast/node.ts","../src/helpers/help.ts","../src/store/keys.ts","../src/store/timing.ts","../src/store/data.ts","../src/operations/shared.ts","../src/operations/write.ts","../src/operations/invalidate.ts","../src/store/store.ts","../src/ast/variables.ts","../src/ast/schemaPredicates.ts","../src/ast/traversal.ts","../src/operations/query.ts","../src/cacheExchange.ts","../src/populateExchange.ts"],"sourcesContent":["import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  GraphQLOutputType,\n  Kind,\n  isWrappingType,\n} from 'graphql';\n\nimport { SelectionSet, GraphQLFlatType } from '../types';\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias !== undefined ? node.alias.value : getName(node);\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet =>\n  node.selectionSet !== undefined ? node.selectionSet.selections : [];\n\nexport const getTypeCondition = ({\n  typeCondition,\n}: {\n  typeCondition?: NamedTypeNode;\n}): string | null =>\n  typeCondition !== undefined ? getName(typeCondition) : null;\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n\nexport const unwrapType = (\n  type: null | undefined | GraphQLOutputType\n): GraphQLFlatType | null => {\n  if (isWrappingType(type)) {\n    return unwrapType(type.ofType);\n  }\n\n  return type || null;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/help.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\nimport { ErrorCode } from '../types';\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\nconst helpUrl =\n  '\\nhttps://github.com/FormidableLabs/urql-exchange-graphcache/blob/master/docs/help.md#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import { stringifyVariables } from 'urql/core';\nimport { Variables, FieldInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: null | Variables) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\n/** Prefix key with its owner type Link / Record */\nexport const prefixKey = (owner: 'l' | 'r', key: string) => `${owner}|${key}`;\n","export const defer: (fn: () => void) => void =\n  process.env.NODE_ENV === 'production' && typeof Promise !== 'undefined'\n    ? Promise.prototype.then.bind(Promise.resolve())\n    : fn => setTimeout(fn, 0);\n","import {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n} from '../types';\nimport { invariant, currentDebugStack } from '../helpers/help';\nimport { fieldInfoOfKey, joinKeys, prefixKey } from './keys';\nimport { defer } from './timing';\n\nimport { observable } from 'mobx';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OptimisticMap<T> = Record<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OptimisticMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n  keys: number[];\n}\n\nexport interface InMemoryData {\n  persistenceScheduled: boolean;\n  persistenceBatch: SerializedEntries;\n  gcScheduled: boolean;\n  gcBatch: Set<string>;\n  queryRootKey: string;\n  refCount: Dict<number>;\n  refLock: OptimisticMap<Dict<number>>;\n  records: NodeMap<EntityField>;\n  links: NodeMap<Link>;\n  storage: StorageAdapter | null;\n}\n\nexport const makeDict = (): any => observable({});\n\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Set<string> = null;\nlet currentOptimisticKey: null | number = null;\n\nconst makeNodeMap = <T>(): NodeMap<T> => ({\n  optimistic: makeDict(),\n  base: new Map(),\n  keys: [],\n});\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  data: InMemoryData,\n  optimisticKey: number | null\n) => {\n  //@ts-ignore\n  window.currentData = currentData = data;\n  currentDependencies = new Set();\n  currentOptimisticKey = optimisticKey;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  const data = currentData!;\n\n  if (!data.gcScheduled && data.gcBatch.size > 0) {\n    data.gcScheduled = true;\n    defer(() => {\n      gc(data);\n    });\n  }\n\n  if (data.storage && !data.persistenceScheduled) {\n    data.persistenceScheduled = true;\n    defer(() => {\n      data.storage!.write(data.persistenceBatch);\n      data.persistenceScheduled = false;\n      data.persistenceBatch = makeDict();\n    });\n  }\n\n  currentData = null;\n  currentDependencies = null;\n  currentOptimisticKey = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Set<string> => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  persistenceScheduled: false,\n  persistenceBatch: makeDict(),\n  gcScheduled: false,\n  queryRootKey,\n  gcBatch: new Set(),\n  refCount: makeDict(),\n  refLock: makeDict(),\n  links: makeNodeMap(),\n  records: makeNodeMap(),\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  let keymap: KeyMap<Dict<T | undefined>>;\n  if (currentOptimisticKey) {\n    // If the optimistic map doesn't exist yet, it' created, and\n    // the optimistic key is stored (in order of priority)\n    if (map.optimistic[currentOptimisticKey] === undefined) {\n      map.optimistic[currentOptimisticKey] = new Map();\n      map.keys.unshift(currentOptimisticKey);\n    }\n\n    keymap = map.optimistic[currentOptimisticKey];\n  } else {\n    keymap = map.base;\n  }\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    const node = optimistic.get(entityKey);\n    // If the node and node value exists it is returned, including undefined\n    if (node !== undefined && fieldKey in node) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  const node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNodeParent = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): Record<string, T | undefined> | undefined => {\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    const node = optimistic.get(entityKey);\n    // If the node and node value exists it is returned, including undefined\n    if (node !== undefined && fieldKey in node) {\n      return node;\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  const node = map.base.get(entityKey);\n  return node !== undefined ? node : undefined;\n};\n\n/** Clears an optimistic layers from a NodeMap */\nconst clearOptimisticNodes = <T>(map: NodeMap<T>, optimisticKey: number) => {\n  // Check whether the optimistic layer exists on the NodeMap\n  const index = map.keys.indexOf(optimisticKey);\n  if (index > -1) {\n    // Then delete it and splice out the optimisticKey\n    delete map.optimistic[optimisticKey];\n    map.keys.splice(index, 1);\n  }\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gcBatch */\nconst updateRCForEntity = (\n  gcBatch: void | Set<string>,\n  refCount: Dict<number>,\n  entityKey: string,\n  by: number\n) => {\n  // Retrieve the reference count\n  const count = refCount[entityKey] !== undefined ? refCount[entityKey] : 0;\n  // Adjust it by the \"by\" value\n  const newCount = (refCount[entityKey] = (count + by) | 0);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gcBatch !== undefined) {\n    if (newCount <= 0) gcBatch.add(entityKey);\n    else if (count <= 0 && newCount > 0) gcBatch.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gcBatch */\nconst updateRCForLink = (\n  gcBatch: void | Set<string>,\n  refCount: Dict<number>,\n  link: Link | undefined,\n  by: number\n) => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gcBatch, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      const entityKey = link[i];\n      if (entityKey) {\n        updateRCForEntity(gcBatch, refCount, entityKey, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n) => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = (data: InMemoryData) => {\n  // Reset gcScheduled flag\n  data.gcScheduled = false;\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  data.gcBatch.forEach(entityKey => {\n    // Check first whether the reference count is still 0\n    const rc = data.refCount[entityKey] || 0;\n    if (rc <= 0) {\n      // Each optimistic layer may also still contain some references to marked entities\n      for (const optimisticKey in data.refLock) {\n        const refCount = data.refLock[optimisticKey];\n        const locks = refCount[entityKey] || 0;\n        // If the optimistic layer has any references to the entity, don't GC it,\n        // otherwise delete the reference count from the optimistic layer\n        if (locks > 0) return;\n        delete refCount[entityKey];\n      }\n\n      // All conditions are met: The entity can be deleted\n\n      // Delete the reference count, and delete the entity from the GC batch\n      delete data.refCount[entityKey];\n      data.gcBatch.delete(entityKey);\n\n      // Delete the record and for each of its fields, delete them on the persistence\n      // layer if one is present\n      // No optimistic data needs to be deleted, as the entity is not being referenced by\n      // anything and optimistic layers will eventually be deleted anyway\n      const recordsNode = data.records.base.get(entityKey);\n      if (recordsNode !== undefined) {\n        data.records.base.delete(entityKey);\n        if (data.storage) {\n          for (const fieldKey in recordsNode) {\n            const key = prefixKey('r', joinKeys(entityKey, fieldKey));\n            data.persistenceBatch[key] = undefined;\n          }\n        }\n      }\n\n      // Delete all the entity's links, but also update the reference count\n      // for those links (which can lead to an unrolled recursive GC of the children)\n      const linkNode = data.links.base.get(entityKey);\n      if (linkNode !== undefined) {\n        data.links.base.delete(entityKey);\n        for (const fieldKey in linkNode) {\n          // Delete all links from the persistence layer if one is present\n          if (data.storage) {\n            const key = prefixKey('l', joinKeys(entityKey, fieldKey));\n            data.persistenceBatch[key] = undefined;\n          }\n\n          updateRCForLink(data.gcBatch, data.refCount, linkNode[fieldKey], -1);\n        }\n      }\n    } else {\n      data.gcBatch.delete(entityKey);\n    }\n  });\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies!.add(entityKey);\n    } else if (fieldKey !== undefined) {\n      currentDependencies!.add(joinKeys(entityKey, fieldKey));\n    }\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\nexport const readParent = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNodeParent(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n  if (currentData!.storage && !currentOptimisticKey) {\n    const key = prefixKey('r', joinKeys(entityKey, fieldKey));\n    currentData!.persistenceBatch[key] = value;\n  }\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: Dict<number>;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gcBatch: void | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount =\n      data.refLock[currentOptimisticKey] ||\n      (data.refLock[currentOptimisticKey] = makeDict());\n    links = data.links.optimistic[currentOptimisticKey];\n  } else {\n    if (data.storage) {\n      const key = prefixKey('l', joinKeys(entityKey, fieldKey));\n      data.persistenceBatch[key] = link;\n    }\n    refCount = data.refCount;\n    links = data.links.base;\n    gcBatch = data.gcBatch;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links !== undefined ? links.get(entityKey) : undefined;\n  const prevLink = prevLinkNode !== undefined ? prevLinkNode[fieldKey] : null;\n\n  // Update dependencies\n  updateDependencies(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gcBatch, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gcBatch, refCount, link, 1);\n};\n\n/** Removes an optimistic layer of links and records */\nexport const clearOptimistic = (data: InMemoryData, optimisticKey: number) => {\n  // We also delete the optimistic reference locks\n  delete data.refLock[optimisticKey];\n  clearOptimisticNodes(data.records, optimisticKey);\n  clearOptimisticNodes(data.links, optimisticKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState(data, 0);\n  for (const key in entries) {\n    const dotIndex = key.indexOf('.');\n    const entityKey = key.slice(2, dotIndex);\n    const fieldKey = key.slice(dotIndex + 1);\n    switch (key.charCodeAt(0)) {\n      case 108:\n        writeLink(entityKey, fieldKey, entries[key] as Link);\n        break;\n      case 114:\n        writeRecord(entityKey, fieldKey, entries[key]);\n        break;\n    }\n  }\n  clearDataState();\n  data.storage = storage;\n};\n","import { FieldNode, InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport { warn, pushDebugNode } from '../helpers/help';\nimport { hasField } from '../store/data';\nimport { Store, keyOfField } from '../store';\n\nimport {\n  Fragments,\n  Variables,\n  SelectionSet,\n  DataField,\n  NullArray,\n  Data,\n} from '../types';\n\nimport {\n  SchemaPredicates,\n  getTypeCondition,\n  getFieldArguments,\n  shouldInclude,\n  isFieldNode,\n  isInlineFragment,\n  getSelectionSet,\n  getName,\n} from '../ast';\n\ninterface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  ctx: Context\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return !getSelectionSet(node).some(node => {\n    if (!isFieldNode(node)) return false;\n    const fieldKey = keyOfField(\n      getName(node),\n      getFieldArguments(node, ctx.variables)\n    );\n    return !hasField(entityKey, fieldKey);\n  });\n};\n\nexport class SelectionIterator {\n  typename: void | string;\n  entityKey: string;\n  indexStack: number[];\n  context: Context;\n  selectionStack: SelectionSet[];\n\n  constructor(\n    typename: void | string,\n    entityKey: string,\n    select: SelectionSet,\n    ctx: Context\n  ) {\n    this.typename = typename;\n    this.entityKey = entityKey;\n    this.context = ctx;\n    this.indexStack = [0];\n    this.selectionStack = [select];\n  }\n\n  next(): void | FieldNode {\n    while (this.indexStack.length !== 0) {\n      const index = this.indexStack[this.indexStack.length - 1]++;\n      const select = this.selectionStack[this.selectionStack.length - 1];\n      if (index >= select.length) {\n        this.indexStack.pop();\n        this.selectionStack.pop();\n        continue;\n      } else {\n        const node = select[index];\n        if (!shouldInclude(node, this.context.variables)) {\n          continue;\n        } else if (!isFieldNode(node)) {\n          // A fragment is either referred to by FragmentSpread or inline\n          const fragmentNode = !isInlineFragment(node)\n            ? this.context.fragments[getName(node)]\n            : node;\n\n          if (fragmentNode !== undefined) {\n            if (process.env.NODE_ENV !== 'production') {\n              pushDebugNode(this.typename, fragmentNode);\n            }\n\n            const isMatching =\n              this.context.schemaPredicates !== undefined\n                ? this.context.schemaPredicates.isInterfaceOfType(\n                    getTypeCondition(fragmentNode),\n                    this.typename\n                  )\n                : isFragmentHeuristicallyMatching(\n                    fragmentNode,\n                    this.typename,\n                    this.entityKey,\n                    this.context\n                  );\n\n            if (isMatching) {\n              this.indexStack.push(0);\n              this.selectionStack.push(getSelectionSet(fragmentNode));\n            }\n          }\n\n          continue;\n        } else if (getName(node) === '__typename') {\n          continue;\n        } else {\n          return node;\n        }\n      }\n    }\n\n    return undefined;\n  }\n}\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x === undefined ? null : (x as Data | NullArray<Data>);\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\n\nimport {\n  getFieldAlias,\n  getFragments,\n  getMainOperation,\n  getSelectionSet,\n  normalizeVariables,\n  getFragmentTypeName,\n  getName,\n  getFieldArguments,\n  SchemaPredicates,\n} from '../ast';\n\nimport {\n  NullArray,\n  Fragments,\n  Variables,\n  Data,\n  Link,\n  SelectionSet,\n  OperationRequest,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  makeDict,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { invariant, warn, pushDebugNode } from '../helpers/help';\nimport { SelectionIterator, ensureData } from './shared';\n\nexport interface WriteResult {\n  dependencies: Set<string>;\n}\n\ninterface Context {\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  fieldName: string;\n  result: WriteResult;\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  optimistic?: boolean;\n  schemaPredicates?: SchemaPredicates;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data\n): WriteResult => {\n  initDataState(store.data, 0);\n  const result = startWrite(store, request, data);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { dependencies: getCurrentDependencies() };\n\n  const select = getSelectionSet(operation);\n  const operationName = store.getRootKey(operation.operation);\n\n  const ctx: Context = {\n    parentTypeName: operationName,\n    parentKey: operationName,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    result,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  if (operationName === ctx.store.getRootKey('query')) {\n    writeSelection(ctx, operationName, select, data);\n  } else {\n    writeRoot(ctx, operationName, select, data);\n  }\n\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  optimisticKey: number\n): WriteResult => {\n  initDataState(store.data, optimisticKey);\n\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { dependencies: getCurrentDependencies() };\n\n  const mutationRootKey = store.getRootKey('mutation');\n  const operationName = store.getRootKey(operation.operation);\n  invariant(\n    operationName === mutationRootKey,\n    'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n      'This case is unsupported and should never occur.',\n    10\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  const ctx: Context = {\n    parentTypeName: mutationRootKey,\n    parentKey: mutationRootKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    result,\n    store,\n    schemaPredicates: store.schemaPredicates,\n    optimistic: true,\n  };\n\n  const data = makeDict();\n  const iter = new SelectionIterator(\n    operationName,\n    operationName,\n    getSelectionSet(operation),\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    if (node.selectionSet !== undefined) {\n      const fieldName = getName(node);\n      const resolver = ctx.store.optimisticMutations[fieldName];\n\n      if (resolver !== undefined) {\n        // We have to update the context to reflect up-to-date ResolveInfo\n        ctx.fieldName = fieldName;\n\n        const fieldArgs = getFieldArguments(node, ctx.variables);\n        const resolverValue = resolver(fieldArgs || makeDict(), ctx.store, ctx);\n        const resolverData = ensureData(resolverValue);\n        writeRootField(ctx, resolverData, getSelectionSet(node));\n        data[fieldName] = resolverValue;\n        const updater = ctx.store.updates[mutationRootKey][fieldName];\n        if (updater !== undefined) {\n          updater(data, fieldArgs || makeDict(), ctx.store, ctx);\n        }\n      }\n    }\n  }\n\n  clearDataState();\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Data,\n  variables?: Variables\n) => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    return warn(\n      'writeFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      11\n    );\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const writeData = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(writeData);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx: Context = {\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: variables || {},\n    fragments,\n    result: { dependencies: getCurrentDependencies() },\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), writeData);\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.getRootKey('query');\n  const typename = isQuery ? entityKey : data.__typename;\n  if (typeof typename !== 'string') return;\n\n  InMemoryData.writeRecord(entityKey, '__typename', typename);\n\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldValue = data[getFieldAlias(node)];\n    const key = joinKeys(entityKey, fieldKey);\n\n    if (process.env.NODE_ENV !== 'production') {\n      if (fieldValue === undefined) {\n        const advice = ctx.optimistic\n          ? '\\nYour optimistic result may be missing a field!'\n          : '';\n\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.' +\n            advice,\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.schemaPredicates && typename) {\n        ctx.schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n      }\n    }\n\n    if (node.selectionSet === undefined) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(entityKey, fieldKey, fieldValue);\n    } else {\n      // Process the field and write links for the child entities that have been written\n      const fieldData = ensureData(fieldValue);\n      const link = writeField(ctx, key, getSelectionSet(node), fieldData);\n      InMemoryData.writeLink(entityKey, fieldKey, link);\n    }\n  }\n};\n\nconst writeField = (\n  ctx: Context,\n  parentFieldKey: string,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>\n): Link => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      const item = data[i];\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = joinKeys(parentFieldKey, `${i}`);\n      // Recursively write array data\n      const links = writeField(ctx, indexKey, select, item);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n    }\n\n    return newData;\n  } else if (data === null) {\n    return null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const key = entityKey !== null ? entityKey : parentFieldKey;\n  const typename = data.__typename;\n\n  if (\n    ctx.store.keys[data.__typename] === undefined &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !typename.endsWith('Connection') &&\n    !typename.endsWith('Edge') &&\n    typename !== 'PageInfo'\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  writeSelection(ctx, key, select, data);\n  return key;\n};\n\n// This is like writeSelection but assumes no parent entity exists\nconst writeRoot = (\n  ctx: Context,\n  typename: string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isRootField =\n    typename === ctx.store.getRootKey('mutation') ||\n    typename === ctx.store.getRootKey('subscription');\n\n  const iter = new SelectionIterator(typename, typename, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = joinKeys(typename, keyOfField(fieldName, fieldArgs));\n    if (node.selectionSet !== undefined) {\n      const fieldValue = ensureData(data[getFieldAlias(node)]);\n      writeRootField(ctx, fieldValue, getSelectionSet(node));\n    }\n\n    if (isRootField) {\n      // We have to update the context to reflect up-to-date ResolveInfo\n      ctx.parentTypeName = typename;\n      ctx.parentKey = typename;\n      ctx.parentFieldKey = fieldKey;\n      ctx.fieldName = fieldName;\n\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater !== undefined) {\n        updater(data, fieldArgs || makeDict(), ctx.store, ctx);\n      }\n    }\n  }\n};\n\n// This is like writeField but doesn't fall back to a generated key\nconst writeRootField = (\n  ctx: Context,\n  data: null | Data | NullArray<Data>,\n  select: SelectionSet\n) => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++)\n      newData[i] = writeRootField(ctx, data[i], select);\n    return newData;\n  } else if (data === null) {\n    return;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(data);\n  if (entityKey !== null) {\n    writeSelection(ctx, entityKey, select, data);\n  } else {\n    const typename = data.__typename;\n    writeRoot(ctx, typename, select, data);\n  }\n};\n","import { FieldNode } from 'graphql';\n\nimport {\n  getMainOperation,\n  normalizeVariables,\n  getFragments,\n  getSelectionSet,\n  getName,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  EntityField,\n  OperationRequest,\n  Variables,\n  Fragments,\n  SelectionSet,\n} from '../types';\n\nimport * as InMemoryData from '../store/data';\nimport { Store, keyOfField } from '../store';\nimport { SchemaPredicates } from '../ast';\nimport { SelectionIterator } from './shared';\n\ninterface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nexport const invalidate = (store: Store, request: OperationRequest) => {\n  const operation = getMainOperation(request.query);\n\n  const ctx: Context = {\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  invalidateSelection(\n    ctx,\n    ctx.store.getRootKey('query'),\n    getSelectionSet(operation)\n  );\n};\n\nexport const invalidateSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet\n) => {\n  const isQuery = entityKey === 'Query';\n\n  let typename: EntityField;\n  if (!isQuery) {\n    typename = InMemoryData.readRecord(entityKey, '__typename');\n    if (typeof typename !== 'string') {\n      return;\n    } else {\n      InMemoryData.writeRecord(entityKey, '__typename', undefined);\n    }\n  } else {\n    typename = entityKey;\n  }\n\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldKey = keyOfField(\n      fieldName,\n      getFieldArguments(node, ctx.variables)\n    );\n\n    if (\n      process.env.NODE_ENV !== 'production' &&\n      ctx.schemaPredicates &&\n      typename\n    ) {\n      ctx.schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    if (node.selectionSet === undefined) {\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n    } else {\n      const fieldSelect = getSelectionSet(node);\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      InMemoryData.writeLink(entityKey, fieldKey, undefined);\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n\n      if (Array.isArray(link)) {\n        for (let i = 0, l = link.length; i < l; i++) {\n          const childLink = link[i];\n          if (childLink !== null) {\n            invalidateSelection(ctx, childLink, fieldSelect);\n          }\n        }\n      } else if (link) {\n        invalidateSelection(ctx, link, fieldSelect);\n      }\n    }\n  }\n};\n","import { DocumentNode } from 'graphql';\nimport { createRequest } from 'urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  Data,\n  QueryInput,\n  UpdatesConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n} from '../types';\n\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidate } from '../operations/invalidate';\nimport { SchemaPredicates } from '../ast';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport class Store implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: UpdatesConfig;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schemaPredicates?: SchemaPredicates;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(\n    schemaPredicates?: SchemaPredicates,\n    resolvers?: ResolverConfig,\n    updates?: Partial<UpdatesConfig>,\n    optimisticMutations?: OptimisticMutationConfig,\n    keys?: KeyingConfig\n  ) {\n    this.resolvers = resolvers || {};\n    this.optimisticMutations = optimisticMutations || {};\n    this.keys = keys || {};\n    this.schemaPredicates = schemaPredicates;\n\n    this.updates = {\n      Mutation: (updates && updates.Mutation) || {},\n      Subscription: (updates && updates.Subscription) || {},\n    } as UpdatesConfig;\n\n    if (schemaPredicates) {\n      const { schema } = schemaPredicates;\n      const queryType = schema.getQueryType();\n      const mutationType = schema.getMutationType();\n      const subscriptionType = schema.getSubscriptionType();\n\n      const queryName = queryType ? queryType.name : 'Query';\n      const mutationName = mutationType ? mutationType.name : 'Mutation';\n      const subscriptionName = subscriptionType\n        ? subscriptionType.name\n        : 'Subscription';\n\n      this.rootFields = {\n        query: queryName,\n        mutation: mutationName,\n        subscription: subscriptionName,\n      };\n\n      this.rootNames = {\n        [queryName]: 'query',\n        [mutationName]: 'mutation',\n        [subscriptionName]: 'subscription',\n      };\n    } else {\n      this.rootFields = {\n        query: 'Query',\n        mutation: 'Mutation',\n        subscription: 'Subscription',\n      };\n\n      this.rootNames = {\n        Query: 'query',\n        Mutation: 'mutation',\n        Subscription: 'subscription',\n      };\n    }\n\n    this.data = InMemoryData.make(this.getRootKey('query'));\n  }\n\n  gcScheduled = false;\n  gc = () => {\n    InMemoryData.gc(this.data);\n    this.gcScheduled = false;\n  };\n\n  keyOfField = keyOfField;\n\n  getRootKey(name: RootField) {\n    return this.rootFields[name];\n  }\n\n  keyOfEntity(data: Data) {\n    const { __typename: typename, id, _id } = data;\n    if (!typename) {\n      return null;\n    } else if (this.rootNames[typename] !== undefined) {\n      return typename;\n    }\n\n    let key: string | null | void;\n    if (this.keys[typename]) {\n      key = this.keys[typename](data);\n    } else if (id !== undefined && id !== null) {\n      key = `${id}`;\n    } else if (_id !== undefined && _id !== null) {\n      key = `${_id}`;\n    }\n\n    return key ? `${typename}:${key}` : null;\n  }\n\n  resolveFieldByKey(entity: Data | string | null, fieldKey: string): DataField {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n    if (entityKey === null) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link ? link : null;\n  }\n\n  resolve(\n    entity: Data | string | null,\n    field: string,\n    args?: Variables\n  ): DataField {\n    return this.resolveFieldByKey(entity, keyOfField(field, args));\n  }\n\n  invalidateQuery(query: string | DocumentNode, variables?: Variables) {\n    invalidate(this, createRequest(query, variables));\n  }\n\n  inspectFields(entity: Data | string | null): FieldInfo[] {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n    return entityKey !== null ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery(\n    input: QueryInput,\n    updater: (data: Data | null) => Data | null\n  ): void {\n    const request = createRequest(input.query, input.variables);\n    const output = updater(this.readQuery(request as QueryInput));\n    if (output !== null) {\n      startWrite(this, request, output);\n    }\n  }\n\n  readQuery(input: QueryInput): Data | null {\n    return read(this, createRequest(input.query, input.variables)).data;\n  }\n\n  readFragment(\n    dataFragment: DocumentNode,\n    entity: string | Data,\n    variables?: Variables\n  ): Data | null {\n    return readFragment(this, dataFragment, entity, variables);\n  }\n\n  writeFragment(\n    dataFragment: DocumentNode,\n    data: Data,\n    variables?: Variables\n  ): void {\n    writeFragment(this, dataFragment, data, variables);\n  }\n}\n","import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\nimport { makeDict } from '../store';\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  if (node.arguments === undefined || node.arguments.length === 0) {\n    return null;\n  }\n\n  const args = makeDict();\n  let argsSize = 0;\n\n  for (let i = 0, l = node.arguments.length; i < l; i++) {\n    const arg = node.arguments[i];\n    const value = valueFromASTUntyped(arg.value, vars);\n    if (value !== undefined && value !== null) {\n      args[getName(arg)] = value;\n      argsSize++;\n    }\n  }\n\n  return argsSize > 0 ? args : null;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n): Variables => {\n  if (node.variableDefinitions === undefined) {\n    return {};\n  }\n\n  const args: Variables = (input as Variables) || {};\n\n  return node.variableDefinitions.reduce((vars, def) => {\n    const name = getName(def.variable);\n    let value = args[name];\n    if (value === undefined) {\n      if (def.defaultValue !== undefined) {\n        value = valueFromASTUntyped(def.defaultValue, args);\n      } else {\n        return vars;\n      }\n    }\n\n    vars[name] = value;\n    return vars;\n  }, makeDict());\n};\n","import {\n  buildClientSchema,\n  isNullableType,\n  isListType,\n  isNonNullType,\n  GraphQLSchema,\n  GraphQLAbstractType,\n  GraphQLObjectType,\n  GraphQLInterfaceType,\n  GraphQLUnionType,\n} from 'graphql';\n\nimport { invariant, warn } from '../helpers/help';\n\nexport class SchemaPredicates {\n  schema: GraphQLSchema;\n\n  constructor(schema: object) {\n    this.schema = buildClientSchema(schema as any);\n  }\n\n  isFieldNullable(typename: string, fieldName: string): boolean {\n    const field = getField(this.schema, typename, fieldName);\n    if (field === undefined) return false;\n    return isNullableType(field.type);\n  }\n\n  isListNullable(typename: string, fieldName: string): boolean {\n    const field = getField(this.schema, typename, fieldName);\n    if (field === undefined) return false;\n    const ofType = isNonNullType(field.type) ? field.type.ofType : field.type;\n    return isListType(ofType) && isNullableType(ofType.ofType);\n  }\n\n  isFieldAvailableOnType(typename: string, fieldname: string): boolean {\n    return !!getField(this.schema, typename, fieldname);\n  }\n\n  isInterfaceOfType(\n    typeCondition: null | string,\n    typename: string | void\n  ): boolean {\n    if (!typename || !typeCondition) return false;\n    if (typename === typeCondition) return true;\n\n    const abstractType = this.schema.getType(typeCondition);\n    const objectType = this.schema.getType(typename);\n\n    if (abstractType instanceof GraphQLObjectType) {\n      return abstractType === objectType;\n    }\n\n    expectAbstractType(abstractType, typeCondition);\n    expectObjectType(objectType, typename);\n    return this.schema.isPossibleType(abstractType, objectType);\n  }\n}\n\nconst getField = (\n  schema: GraphQLSchema,\n  typename: string,\n  fieldName: string\n) => {\n  const object = schema.getType(typename);\n  expectObjectType(object, typename);\n\n  const field = object.getFields()[fieldName];\n  if (field === undefined) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n\n    return undefined;\n  }\n\n  return field;\n};\n\nfunction expectObjectType(x: any, typename: string): asserts x is GraphQLObjectType {\n  invariant(\n    x instanceof GraphQLObjectType,\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(x: any, typename: string): asserts x is GraphQLAbstractType {\n  invariant(\n    x instanceof GraphQLInterfaceType || x instanceof GraphQLUnionType,\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n","import {\n  SelectionNode,\n  DefinitionNode,\n  DocumentNode,\n  FragmentDefinitionNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { invariant } from '../helpers/help';\nimport { getName } from './node';\nimport { Fragments, Variables } from '../types';\n\nconst isFragmentNode = (node: DefinitionNode): node is FragmentDefinitionNode =>\n  node.kind === Kind.FRAGMENT_DEFINITION;\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  const operation = doc.definitions.find(\n    node => node.kind === Kind.OPERATION_DEFINITION\n  ) as OperationDefinitionNode;\n\n  invariant(\n    !!operation,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n\n  return operation;\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments =>\n  doc.definitions.filter(isFragmentNode).reduce((map: Fragments, node) => {\n    map[getName(node)] = node;\n    return map;\n  }, {});\n\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  const { directives } = node;\n  if (directives === undefined) {\n    return true;\n  }\n\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0, l = directives.length; i < l; i++) {\n    const directive = directives[i];\n    const name = getName(directive);\n\n    // Ignore other directives\n    const isInclude = name === 'include';\n    if (!isInclude && name !== 'skip') continue;\n\n    // Get the first argument and expect it to be named \"if\"\n    const arg = directive.arguments ? directive.arguments[0] : null;\n    if (!arg || getName(arg) !== 'if') continue;\n\n    const value = valueFromASTUntyped(arg.value, vars);\n    if (typeof value !== 'boolean' && value !== null) continue;\n\n    // Return whether this directive forces us to skip\n    // `@include(if: false)` or `@skip(if: true)`\n    return isInclude ? !!value : !value;\n  }\n\n  return true;\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { action } from 'mobx';\nimport {\n  getFragments,\n  getMainOperation,\n  getSelectionSet,\n  normalizeVariables,\n  getName,\n  getFieldArguments,\n  getFieldAlias,\n  getFragmentTypeName,\n} from '../ast';\n\nimport {\n  Fragments,\n  Variables,\n  Data,\n  DataField,\n  Link,\n  SelectionSet,\n  OperationRequest,\n  NullArray,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  // clearDataState,\n  makeDict,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode } from '../helpers/help';\nimport { SelectionIterator, ensureData } from './shared';\nimport { SchemaPredicates } from '../ast';\n\nexport interface QueryResult {\n  dependencies: Set<string>;\n  partial: boolean;\n  data: null | Data;\n}\n\ninterface Context {\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  fieldName: string;\n  partial: boolean;\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data\n): QueryResult => {\n  initDataState(store.data, 0);\n  const result = read(store, request, data);\n  // clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.getRootKey(operation.operation);\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx: Context = {\n    parentTypeName: rootKey,\n    parentKey: rootKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    partial: false,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  let data = input || makeDict();\n  data =\n    rootKey !== ctx.store.getRootKey('query')\n      ? readRoot(ctx, rootKey, rootSelect, data)\n      : readSelection(ctx, rootKey, rootSelect, data);\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: data === undefined ? false : ctx.partial,\n    data: data === undefined ? null : data,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  originalData: Data\n): Data => {\n  if (typeof originalData.__typename !== 'string') {\n    return originalData;\n  }\n\n  const iter = new SelectionIterator(entityKey, entityKey, select, ctx);\n  const data = makeDict();\n  data.__typename = originalData.__typename;\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = originalData[fieldAlias];\n    if (node.selectionSet !== undefined && fieldValue !== null) {\n      const fieldData = ensureData(fieldValue);\n      data[fieldAlias] = readRootField(ctx, getSelectionSet(node), fieldData);\n    } else {\n      data[fieldAlias] = fieldValue;\n    }\n  }\n\n  return data;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: null | Data | NullArray<Data>\n): Data | NullArray<Data> | null => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    for (let i = 0, l = originalData.length; i < l; i++)\n      newData[i] = readRootField(ctx, select, originalData[i]);\n    return newData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    const fieldValue = readSelection(ctx, entityKey, select, makeDict());\n    return fieldValue === undefined ? null : fieldValue;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Data | string,\n  variables?: Variables\n): Data | null => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    warn(\n      'readFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      6\n    );\n\n    return null;\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename) {\n    entity.__typename = typename;\n  }\n\n  const entityKey =\n    typeof entity !== 'string'\n      ? store.keyOfEntity({ __typename: typename, ...entity } as Data)\n      : entity;\n\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx: Context = {\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: variables || {},\n    fragments,\n    partial: false,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  return (\n    readSelection(ctx, entityKey, getSelectionSet(fragment), makeDict()) || null\n  );\n};\n\nconst readSelection = action(\n  (\n    ctx: Context,\n    entityKey: string,\n    select: SelectionSet,\n    data: Data\n  ): Data | undefined => {\n    const { store, schemaPredicates } = ctx;\n    const isQuery = entityKey === store.getRootKey('query');\n\n    // Get the __typename field for a given entity to check that it exists\n    const typename = !isQuery\n      ? InMemoryData.readRecord(entityKey, '__typename')\n      : entityKey;\n    if (typeof typename !== 'string') {\n      return undefined;\n    }\n\n    data.__typename = typename;\n    const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n    let node: FieldNode | void;\n    let hasFields = false;\n    let hasPartials = false;\n    while ((node = iter.next()) !== undefined) {\n      // Derive the needed data from our node.\n      const fieldName = getName(node);\n      const fieldArgs = getFieldArguments(node, ctx.variables);\n      const fieldAlias = getFieldAlias(node);\n      const fieldKey = keyOfField(fieldName, fieldArgs);\n      const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n      const fieldParent = InMemoryData.readParent(entityKey, fieldKey);\n      const key = joinKeys(entityKey, fieldKey);\n      let pleaseDontAssign = false;\n\n      if (\n        process.env.NODE_ENV !== 'production' &&\n        schemaPredicates &&\n        typename\n      ) {\n        schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n      }\n\n      // We temporarily store the data field in here, but undefined\n      // means that the value is missing from the cache\n      let dataFieldValue: void | DataField;\n\n      const resolvers = store.resolvers[typename];\n      if (\n        resolvers !== undefined &&\n        typeof resolvers[fieldName] === 'function'\n      ) {\n        // We have to update the information in context to reflect the info\n        // that the resolver will receive\n        ctx.parentTypeName = typename;\n        ctx.parentKey = entityKey;\n        ctx.parentFieldKey = key;\n        ctx.fieldName = fieldName;\n\n        // We have a resolver for this field.\n        // Prepare the actual fieldValue, so that the resolver can use it\n        if (fieldValue !== undefined) {\n          data[fieldAlias] = fieldValue;\n        }\n\n        dataFieldValue = resolvers[fieldName](\n          data,\n          fieldArgs || makeDict(),\n          store,\n          ctx\n        );\n\n        if (node.selectionSet !== undefined) {\n          // When it has a selection set we are resolving an entity with a\n          // subselection. This can either be a list or an object.\n          dataFieldValue = resolveResolverResult(\n            ctx,\n            typename,\n            fieldName,\n            key,\n            getSelectionSet(node),\n            (data[fieldAlias] as Data) || makeDict(),\n            dataFieldValue\n          );\n        }\n\n        if (\n          schemaPredicates !== undefined &&\n          dataFieldValue === null &&\n          !schemaPredicates.isFieldNullable(typename, fieldName)\n        ) {\n          // Special case for when null is not a valid value for the\n          // current field\n          return undefined;\n        }\n      } else if (node.selectionSet === undefined) {\n        // The field is a scalar and can be retrieved directly\n        dataFieldValue = fieldValue;\n        pleaseDontAssign = true;\n        if (data[fieldAlias] === undefined) {\n          Object.defineProperty(data, fieldAlias, {\n            get: () => {\n              return fieldParent !== null && fieldParent !== undefined\n                ? fieldParent[fieldAlias]\n                : undefined;\n            },\n          });\n        }\n      } else {\n        // We have a selection set which means that we'll be checking for links\n        const link = InMemoryData.readLink(entityKey, fieldKey);\n        if (link !== undefined) {\n          dataFieldValue = resolveLink(\n            ctx,\n            link,\n            typename,\n            fieldName,\n            getSelectionSet(node),\n            data[fieldAlias] as Data\n          );\n\n          if (data[fieldAlias] === undefined) {\n            pleaseDontAssign = true;\n            const localNode = node;\n            const localTypeName = typename;\n            Object.defineProperty(data, fieldAlias, {\n              get: () => {\n                const localLink = InMemoryData.readLink(entityKey, fieldKey);\n                if (!localLink) {\n                  return undefined;\n                }\n\n                const linkedEntity = resolveLink(\n                  ctx,\n                  localLink,\n                  localTypeName,\n                  fieldName,\n                  getSelectionSet(localNode),\n                  undefined\n                );\n                return linkedEntity !== null && linkedEntity !== undefined\n                  ? linkedEntity\n                  : undefined;\n              },\n            });\n          }\n        } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n          // The entity on the field was invalid but can still be recovered\n          dataFieldValue = fieldValue;\n        }\n      }\n\n      // Now that dataFieldValue has been retrieved it'll be set on data\n      // If it's uncached (undefined) but nullable we can continue assembling\n      // a partial query result\n      if (\n        dataFieldValue === undefined &&\n        schemaPredicates !== undefined &&\n        schemaPredicates.isFieldNullable(typename, fieldName)\n      ) {\n        // The field is uncached but we have a schema that says it's nullable\n        // Set the field to null and continue\n        hasPartials = true;\n        data[fieldAlias] = null;\n      } else if (dataFieldValue === undefined) {\n        // The field is uncached and not nullable; return undefined\n        return undefined;\n      } else {\n        // Otherwise continue as usual\n        hasFields = true;\n        if (pleaseDontAssign === false) {\n          data[fieldAlias] = dataFieldValue;\n        }\n      }\n    }\n\n    if (hasPartials) ctx.partial = true;\n    return isQuery && hasPartials && !hasFields ? undefined : data;\n  }\n);\n\nconst readResolverResult = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  data: Data,\n  result: Data\n): Data | undefined => {\n  const { store, schemaPredicates } = ctx;\n  const entityKey = store.keyOfEntity(result) || key;\n  const resolvedTypename = result.__typename;\n  const typename =\n    InMemoryData.readRecord(entityKey, '__typename') || resolvedTypename;\n\n  if (\n    typeof typename !== 'string' ||\n    (resolvedTypename && typename !== resolvedTypename)\n  ) {\n    // TODO: This may be an invalid error for resolvers that return interfaces\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return undefined;\n  }\n\n  // The following closely mirrors readSelection, but differs only slightly for the\n  // sake of resolving from an existing resolver result\n  data.__typename = typename;\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasFields = false;\n  let hasPartials = false;\n  while ((node = iter.next()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(\n      fieldName,\n      getFieldArguments(node, ctx.variables)\n    );\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result[fieldName];\n\n    if (process.env.NODE_ENV !== 'production' && schemaPredicates && typename) {\n      schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n    if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (node.selectionSet === undefined) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        data[fieldAlias] as Data,\n        resultValue\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          data[fieldAlias] as Data\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (\n      dataFieldValue === undefined &&\n      schemaPredicates !== undefined &&\n      schemaPredicates.isFieldNullable(typename, fieldName)\n    ) {\n      // The field is uncached but we have a schema that says it's nullable\n      // Set the field to null and continue\n      hasPartials = true;\n      data[fieldAlias] = null;\n    } else if (dataFieldValue === undefined) {\n      // The field is uncached and not nullable; return undefined\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = true;\n      data[fieldAlias] = dataFieldValue;\n    }\n  }\n\n  if (hasPartials) ctx.partial = true;\n  return !hasFields ? undefined : data;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | Data | Data[],\n  result: void | DataField\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { schemaPredicates } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const isListNullable =\n      schemaPredicates === undefined ||\n      schemaPredicates.isListNullable(typename, fieldName);\n    const data = new Array(result.length);\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        // Get the inner previous data from prevData\n        prevData !== undefined ? prevData[i] : undefined,\n        result[i]\n      );\n\n      if (childResult === undefined && !isListNullable) {\n        return undefined;\n      } else {\n        data[i] = childResult !== undefined ? childResult : null;\n      }\n    }\n\n    return data;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (isDataOrKey(result)) {\n    const data = prevData === undefined ? makeDict() : prevData;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readResolverResult(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | Data | Data[]\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { schemaPredicates } = ctx;\n    const isListNullable =\n      schemaPredicates !== undefined &&\n      schemaPredicates.isListNullable(typename, fieldName);\n    const newLink = new Array(link.length);\n    for (let i = 0, l = link.length; i < l; i++) {\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData !== undefined ? prevData[i] : undefined\n      );\n      if (childLink === undefined && !isListNullable) {\n        return undefined;\n      } else {\n        newLink[i] = childLink !== undefined ? childLink : null;\n      }\n    }\n\n    return newLink;\n  } else if (link === null) {\n    return null;\n  } else {\n    // console.log({link, prevData})\n    return readSelection(\n      ctx,\n      link,\n      select,\n      prevData === undefined ? makeDict() : prevData\n    );\n  }\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import { IntrospectionQuery } from 'graphql';\n\nimport {\n  Exchange,\n  formatDocument,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from 'urql/core';\n\nimport {\n  filter,\n  map,\n  merge,\n  pipe,\n  share,\n  tap,\n  fromPromise,\n  fromArray,\n  buffer,\n  take,\n  mergeMap,\n  concat,\n  empty,\n  Source,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { SchemaPredicates } from './ast';\nimport { hydrateData } from './store/data';\nimport { makeDict, Store, clearOptimistic } from './store';\n\nimport {\n  UpdatesConfig,\n  ResolverConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  StorageAdapter,\n} from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n};\n\ntype OperationMap = Map<number, Operation>;\n\ninterface DependentOperations {\n  [key: string]: number[];\n}\n\n// Returns the given operation result with added cacheOutcome meta field\nconst addCacheOutcome = (op: Operation, outcome: CacheOutcome): Operation => ({\n  ...op,\n  context: {\n    ...op.context,\n    meta: {\n      ...op.context.meta,\n      cacheOutcome: outcome,\n    },\n  },\n});\n\n// Returns the given operation with added __typename fields on its query\nconst addTypeNames = (op: Operation): Operation => ({\n  ...op,\n  query: formatDocument(op.query),\n});\n\n// Retrieves the requestPolicy from an operation\nconst getRequestPolicy = (op: Operation) => op.context.requestPolicy;\n\n// Returns whether an operation is a query\nconst isQueryOperation = (op: Operation): boolean =>\n  op.operationName === 'query';\n\n// Returns whether an operation is a mutation\nconst isMutationOperation = (op: Operation): boolean =>\n  op.operationName === 'mutation';\n\n// Returns whether an operation can potentially be read from cache\nconst isCacheableQuery = (op: Operation): boolean => {\n  return isQueryOperation(op) && getRequestPolicy(op) !== 'network-only';\n};\n\n// Returns whether an operation potentially triggers an optimistic update\nconst isOptimisticMutation = (op: Operation): boolean => {\n  return isMutationOperation(op) && getRequestPolicy(op) !== 'network-only';\n};\n\n// Copy an operation and change the requestPolicy to skip the cache\nconst toRequestPolicy = (\n  operation: Operation,\n  requestPolicy: RequestPolicy\n): Operation => ({\n  ...operation,\n  context: {\n    ...operation.context,\n    requestPolicy,\n  },\n});\n\nexport interface CacheExchangeOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionQuery;\n  storage?: StorageAdapter;\n}\n\nexport const cacheExchange = (opts?: CacheExchangeOpts): Exchange => ({\n  forward,\n  client,\n}) => {\n  if (!opts) opts = {};\n\n  const store = new Store(\n    opts.schema ? new SchemaPredicates(opts.schema) : undefined,\n    opts.resolvers,\n    opts.updates,\n    opts.optimistic,\n    opts.keys\n  );\n\n  let hydration: void | Promise<void>;\n  if (opts.storage) {\n    const storage = opts.storage;\n    hydration = storage.read().then(entries => {\n      hydrateData(store.data, storage, entries);\n    });\n  }\n\n  const optimisticKeysToDependencies = new Map<number, Set<string>>();\n  const ops: OperationMap = new Map();\n  const deps: DependentOperations = makeDict();\n\n  const collectPendingOperations = (\n    pendingOperations: Set<number>,\n    dependencies: void | Set<string>\n  ) => {\n    if (dependencies !== undefined) {\n      // Collect operations that will be updated due to cache changes\n      dependencies.forEach(dep => {\n        const keys = deps[dep];\n        if (keys !== undefined) {\n          deps[dep] = [];\n          for (let i = 0, l = keys.length; i < l; i++) {\n            pendingOperations.add(keys[i]);\n          }\n        }\n      });\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Set<number>\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    pendingOperations.forEach(key => {\n      if (key !== operation.key) {\n        const op = ops.get(key);\n        if (op !== undefined) {\n          ops.delete(key);\n          client.reexecuteOperation(toRequestPolicy(op, 'cache-first'));\n        }\n      }\n    });\n  };\n\n  // This executes an optimistic update for mutations and registers it if necessary\n  const optimisticUpdate = (operation: Operation) => {\n    if (isOptimisticMutation(operation)) {\n      const { key } = operation;\n      const { dependencies } = writeOptimistic(store, operation, key);\n      if (dependencies.size !== 0) {\n        optimisticKeysToDependencies.set(key, dependencies);\n        const pendingOperations = new Set<number>();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Set<string>) => {\n    dependencies.forEach(dep => {\n      const keys = deps[dep] || (deps[dep] = []);\n      keys.push(op.key);\n\n      if (!ops.has(op.key)) {\n        ops.set(\n          op.key,\n          getRequestPolicy(op) === 'network-only'\n            ? toRequestPolicy(op, 'cache-and-network')\n            : op\n        );\n      }\n    });\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const { data, dependencies, partial } = query(store, operation);\n    let cacheOutcome: CacheOutcome;\n\n    if (data === null) {\n      cacheOutcome = 'miss';\n    } else {\n      updateDependencies(operation, dependencies);\n      cacheOutcome =\n        !partial || getRequestPolicy(operation) === 'cache-only'\n          ? 'hit'\n          : 'partial';\n    }\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (result: OperationResult): OperationResult => {\n    const { operation, error, extensions } = result;\n    const isQuery = isQueryOperation(operation);\n    let { data } = result;\n\n    // Clear old optimistic values from the store\n    const { key } = operation;\n    const pendingOperations = new Set<number>();\n    collectPendingOperations(\n      pendingOperations,\n      optimisticKeysToDependencies.get(key)\n    );\n    optimisticKeysToDependencies.delete(key);\n    clearOptimistic(store.data, key);\n\n    let writeDependencies: Set<string> | void;\n    let queryDependencies: Set<string> | void;\n    if (data !== null && data !== undefined) {\n      writeDependencies = write(store, operation, data).dependencies;\n\n      if (isQuery) {\n        const queryResult = query(store, operation);\n        data = queryResult.data;\n        queryDependencies = queryResult.dependencies;\n      } else {\n        data = query(store, operation, data).data;\n      }\n    }\n\n    // Collect all write dependencies and query dependencies for queries\n    collectPendingOperations(pendingOperations, writeDependencies);\n    if (isQuery) {\n      collectPendingOperations(pendingOperations, queryDependencies);\n    }\n\n    // Execute all pending operations related to changed dependencies\n    executePendingOperations(result.operation, pendingOperations);\n\n    // Update this operation's dependencies if it's a query\n    if (isQuery && queryDependencies !== undefined) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Buffer operations while waiting on hydration to finish\n    // If no hydration takes place we replace this stream with an empty one\n    const bufferedOps$ = hydration\n      ? pipe(\n          sharedOps$,\n          buffer(fromPromise(hydration)),\n          take(1),\n          mergeMap(fromArray)\n        )\n      : (empty as Source<Operation>);\n\n    const inputOps$ = pipe(\n      concat([bufferedOps$, sharedOps$]),\n      map(addTypeNames),\n      tap(optimisticUpdate),\n      share\n    );\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cache$ = pipe(\n      inputOps$,\n      filter(op => isCacheableQuery(op)),\n      map(operationResultFromCache),\n      share\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheOps$ = pipe(\n      cache$,\n      filter(res => res.outcome === 'miss'),\n      map(res => addCacheOutcome(res.operation, res.outcome))\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cache$,\n      filter(res => res.outcome !== 'miss'),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome } = res;\n          const policy = getRequestPolicy(operation);\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            policy === 'cache-and-network' ||\n            (policy === 'cache-first' && outcome === 'partial')\n          ) {\n            result.stale = true;\n            client.reexecuteOperation(\n              toRequestPolicy(operation, 'network-only')\n            );\n          }\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      forward(\n        merge([\n          pipe(\n            inputOps$,\n            filter(op => !isCacheableQuery(op))\n          ),\n          cacheOps$,\n        ])\n      ),\n      map(updateCacheWithResult)\n    );\n\n    return merge([result$, cacheResult$]);\n  };\n};\n","import {\n  DocumentNode,\n  buildClientSchema,\n  visitWithTypeInfo,\n  TypeInfo,\n  FragmentDefinitionNode,\n  GraphQLSchema,\n  IntrospectionQuery,\n  FragmentSpreadNode,\n  NameNode,\n  ASTNode,\n  isCompositeType,\n  isAbstractType,\n  Kind,\n  visit,\n} from 'graphql';\n\nimport { pipe, tap, map } from 'wonka';\nimport { Exchange, Operation } from 'urql/core';\n\nimport { getName, getSelectionSet, unwrapType } from './ast';\nimport { makeDict } from './store';\nimport { invariant, warn } from './helpers/help';\n\ninterface PopulateExchangeOpts {\n  schema: IntrospectionQuery;\n}\n\n/** An exchange for auto-populating mutations with a required response body. */\nexport const populateExchange = ({\n  schema: ogSchema,\n}: PopulateExchangeOpts): Exchange => ({ forward }) => {\n  const schema = buildClientSchema(ogSchema);\n  /** List of operation keys that have already been parsed. */\n  const parsedOperations = new Set<number>();\n  /** List of operation keys that have not been torn down. */\n  const activeOperations = new Set<number>();\n  /** Collection of fragments used by the user. */\n  const userFragments: UserFragmentMap = makeDict();\n  /** Collection of actively in use type fragments. */\n  const activeTypeFragments: TypeFragmentMap = makeDict();\n\n  /** Handle mutation and inject selections + fragments. */\n  const handleIncomingMutation = (op: Operation) => {\n    if (op.operationName !== 'mutation') {\n      return op;\n    }\n\n    const activeSelections: TypeFragmentMap = makeDict();\n    for (const name in activeTypeFragments) {\n      activeSelections[name] = activeTypeFragments[name].filter(s =>\n        activeOperations.has(s.key)\n      );\n    }\n\n    return {\n      ...op,\n      query: addFragmentsToQuery(\n        schema,\n        op.query,\n        activeSelections,\n        userFragments\n      ),\n    };\n  };\n\n  /** Handle query and extract fragments. */\n  const handleIncomingQuery = ({ key, operationName, query }: Operation) => {\n    if (operationName !== 'query') {\n      return;\n    }\n\n    activeOperations.add(key);\n    if (parsedOperations.has(key)) {\n      return;\n    }\n\n    parsedOperations.add(key);\n\n    const [extractedFragments, newFragments] = extractSelectionsFromQuery(\n      schema,\n      query\n    );\n\n    for (let i = 0, l = extractedFragments.length; i < l; i++) {\n      const fragment = extractedFragments[i];\n      userFragments[getName(fragment)] = fragment;\n    }\n\n    for (let i = 0, l = newFragments.length; i < l; i++) {\n      const fragment = newFragments[i];\n      const type = getName(fragment.typeCondition);\n      const current =\n        activeTypeFragments[type] || (activeTypeFragments[type] = []);\n\n      (fragment as any).name.value += current.length;\n      current.push({ key, fragment });\n    }\n  };\n\n  const handleIncomingTeardown = ({ key, operationName }: Operation) => {\n    if (operationName === 'teardown') {\n      activeOperations.delete(key);\n    }\n  };\n\n  return ops$ => {\n    return pipe(\n      ops$,\n      tap(handleIncomingQuery),\n      tap(handleIncomingTeardown),\n      map(handleIncomingMutation),\n      forward\n    );\n  };\n};\n\ntype UserFragmentMap<T extends string = string> = Record<\n  T,\n  FragmentDefinitionNode\n>;\n\ntype TypeFragmentMap<T extends string = string> = Record<T, TypeFragment[]>;\n\ninterface TypeFragment {\n  /** Operation key where selection set is being used. */\n  key: number;\n  /** Selection set. */\n  fragment: FragmentDefinitionNode;\n}\n\n/** Gets typed selection sets and fragments from query */\nexport const extractSelectionsFromQuery = (\n  schema: GraphQLSchema,\n  query: DocumentNode\n) => {\n  const extractedFragments: FragmentDefinitionNode[] = [];\n  const newFragments: FragmentDefinitionNode[] = [];\n  const typeInfo = new TypeInfo(schema);\n\n  visit(\n    query,\n    visitWithTypeInfo(typeInfo, {\n      Field: node => {\n        if (node.selectionSet) {\n          const type = getTypeName(typeInfo);\n          newFragments.push({\n            kind: Kind.FRAGMENT_DEFINITION,\n            typeCondition: {\n              kind: Kind.NAMED_TYPE,\n              name: nameNode(type),\n            },\n            name: nameNode(`${type}_PopulateFragment_`),\n            selectionSet: node.selectionSet,\n          });\n        }\n      },\n      FragmentDefinition: node => {\n        extractedFragments.push(node);\n      },\n    })\n  );\n\n  return [extractedFragments, newFragments];\n};\n\n/** Replaces populate decorator with fragment spreads + fragments. */\nexport const addFragmentsToQuery = (\n  schema: GraphQLSchema,\n  query: DocumentNode,\n  activeTypeFragments: TypeFragmentMap,\n  userFragments: UserFragmentMap\n) => {\n  const typeInfo = new TypeInfo(schema);\n\n  const requiredUserFragments: Record<\n    string,\n    FragmentDefinitionNode\n  > = makeDict();\n\n  const additionalFragments: Record<\n    string,\n    FragmentDefinitionNode\n  > = makeDict();\n\n  /** Fragments provided and used by the current query */\n  const existingFragmentsForQuery: Set<string> = new Set();\n\n  return visit(\n    query,\n    visitWithTypeInfo(typeInfo, {\n      Field: {\n        enter: node => {\n          if (!node.directives) {\n            return;\n          }\n\n          const directives = node.directives.filter(\n            d => getName(d) !== 'populate'\n          );\n          if (directives.length === node.directives.length) {\n            return;\n          }\n\n          const possibleTypes = getTypes(schema, typeInfo);\n          const newSelections = possibleTypes.reduce((p, possibleType) => {\n            const typeFrags = activeTypeFragments[possibleType.name];\n            if (!typeFrags) {\n              return p;\n            }\n\n            for (let i = 0, l = typeFrags.length; i < l; i++) {\n              const { fragment } = typeFrags[i];\n              const fragmentName = getName(fragment);\n              const usedFragments = getUsedFragments(fragment);\n\n              // Add used fragment for insertion at Document node\n              for (let j = 0, l = usedFragments.length; j < l; j++) {\n                const name = usedFragments[j];\n                if (!existingFragmentsForQuery.has(name)) {\n                  requiredUserFragments[name] = userFragments[name];\n                }\n              }\n\n              // Add fragment for insertion at Document node\n              additionalFragments[fragmentName] = fragment;\n\n              p.push({\n                kind: Kind.FRAGMENT_SPREAD,\n                name: nameNode(fragmentName),\n              });\n            }\n\n            return p;\n          }, [] as FragmentSpreadNode[]);\n\n          const existingSelections = getSelectionSet(node);\n\n          const selections =\n            existingSelections.length + newSelections.length !== 0\n              ? [...newSelections, ...existingSelections]\n              : [\n                  {\n                    kind: Kind.FIELD,\n                    name: nameNode('__typename'),\n                  },\n                ];\n\n          return {\n            ...node,\n            directives,\n            selectionSet: {\n              kind: Kind.SELECTION_SET,\n              selections,\n            },\n          };\n        },\n      },\n      Document: {\n        enter: node => {\n          node.definitions.reduce((set, definition) => {\n            if (definition.kind === 'FragmentDefinition') {\n              set.add(definition.name.value);\n            }\n            return set;\n          }, existingFragmentsForQuery);\n        },\n        leave: node => {\n          const definitions = [...node.definitions];\n          for (const key in additionalFragments)\n            definitions.push(additionalFragments[key]);\n          for (const key in requiredUserFragments)\n            definitions.push(requiredUserFragments[key]);\n          return { ...node, definitions };\n        },\n      },\n    })\n  );\n};\n\nconst nameNode = (value: string): NameNode => ({\n  kind: Kind.NAME,\n  value,\n});\n\n/** Get all possible types for node with TypeInfo. */\nconst getTypes = (schema: GraphQLSchema, typeInfo: TypeInfo) => {\n  const type = unwrapType(typeInfo.getType());\n  if (!isCompositeType(type)) {\n    warn(\n      'Invalid type: The type ` + type + ` is used with @populate but does not exist.',\n      17\n    );\n    return [];\n  }\n\n  return isAbstractType(type) ? schema.getPossibleTypes(type) : [type];\n};\n\n/** Get name of non-abstract type for adding to 'activeTypeFragments'. */\nconst getTypeName = (typeInfo: TypeInfo) => {\n  const type = unwrapType(typeInfo.getType());\n  invariant(\n    type && !isAbstractType(type),\n    'Invalid TypeInfo state: Found no flat schema type when one was expected.',\n    18\n  );\n\n  return type.toString();\n};\n\n/** Get fragment names referenced by node. */\nconst getUsedFragments = (node: ASTNode) => {\n  const names: string[] = [];\n\n  visit(node, {\n    FragmentSpread: f => {\n      names.push(getName(f));\n    },\n  });\n\n  return names;\n};\n"],"names":["const","getName","node","getFragmentTypeName","getFieldAlias","getSelectionSet","getTypeCondition","ref","typeCondition","isFieldNode","Kind","isInlineFragment","unwrapType","type","helpUrl","cache","Set","currentDebugStack","pushDebugNode","typename","identifier","getDebugOutput","condition","message","code","process","errorMessage","error","Error","console","keyOfField","fieldName","args","stringifyVariables","fieldInfoOfKey","fieldKey","parenIndex","arguments","JSON","joinKeys","parentKey","key","prefixKey","owner","defer","Promise","fn","makeDict","currentData","currentDependencies","currentOptimisticKey","makeNodeMap","optimistic","base","Map","keys","initDataState","data","optimisticKey","window","clearDataState","gc","getCurrentDependencies","invariant","setNode","map","entityKey","value","undefined","keymap","entity","getNode","i","l","clearOptimisticNodes","index","updateRCForEntity","gcBatch","refCount","by","count","newCount","updateRCForLink","link","Array","extractNodeFields","fieldInfos","seenFieldKeys","extractNodeMapFields","linkNode","updateDependencies","readRecord","readLink","writeRecord","writeLink","links","prevLinkNode","prevLink","hydrateData","storage","entries","dotIndex","isFragmentHeuristicallyMatching","ctx","warn","getFieldArguments","hasField","SelectionIterator","select","this","shouldInclude","fragmentNode","ensureData","x","write","store","request","startWrite","operation","getMainOperation","result","dependencies","operationName","parentTypeName","parentFieldKey","variables","normalizeVariables","fragments","getFragments","schemaPredicates","writeSelection","writeRoot","writeOptimistic","mutationRootKey","iter","resolver","resolverData","resolverValue","updater","fieldArgs","writeFragment","query","names","Object","fragment","_extends","__typename","writeData","InMemoryData","fieldValue","advice","expected","fieldData","writeField","newData","item","indexKey","isRootField","writeRootField","invalidateSelection","fieldSelect","let","childLink","Store","resolvers","updates","optimisticMutations","queryType","mutationType","schema","subscriptionType","queryName","mutationName","subscriptionName","queryRootKey","persistenceScheduled","persistenceBatch","gcScheduled","refLock","records","name","id","_id","field","createRequest","input","output","dataFragment","vars","argsSize","arg","valueFromASTUntyped","def","SchemaPredicates","buildClientSchema","getField","isNullableType","isNonNullType","ofType","fieldname","abstractType","objectType","expectAbstractType","object","isFragmentNode","doc","directives","directive","isInclude","read","rootKey","rootSelect","partial","readRoot","readSelection","originalData","fieldAlias","readRootField","readFragment","action","isQuery","hasFields","hasPartials","fieldParent","pleaseDontAssign","dataFieldValue","resolveResolverResult","get","resolveLink","localNode","localTypeName","localLink","linkedEntity","prevData","childResult","isListNullable","isDataOrKey","resolvedTypename","resultValue","readResolverResult","newLink","addCacheOutcome","op","outcome","context","meta","cacheOutcome","addTypeNames","formatDocument","getRequestPolicy","isQueryOperation","isCacheableQuery","toRequestPolicy","requestPolicy","res","extractSelectionsFromQuery","extractedFragments","newFragments","typeInfo","TypeInfo","visitWithTypeInfo","Field","getTypeName","kind","nameNode","selectionSet","FragmentDefinition","d","set","definition","addFragmentsToQuery","activeTypeFragments","userFragments","p","possibleType","typeFrags","fragmentName","usedFragments","getUsedFragments","j","requiredUserFragments","additionalFragments","existingFragmentsForQuery","enter","newSelections","getTypes","existingSelections","selections","Document","leave","definitions","isAbstractType","FragmentSpread","f","opts","policy","extensions","client","hydration","optimisticKeysToDependencies","ops","deps","collectPendingOperations","pendingOperations","dep","executePendingOperations","optimisticUpdate","isOptimisticMutation","operationResultFromCache","updateCacheWithResult","writeDependencies","queryResult","queryDependencies","ops$","sharedOps$","share","bufferedOps$","mergeMap","fromArray","take","buffer","fromPromise","empty","tap","concat","cache$","filter","inputOps$","forward","merge","cacheOps$","result$","cacheResult$","s","ogSchema","parsedOperations","activeOperations","handleIncomingMutation","activeSelections","handleIncomingQuery","current","handleIncomingTeardown"],"mappings":";;;;;;;;;;;;;;;;AAgBOA,IAAMC,mBAAWC;;GAEXC,+BAAuBD;;GAIvBE,yBAAiBF;oBAC5BA,UAA2BA,gBAAmBD,QAAQC;GAG3CG,2BAAmBH;oBAG9BA,iBAAkCA,4BAA+B;GAEtDI,4BAAoBC;4CAKDN,QAAQO,KAAiB;GAE5CC,uBAAeP;oBACZQ;GAEHC,4BACXT;oBAC6CQ;GAElCE,sBACXC;gCAEmBA,KACVD,WAAWC,YAGbA,KAAQ;GC1CXC,UACJ,0FACIC,QAAQ,IAAIC,KAELC,oBAA8B,IAE9BC,yBAAiBC,GAAyBjB;MACjDkB,IAAa;aACCV,+BAChBU,IAAaD,6BACcA,UACvB,oBACKjB,WAAcQ,oCAEvBU,KADalB,eAAgBA,qBAAqB,mBAC1BA,cACfA,WAAcQ,qCACvBU,IAAa,MAAIlB;OAIjBe,uBAAuBG;GAIrBC;oCAEA,mBAAmBJ,uBAAuB,QAAQ,MAClD;;;mBAGJK,GACAC,GACAC;OAEKF;cACgBC,KAAW,oBAAoBC,IAAO,MAC5B,iBAAzBC,yBACFC,KAAgBL;KAGZM,IAAYC,MAAMF,IAAeZ,UAAUU,WACpC,oBACPG;;;;cAIWJ,GAAiBC;EAC/BT,UAAUQ,OACbM,aAAaN,IAAUF,mBAAmBP,UAAUU,IACpDT,UAAUQ;;;ACxDPvB,IAAM8B,sBAAcC,GAAmBC;aAClCD,UAAaE,wBAAmBD,WAAWD;GAE1CG,0BAAkBC;MACvBC,IAAaD,UAAiB;cAChCC,IACK;cACLD;IACAJ,WAAWI,QAAe,GAAGC;IAC7BC,WAAWC,WAAWH,QAAeC,IAAa;MAG7C;cACLD;IACAJ,WAAWI;IACXE,WAAW;;GAKJE,oBAAYC,GAAmBC;mBAC1BA;GAGLC,qBAAaC,GAAkBF;mBAA4BA;GC3B3DG,QACc,iBAAzBnB,wBAA4D,gCACxDoB,4BAA4BA,8BAC5BC;oBAAiBA,GAAI;GCiCdC;yBAAiC;GAE1CC,cAAmC,MACnCC,sBAA0C,MAC1CC,uBAAsC,MAEpCC;SAAoC;IACxCC,YAAYL;IACZM,MAAM,IAAIC;IACVC,MAAM;;GAIKC,yBACXC,GACAC;EAGAC,qBAAqBX,cAAcS;wBACb,IAAIzC;yBACH0C;mBACnBjC,yBACFR,2BAA2B;GAKlB2C;MACLH,IAAOT;GAERS,iBAAwC,IAApBA,mBACvBA,iBAAmB,GACnBb;IACEiB,GAAGJ;;gBAIcA,2BACnBA,0BAA4B,GAC5Bb;IACEa,gBAAoBA;8BACQ;yBACJV;;yBAK5BE,sBADAD,cAAc;mBAGVvB,yBACFR,2BAA2B;GAKlB6C;EACXC,UAC0B,SAAxBd,6DACA,0KAGA;;GAoBEe,mBACJC,GACAC,GACA/B,GACAgC;EAKIjB,6BAG2CkB,MAAzCH,aAAef,0BACjBe,aAAef,wBAAwB,IAAII;EAC3CW,eAAiBf,wBAGnBmB,IAASJ,aAAef,yBAExBmB,IAASJ;MAIPK,IAASD,MAAWH;aACpBI,KACFD,MAAWH,GAAYI,IAASvB;aAM9BoB,KAAwBjB,uBAG1BoB,EAAOnC,KAAYgC,WAFZG,EAAOnC;GAOZoC,mBACJN,GACAC,GACA/B;OACe,IAENqC,IAAI,GAAGC,IAAIR,eAAiBO,IAAIC,GAAGD,KAAK;QAEzCtE,IADa+D,aAAeA,OAASO,QACfN;aAEfE,MAATlE,KAAsBiC;eACZA;;;qBAKVjC,IAAO+D,WAAaC,MACEhE,EAAKiC,UAAYiC;GAyBzCM,gCAA2BT,GAAiBP;MAE1CiB,IAAQV,eAAiBP;OAC3BiB,aAEKV,aAAeP,IACtBO,cAAgBU,GAAO;GAKrBC,6BACJC,GACAC,GACAZ,GACAa;MAGMC,SAAgCZ,MAAxBU,EAASZ,KAA2BY,EAASZ,KAAa;MAEtDY,EAASZ,KAAcc,IAAQD,IAAM;aAGnDF,MACc,KAAZI,IAAeJ,MAAYX,KACb,KAATc,KAAyB,IAAXC,KAAcJ,SAAeX;GAKlDgB,2BACJL,GACAC,GACAK,GACAJ;MAEoB;IAClBH,kBAAkBC,GAASC,GAAUK,GAAMJ;aAClCK,cAAcD;SAAO,IACrBX,IAAI,GAAGC,IAAIU,UAAaX,IAAIC,GAAGD,KAAK;UACrCN,IAAYiB,EAAKX;WAErBI,kBAAkBC,GAASC,GAAUZ,GAAWa;;;GAOlDM,6BACJC,GACAC,GACArF;WAEakE,MAATlE;SACGF,IAAMmC;MACJoD,MAAkBpD,OAGrBmD,OAAgBpD,eAAeC,KAC/BoD,MAAkBpD;;;GAOpBqD,gCACJF,GACAC,GACArB,GACAD;EAGAoB,kBAAkBC,GAAYC,GAAetB,WAAaC;OAF1D,IAKSM,IAAI,GAAGC,IAAIR,eAAiBO,IAAIC,GAAGD;IAE1Ca,kBAAkBC,GAAYC,GADXtB,aAAeA,OAASO,QACiBN;;GAKnDL,cAAMJ;EAEjBA,iBAAmB;8BAIES;QAGT,MADCT,WAAcS,MAAc,IAC1B;WAENlE,IAAM0D,gBAA+B;YAClCoB,IAAWrB,UAAaC;YAIlB,KAHEoB,EAASZ,MAAc;;;eAI9BY,EAASZ;;aAMXT,WAAcS;uBACDA;eAOAE,WADAX,mBAAsBS,QAExCT,sBAAyBS,IACrBT;aACGzD,IAAMmC;UACHM,IAAMC,UAAU,KAAKH,SAAS2B,GAAW/B,KAC/CsB,mBAAsBhB,UAAO2B;;;eAQlBA,WADAX,iBAAoBS,KACT;QAC1BT,oBAAuBS;aAClBlE,IAAMmC;UAELsB,cACIhB,IAAMC,UAAU,KAAKH,SAAS2B,GAAW/B,KAC/CsB,mBAAsBhB,UAAO2B;UAG/Bc,gBAAgBzB,WAAcA,YAAegC,EAAStD;;;;uBAItC+B;;;GAKpBwB,8BAAsBxB,GAAmB/B;EAC5B,iBAAbA,MACE+B,MAAclB,2BAChBC,wBAAyBiB,UACHE,MAAbjC,KACTc,wBAAyBV,SAAS2B,GAAW/B;GAMtCwD,sBACXzB,GACA/B;EAEAuD,mBAAmBxB,GAAW/B;iBACfa,qBAAsBkB,GAAW/B;GAYrCyD,oBACX1B,GACA/B;EAEAuD,mBAAmBxB,GAAW/B;iBACfa,mBAAoBkB,GAAW/B;GAInC0D,uBACX3B,GACA/B,GACAgC;EAEAuB,mBAAmBxB,GAAW/B;UACtBa,qBAAsBkB,GAAW/B,GAAUgC;0BACtBjB,yBACrBT,IAAMC,UAAU,KAAKH,SAAS2B,GAAW/B;EAC/Ca,6BAA8BP,KAAO0B;GAS5B2B,qBACX5B,GACA/B,GACAgD;MAEM1B,IAAOT;MAOTE,sBAAsB;IAGxB4B,IAAAA,IACErB,UAAaP,0BACZO,UAAaP,wBAAwBH;QACxCgD,IAAQtC,mBAAsBP;SACzB;IACDO,cACIhB,IAAMC,UAAU,KAAKH,SAAS2B,GAAW/B,KAC/CsB,mBAAsBhB,KAAO0C;QAEpB1B;QACHA;QACRoB,IAAUpB;;WAKsBW,OAD5B4B,SAAyB5B,MAAV2B,IAAsBA,MAAU7B,UAAaE,KACpB4B,EAAa7D,KAAY;qBAGpD+B,GAAW/B;UAEtBsB,SAAYS,GAAW/B,GAAUgD;kBAEzBN,GAASC,GAAUmB;kBAEnBpB,GAASC,GAAUK,GAAM;GAyB9Be,uBACXzC,GACA0C,GACAC;EAEA5C,cAAcC,GAAM;OACfzD,IAAMyC,QAAgB;QACnB4D,IAAW5D,UAAY,MACvByB,IAAYzB,QAAU,GAAG4D;QACd5D,QAAU4D,IAAW;YAC9B5D,aAAe;UAChB;MACHqD,UAAU5B,GAAW/B,GAAUiE,EAAQ3D;;;UAEpC;MACHoD,YAAY3B,GAAW/B,GAAUiE,EAAQ3D;;;EAI/CmB;cACeuC;GCncXG,2CACJpG,GACAiB,GACA+C,GACAqC;OAEKpF;YAAiB;;MAChBX,IAAgBF,iBAAiBJ;MACnCiB,MAAaX;YAAsB;;2CAEvCgG,KACE,6EACErF,IACA,wCAEAX,IACA,6CACAA,IACA,iJAGF;UAGMH,gBAAgBH,kBAAWA;SAC5BO,YAAYP;cAAc;;QACd4B,WACf7B,QAAQC,IACRuG,kBAAkBvG,GAAMqG;qBD6ULrC,GAAmB/B;wBAC1CwD,WAAWzB,GAAW/B,WACYiC,MAAlCwB,SAAS1B,GAAW/B;KC7UVuE,CAASxC,GAAW/B;;GAInBwE,oBAOXA,SACExF,GACA+C,GACA0C,GACAL;kBAEgBpF;mBACC+C;iBACFqC;oBACG,EAAC;wBACG,EAACK;;;;QAIW,MAA3BC,0BAA8B;QAC7BlC,IAAQkC,gBAAgBA,yBAAyB,MACjDD,IAASC,oBAAoBA,6BAA6B;QAC5DlC,KAASiC;;eAMNE,cADC5G,IAAO0G,EAAOjC,IACKkC;MAElB,IAAKpG,YAAYP;YA+BK,iBAAlBD,QAAQC;;;sBAzBIkE,OAJf2C,IAAgBpG,iBAAiBT,KAEnCA,IADA2G,uBAAuB5G,QAAQC,SAIJ,iBAAzBuB,sCACYoF,eAAeE;WAIK3C,MAAlCyC,gCACIA,gDACEvG,iBAAiByG,IACjBF,iBAEFP,gCACES,GACAF,eACAA,gBACAA;6BAIe,6BACIxG,gBAAgB0G;;;;;;IAiB1CC,sBAAcC;oBACzBA,IAAkB,OAAQA;GCvFfC,iBACXC,GACAC,GACA3D;EAEAD,cAAc2D,QAAY;MACXE,WAAWF,GAAOC,GAAS3D;;;GAK/B4D,sBACXF,GACAC,GACA3D;MAEM6D,IAAYC,iBAAiBH,UAC7BI,IAAsB;IAAEC,cAAc3D;KAEtC8C,IAASvG,gBAAgBiH,IACzBI,IAAgBP,aAAiBG;MAElB;IACnBK,gBAAgBD;IAChBlF,WAAWkF;IACXE,gBAAgB;IAChB7F,WAAW;IACX8F,WAAWC,mBAAmBR,GAAWF;IACzCW,WAAWC,aAAaZ;YACxBI;WACAL;IACAc,kBAAkBd;;mBAGhB1F,wBACFP,cAAcwG,GAAeJ;QAGTf,mBAAqB,WACzC2B,eAAe3B,GAAKmB,GAAed,GAAQnD,KAE3C0E,UAAU5B,GAAKmB,GAAed,GAAQnD;;GAM7B2E,2BACXjB,GACAC,GACA1D;EAEAF,cAAc2D,QAAYzD;MAEpB4D,IAAYC,iBAAiBH;MACP;IAAEK,cAAc3D;;MAEtCuE,IAAkBlB,aAAiB,aACnCO,IAAgBP,aAAiBG;YAErCI,MAAkBW,2CAClB,oIAEA;mBAGE5G,wBACFP,cAAcwG,GAAeJ;MAGV;IACnBK,gBAAgBU;IAChB7F,WAAW6F;IACXT,gBAAgB;IAChB7F,WAAW;IACX8F,WAAWC,mBAAmBR,GAAWF;IACzCW,WAAWC,aAAaZ;YACxBI;WACAL;IACAc,kBAAkBd;IAClB/D,aAAY;;MAGDL;MACA,IAAI4D,kBACfe,GACAA,GACArH,gBAAgBiH,IAChBf;WAGErG,QAC4BkE,OAAxBlE,IAAOoI;aACalE,MAAtBlE,gBAAiC;UAC7B6B,IAAY9B,QAAQC,IACpBqI,IAAWhC,4BAA8BxE;eAE9BqC,MAAbmE,GAAwB;QAE1BhC,cAAgBxE;YAGMwG,OADJ9B,kBAAkBvG,GAAMqG,iBACExD,YAAYwD,SAAWA;YAC7DiC,IAAexB,WAAWyB;uBACjBlC,GAAKiC,GAAcnI,gBAAgBH;UAC7C6B,KAAa0G;wBACFlC,gBAAkB8B,GAAiBtG,OAEjD2G,EAAQjF,GAAMkF,KAAa5F,YAAYwD,SAAWA;;;;EAM1D3C;;GAIWgF,yBACXzB,GACA0B,GACApF,GACAoE;EAEME,IAAYC,aAAaa;MACzBC,IAAQC,YAAYhB;WAET3D,WADA2D,EAAUe,EAAM;mDAExBtC,KACL,mIAEA;;MAIErF,IAAWhB,oBAAoB6I;MACnBC;IAAEC,YAAY/H;KAAasC;MACvCS,IAAYiD,cAAkBgC;OAC/BjF;mDACIsC,KACL,sIAEErF,IACA,MACF;;mBAIAM,wBACFP,cAAcC,GAAU6H;MAGL;IACnBrB,gBAAgBxG;IAChBqB,WAAW0B;IACX0D,gBAAgB;IAChB7F,WAAW;IACX8F,WAAWA,KAAa;eACxBE;IACAP,QAAQ;MAAEC,cAAc3D;;WACxBqD;IACAc,kBAAkBd;;iBAGLZ,GAAKrC,GAAW7D,gBAAgB2I,IAAWG;GAGtDjB,0BACJ3B,GACArC,GACA0C,GACAnD;MAGMtC,IADU+C,MAAcqC,mBAAqB,WACxBrC,IAAYT;MACf;IAExB2F,YAAyBlF,GAAW,cAAc/C;QAErC,IAAIwF,kBAAkBxF,GAAU+C,GAAW0C,GAAQL;aAE5DrG,QAC4BkE,OAAxBlE,IAAOoI,aAA4B;UACnCvG,IAAY9B,QAAQC,IACpByI,IAAYlC,kBAAkBvG,GAAMqG;UACzBzE,WAAWC,GAAW4G;UACjCU,IAAa5F,EAAKrD,cAAcF,KAChCuC,IAAMF,SAAS2B,GAAW/B;UAEH,iBAAzBV;iBACiB2C,MAAfiF,GAA0B;UACtBC,IAAS/C,eACX,qDACA;mBAGoBnC,MAAtBlE,iBACI,kCACA;mDAENsG,KACE,sCACErE,IACA,uDACAoH,IACA,qBACAD,GACF;;;gCAI+BnI,KACjCoF,0CAA4CpF,GAAUY;;;iBAItD7B,iBAEFkJ,YAAyBlF,GAAW/B,GAAUkH,MAGxCG,IAAYxC,WAAWqC,IACvBlE,IAAOsE,WAAWlD,GAAK9D,GAAKpC,gBAAgBH,IAAOsJ;MACzDJ,UAAuBlF,GAAW/B,GAAUgD;;;GAK5CsE,sBACJlD,GACAqB,GACAhB,GACAnD;MAEI2B,cAAc3B,IAAO;aACjBiG,IAActE,MAAM3B,WACjBe,IAAI,GAAGC,IAAIhB,UAAae,IAAIC,GAAGD,KAAK;UACrCmF,IAAOlG,EAAKe,IAEZoF,IAAWrH,SAASqF,QAAmBpD;UAE/BiF,WAAWlD,GAAKqD,GAAUhD,GAAQ+C;QAExCnF,KAAKuB;;;;EAIV,IAAa,SAATtC;;;MAKe,cADR8C,oBAAsB9C,MACPS,IAAY0D;MAC5BnE;aAGf8C,aAAe9C,iBACD,SAAdS,KACoB,wBACnB/C,WAAkB,iBAClBA,WAAkB,WACN,eAAbA,8CAEAqF,KACE,qDACEoB,IACA,6LAIAzG,IACA,mIAGAA,IACA,+BACF;iBAIWoF,GAAK9D,GAAKmE,GAAQnD;;GAK7B0E,qBACJ5B,GACApF,GACAyF,GACAnD;MAEMoG,IACJ1I,MAAaoF,mBAAqB,eAClCpF,MAAaoF,mBAAqB;MAEvB,IAAII,kBAAkBxF,GAAUA,GAAUyF,GAAQL;WAE3DrG,QAC4BkE,OAAxBlE,IAAOoI,aAA4B;QACnCvG,IAAY9B,QAAQC,IACpByI,IAAYlC,kBAAkBvG,GAAMqG,cACpCpE,IAAWI,SAASpB,GAAUW,WAAWC,GAAW4G;aAChCvE,MAAtBlE,gBAAiC;UAC7BmJ,IAAarC,WAAWvD,EAAKrD,cAAcF;qBAClCqG,GAAK8C,GAAYhJ,gBAAgBH;;IAG9C2J,MAEFtD,mBAAqBpF,GACrBoF,cAAgBpF,GAChBoF,mBAAqBpE,GACrBoE,cAAgBxE;SAKAqC,OADVsE,IAAUnC,gBAAkBpF,GAAUY,OAE1C2G,EAAQjF,GAAMkF,KAAa5F,YAAYwD,SAAWA;;GAOpDuD,0BACJvD,GACA9C,GACAmD;MAEIxB,cAAc3B,IAAO;aACjBiG,IAActE,MAAM3B,WACjBe,IAAI,GAAGC,IAAIhB,UAAae,IAAIC,GAAGD;MACtCkF,EAAQlF,KAAKsF,eAAevD,GAAK9C,EAAKe,IAAIoC;;;;EAE1B,SAATnD,MAMO,UADZS,IAAYqC,oBAAsB9C,MAEtCyE,eAAe3B,GAAKrC,GAAW0C,GAAQnD,KAGvC0E,UAAU5B,GADO9C,cACQmD,GAAQnD;GCjWxBsG,+BACXxD,GACArC,GACA0C;MAE8B,YAAd1C,GAGF;IACZ/C,IAAAA,IAAWiI,WAAwBlF,GAAW;QACtB;;;gBAGGA,GAAW,mBAAcE;;QAGzCF;;MAGA,IAAIyC,kBAAkBxF,GAAU+C,GAAW0C,GAAQL;WAE5DrG,QAC4BkE,OAAxBlE,IAAOoI,aAA4B;QACnCvG,IAAY9B,QAAQC,IACpBiC,IAAWL,WACfC,GACA0E,kBAAkBvG,GAAMqG;qBAIxB9E,wBACA8E,sBACApF,KAEAoF,0CAA4CpF,GAAUY;aAG9BqC,MAAtBlE;MACFkJ,YAAyBlF,GAAW/B,QAAUiC;eAExC4F,IAAc3J,gBAAgBH,IAC9BiF,IAAOiE,SAAsBlF,GAAW/B,IAE9CiH,UAAuBlF,GAAW/B,QAAUiC;IAC5CgF,YAAyBlF,GAAW/B,QAAUiC,IAE1CgB,cAAcD,IAAO;MACdX,IAAI;WAARyF,IAAWxF,IAAIU,UAAaX,IAAIC,GAAGD,KAAK;YACrC0F,IAAY/E,EAAKX;iBACnB0F,KACFH,oBAAoBxD,GAAK2D,GAAWF;;;WAIxCD,oBAAoBxD,GAAKpB,GAAM6E;;;GC7E1BG,QAYXA,SACElC,GACAmC,GACAC,GACAC,GACA/G;;sBAoDY;;OAEIsD;qBACG;;oBAGR/E;mBAxDMsI,KAAa;6BACHE,KAAuB;cACtC/G,KAAQ;0BACI0E;iBAET;cACFoC,KAAWA,cAAqB;kBAC5BA,KAAWA,kBAAyB;;OAK7CE,mCACAC,IAAeC,qBACfC,IAAmBD;oBAQP;WANZE,IAAYJ,IAAYA,SAAiB;cACzCK,IAAeJ,IAAeA,SAAoB;kBAClDK,IAAmBH,IACrBA,SACA;4BAQa,IACdC,KAAY,WACZC,KAAe,cACfC,KAAmB;2BAGJ;WACT;cACG;kBACI;sBAGC;WACR;cACG;kBACI;;uBJgBDC;WAAwC;MAC3DC,uBAAsB;MACtBC,kBAAkBjI;MAClBkI,cAAa;oBACbH;MACAjG,SAAS,IAAI7D;MACb8D,UAAU/B;MACVmI,SAASnI;MACTgD,OAAO5C;MACPgI,SAAShI;MACTgD,SAAS;;GItBKiD,CAAkBvC,gBAAgB;;;sCAWrCuE;yBACcA;;;uCAGb3H;;OAELtC;;;WAEmCiD,MAA7ByC,eAAe1F;;;MAItBsB;YACUtB,SACN0F,UAAU1F,GAAUsC,KACVW,QAAPiH,IACT5I,IAAM,KAAG4I,IACQjH,QAARkH,MACT7I,IAAM,KAAG6I;aAGKnK,UAAYsB,IAAQ;;;6CAGpB6B,GAA8BnC;MAK5B,UAJZ+B,IACO,SAAXI,KAAqC,uBACjCuC,iBAAiBvC,KACjBA;;;MAEA+E,IAAaD,WAAwBlF,GAAW/B;oBAClDkH,IAAiCA,KAC/BlE,IAAOiE,SAAsBlF,GAAW/B,MAChCgD,IAAO;;;mCAIrBb,GACAiH,GACAvJ;gCAE8BsC,GAAQxC,WAAWyJ,GAAOvJ;;;2CAG1C6G,GAA8BhB;YDnHrBV,GAAcC;QACjCE,IAAYC,iBAAiBH;QAEd;MACnBS,WAAWC,mBAAmBR,GAAWF;MACzCW,WAAWC,aAAaZ;aACxBD;MACAc,kBAAkBd;;wBAIlBZ,GACAA,mBAAqB,UACrBlG,gBAAgBiH;ICuGLT,MAAM2E,mBAAc3C,GAAOhB;;;yCAG1BvD;mBACNJ,IACO,SAAXI,KAAqC,uBACjCuC,iBAAiBvC,KACjBA,cJySoBJ;wDAEtBoB,IAA0B,IAC1BC,IAA6B,IAAIvE;uBAEpBkD;yBAGEoB,GAAYC,GAAerB,GAAW6B;yBACtCT,GAAYC,GAAerB,GAAWiH;;GIjT7B/B,CAA2BlF,KAAa;;;uCAIpEuH,GACA/C;EAEMtB,IAAUoE,mBAAcC,SAAaA;gBAC5B/C,EAAQ7B,eAAeO,mBAEzBP,MAAMO,GAASsE;;;qCAIpBD;cACI5E,MAAM2E,mBAAcC,SAAaA;;;wCAI7CE,GACArH,GACAuD;sBAEoBhB,MAAM8E,GAAcrH,GAAQuD;;;yCAIhD8D,GACAlI,GACAoE;gBAEchB,MAAM8E,GAAclI,GAAMoE;;;IC/K/BpB,6BACXvG,GACA0L;WAEuBxH,MAAnBlE,eAA0D,MAA1BA;;;WAI9B8B,IAAOe,YACT8I,IAAW,GAENrH,IAAI,GAAGC,IAAIvE,oBAAuBsE,IAAIC,GAAGD,KAAK;QAC/CsH,IAAM5L,YAAesE,IACrBL,IAAQ4H,4BAAoBD,SAAWF;YACzCzH,MACFnC,EAAK/B,QAAQ6L,MAAQ3H,GACrB0H;;aAIGA,IAAe7J,IAAO;GAIlB8F,8BACX5H,GACAuL;WAEiCrH,MAA7BlE;WACK;;MAGH8B,IAAmByJ,KAAuB;gDAERG,GAAMI;QACtCZ,IAAOnL,QAAQ+L,aACjB7H,IAAQnC,EAAKoJ;aACHhH,MAAVD;eACuBC,MAArB4H;QACF7H,IAAQ4H,4BAAoBC,gBAAkBhK;;;;;MAM7CoJ,KAAQjH;;MAEZpB;GC5CQkJ,mBAGXA,SAAYxB;gBACIyB,0BAAkBzB;;;sDAGlBtJ,GAAkBY;qBAC1BwJ,IAAQY,SAAStF,aAAa1F,GAAUY,OACd,IACzBqK,uBAAeb;;;qDAGTpK,GAAkBY;WAEjBqC,OADRmH,IAAQY,SAAStF,aAAa1F,GAAUY;YACd;;MACjBsK,sBAAcd,UAAcA,gBAAoBA;4BAC7Ce,MAAWF,uBAAeE;;;6DAGvBnL,GAAkBoL;WAC9BJ,SAAStF,aAAa1F,GAAUoL;;;wDAIzC/L,GACAW;OAEKA,MAAaX;YAAsB;;MACpCW,MAAaX;YAAsB;;MAEjCgM,IAAe3F,oBAAoBrG,IACnCiM,IAAa5F,oBAAoB1F;MAEnCqL;iBACsBC;;GA+C9BC,4BAA4BzF,GAAQ9F;IAClC4C,UACEkD,6CAAqCA,+EACrC,sCACE9F,IACA,2IAEF;IAnDmBqL,GAAchM;mBAChBiM,GAAYtL;oCACKqL,GAAcC;;;IAI9CN,oBACJ1B,GACAtJ,GACAY;mBAEM4K,IAASlC,UAAetJ,IACLA;WAGXiD,WADAuI,cAAmB5K;6CAE/ByE,KACE,+BACEzE,IACA,0BACAZ,IACA,2HAGF;;;;;;0BASoB8F,GAAQ9F;EAChC4C,UACEkD,gFACA,oCACE9F,IACA,6FAEF;;;AC9EJnB,IAAM4M,0BAAkB1M;oBACRQ;;;cAOZR;oBAAsBQ;;;AAJnBV,IAAMuH,4BACXsF;eAEMvF,IAAYuF,mEAMhB,wIAEA;;;;eAQ6C5I,GAAgB/D;EAC7D+D,EAAIhE,QAAQC,MAASA;;;;AAFlBF,IAAMgI,wBAAgB6E;8BACJD,8BAGpB;GAEQ9F,yBACX5G,GACA0L;WAGmBxH;YACV;;OAHA,IAOAI,IAAI,GAAGC,IAAIqI,UAAmBtI,IAAIC,GAAGD,KAAK;QAC3CuI,IAAYD,EAAWtI,IACvB4G,IAAOnL,QAAQ8M,IAGfC,IAAqB,cAAT5B;SACb4B,KAAsB,WAAT5B,OAGZU,IAAMiB,cAAsBA,YAAoB,KAAK,SAC9B,SAAjB9M,QAAQ6L,OAGC,qBADf3H,IAAQ4H,4BAAoBD,SAAWF,OACD,SAAVzH;mBAIbA,KAASA;;;UAGzB;GCfI0E,iBACX1B,GACAC,GACA3D;EAEAD,cAAc2D,QAAY;cACNA,GAAOC,GAAS3D;GAKzBwJ,gBACX9F,GACAC,GACAqE;MAEMnE,IAAYC,iBAAiBH,UAC7B8F,IAAU/F,aAAiBG,cAC3B6F,IAAa9M,gBAAgBiH;MAEd;IACnBK,gBAAgBuF;IAChB1K,WAAW0K;IACXtF,gBAAgB;IAChB7F,WAAW;IACX8F,WAAWC,mBAAmBR,GAAWF;IACzCW,WAAWC,aAAaZ;IACxBgG,UAAS;WACTjG;IACAc,kBAAkBd;;mBAGhB1F,wBACFP,cAAcgM,GAAS5F;MAGdmE,KAAS1I;MAElBmK,MAAY3G,mBAAqB,WAC7B8G,SAAS9G,GAAK2G,GAASC,GAAY1J,KACnC6J,cAAc/G,GAAK2G,GAASC,GAAY1J;SAEvC;IACLgE,cAAc3D;IACdsJ,cAAkBhJ,MAATX,KAAqB,IAAQ8C;IACtC9C,WAAeW,MAATX,IAAqB,OAAOA;;GAIhC4J,oBACJ9G,GACArC,GACA0C,GACA2G;MAEuC;;;MAI1B,IAAI5G,kBAAkBzC,GAAWA,GAAW0C,GAAQL;OACpDxD,yBACKwK;WAEdrN,QAC4BkE,OAAxBlE,IAAOoI,aAA4B;QACnCkF,IAAapN,cAAcF,IAC3BmJ,IAAakE,EAAaC;eAC5BtN,kBAAkD,SAAfmJ,KAC/BG,IAAYxC,WAAWqC,IAC7B5F,EAAK+J,KAAcC,cAAclH,GAAKlG,gBAAgBH,IAAOsJ,MAE7D/F,EAAK+J,KAAcnE;;;GAOnBoE,yBACJlH,GACAK,GACA2G;MAEInI,cAAcmI,IAAe;aACzB7D,IAActE,MAAMmI,WACjB/I,IAAI,GAAGC,IAAI8I,UAAqB/I,IAAIC,GAAGD;MAC9CkF,EAAQlF,KAAKiJ,cAAclH,GAAKK,GAAQ2G,EAAa/I;;;;EAElD,IAAqB,SAAjB+I;;;uBAKOhH,oBAAsBgH,WAKhBnJ,OADhBiF,IAAaiE,cAAc/G,GAAKrC,GAAW0C,GAAQ7D,eACvB,OAAOsG,IAElCgE,SAAS9G,GAAKgH,cAAyB3G,GAAQ2G;GAI7CG,wBACXvG,GACA0B,GACAvE,GACAuD;EAEME,IAAYC,aAAaa;MACzBC,IAAQC,YAAYhB;WAET3D,WADA2D,EAAUe,EAAM;oDAE/BtC,KACE,kIAEA;IAGK;;MAGHrF,IAAWhB,oBAAoB6I;0BACF1E,iBACjCA,eAAoBnD;YAIF,uBACdgG,cAAkB8B;IAAEC,YAAY/H;KAAamD,MAC7CA;oDAGJkC,KACE,gIAEErF,IACA,MACF;IAGK;;mBAGLM,wBACFP,cAAcC,GAAU6H;uBAGLzC;IACnBoB,gBAAgBxG;IAChBqB,WAAW0B;IACX0D,gBAAgB;IAChB7F,WAAW;IACX8F,WAAWA,KAAa;eACxBE;IACAqF,UAAS;WACTjG;IACAc,kBAAkBd;KAICjD,GAAW7D,gBAAgB2I,IAAWjG,eAAe;GAItEuK,gBAAgBK,sBAElBpH,GACArC,GACA0C,GACAnD;2CAGMmK,IAAU1J,MAAciD,aAAiB,UAGzChG,IAAYyM,IAEd1J,IADAkF,WAAwBlF,GAAW;MAEf;IAIxBT,eAAkBtC;QACL,IAAIwF,kBAAkBxF,GAAU+C,GAAW0C,GAAQL;aAE5DrG,GACA2N,KAAY,GACZC,KAAc;UAGV/L,IAAY9B,QAAQC,IACpByI,IAAYlC,kBAAkBvG,GAAMqG,cACpCiH,IAAapN,cAAcF,IAC3BiC,IAAWL,WAAWC,GAAW4G,IACjCU,IAAaD,WAAwBlF,GAAW/B,IAChD4L,aR8GV7J,GACA/B;QAEAuD,mBAAmBxB,GAAW/B;wBA7L9B8B,GACAC,GACA/B;eAC2C,IAElCqC,IAAI,GAAGC,IAAIR,eAAiBO,IAAIC,GAAGD,KAAK;gBAEzCtE,IADa+D,aAAeA,OAASO,QACfN;qBAEfE,MAATlE,KAAsBiC;;;;6BAMtBjC,IAAO+D,WAAaC,MACEhE,SAAOkE;UA8KdpB,qBAAsBkB,GAAW/B;OQlH9BiH,CAAwBlF,GAAW/B,IACjDM,IAAMF,SAAS2B,GAAW/B,IAC5B6L,KAAmB;uBAGrBvM,wBACAwG,KACA9G,KAEA8G,yBAAwC9G,GAAUY;UAKhDkM,YAEE7D,IAAYjD,YAAgBhG;eAElBiD,MAAdgG,KACgC,uBAAfrI;YAIjBwE,mBAAqBpF,GACrBoF,cAAgBrC,GAChBqC,mBAAqB9D,GACrB8D,cAAgBxE;aAIGqC,MAAfiF,MACF5F,EAAK+J,KAAcnE,IAGrB4E,IAAiB7D,EAAUrI,GACzB0B,GACAkF,KAAa5F,YACboE,GACAZ,SAGwBnC,MAAtBlE,mBAGF+N,IAAiBC,sBACf3H,GACApF,GACAY,GACAU,GACApC,gBAAgBH,IACfuD,EAAK+J,MAAwBzK,YAC9BkL;aAKmB7J,MAArB6D,KACmB,SAAnBgG,MACChG,kBAAiC9G,GAAUY;;oBAIrCqC;;;sBAEsBA,MAAtBlE;QAET+N,IAAiB5E,GACjB2E,KAAmB,QACM5J,MAArBX,EAAK+J,MACPzE,sBAAsBtF,GAAM+J,GAAY;UACtCW;2BACSJ,IACHA,EAAYP,UACZpJ;;;sBAOGA,OADPe,IAAOiE,SAAsBlF,GAAW/B;YAE5C8L,IAAiBG,YACf7H,GACApB,GACAhE,GACAY,GACA1B,gBAAgBH,IAChBuD,EAAK+J,UAGkBpJ,MAArBX,EAAK+J,IAA2B;UAClCQ,KAAmB;cACbK,IAAYnO,GACZoO,IAAgBnN;gCACAsC,GAAM+J,GAAY;YACtCW;kBACQI,IAAYnF,SAAsBlF,GAAW/B;kBAC9CoM;uBAYmB,aARHH,YACnB7H,GACAgI,GACAD,GACAvM,GACA1B,gBAAgBgO,SAChBjK,MAGEoK,SACApK;;;;;;QAIqB,wBAA2B,SAAfiF,MAE3C4E,IAAiB5E;;eAQAjF,MAAnB6J,UACqB7J,MAArB6D,KACAA,kBAAiC9G,GAAUY;QAI3C+L,KAAc,GACdrK,EAAK+J,KAAc;aACd;QAAA,SAAuBpJ,MAAnB6J;;oBAEF7J;;;aAGK;eACR4J,MACFvK,EAAK+J,KAAcS;;YAnJO7J,OAAxBlE,IAAOoI;;;;;;IAwJXwF,MAAavH,aAAc;gBACbuH,MAAgBD,SAAYzJ,IAAYX;;KA0HxDyK,iCACJ3H,GACApF,GACAY,GACAU,GACAmE,GACA6H,GACAjH;MAEIpC,cAAcoC,IAAS;;aAKFpD,MAArB6D,KACAA,iBAAgC9G,GAAUY;aACtC0B,IAAW2B,MAAMoC,WACdhD,IAAI,GAAGC,IAAI+C,UAAehD,IAAIC,GAAGD,KAAK;UAEvCkK,IAAcR,sBAClB3H,GACApF,GACAY,GACAQ,SAASE,QAAQ+B,IACjBoC,QAEaxC,MAAbqK,IAAyBA,EAASjK,UAAKJ,GACvCoD,EAAOhD;eAGWJ,MAAhBsK,KAA8BC;QAGhClL,EAAKe,UAAqBJ,MAAhBsK,IAA4BA,IAAc;;;;;;;EAKnD,IAAe,QAAXlH;;;MAEAoH,YAAYpH;oBACKpD,MAAbqK,IAAyB1L,aAAa0L,GAC1B,uBACrBnB,cAAc/G,GAAKiB,GAAQZ,GAAQnD,cAhKzC8C,GACA9D,GACAmE,GACAnD,GACA+D;;8BAGoCA,MAAW/E;UACzCoM,IAAmBrH,cACnBrG,IACJiI,WAAwBlF,GAAW,iBAAiB2K;UAGhC,wBACnBA,KAAoB1N,MAAa0N;iDAGlCrI,KACE,6CACEtC,IACA,+EAEF;;QAQJT,eAAkBtC;YACL,IAAIwF,kBAAkBxF,GAAU+C,GAAW0C,GAAQL;iBAG5DsH,KAAY,GACZC,KAAc,QACc1J,OAAxBlE,IAAOoI,aAA4B;cAEnCvG,IAAY9B,QAAQC,IACpBsN,IAAapN,cAAcF,IAC3BiC,IAAWL,WACfC,GACA0E,kBAAkBvG,GAAMqG,eAEpB9D,IAAMF,SAAS2B,GAAW/B,IAC1BkH,IAAaD,WAAwBlF,GAAW/B,IAChD2M,IAActH,EAAOzF;2BAEvBN,wBAAyCwG,KAAoB9G,KAC/D8G,yBAAwC9G,GAAUY;cAKhDkM;qBACAa,UAAmD1K,MAAtBlE,iBAE/B+N,IAAiBa,SACc1K,MAAtBlE,iBAET+N,IAAiB5E,SACQjF,MAAhB0K,IAETb,IAAiBC,sBACf3H,GACApF,GACAY,GACAU,GACApC,gBAAgBH,IAChBuD,EAAK+J,IACLsB,UAMW1K,OAFPe,IAAOiE,SAAsBlF,GAAW/B,MAG5C8L,IAAiBG,YACf7H,GACApB,GACAhE,GACAY,GACA1B,gBAAgBH,IAChBuD,EAAK+J,MAEwB,wBAA2B,SAAfnE,MAE3C4E,IAAiB5E;mBAQAjF,MAAnB6J,UACqB7J,MAArB6D,KACAA,kBAAiC9G,GAAUY;YAI3C+L,KAAc,GACdrK,EAAK+J,KAAc;iBACd;YAAA,SAAuBpJ,MAAnB6J;;;iBAKG;cACPT,KAAcS;;;QAInBH,MAAavH,aAAc;mBACC9C,SAAZW;;KA+Cd2K,CAAmBxI,GAAK9D,GAAKmE,GAAQnD,GAAM+D;;2CAE/ChB,KACE,2CACE/D,IACA,uGAEF;GAOA2L,uBACJ7H,GACApB,GACAhE,GACAY,GACA6E,GACA6H;MAEIrJ,cAAcD,IAAO;;aAGAf,MAArB6D,KACAA,iBAAgC9G,GAAUY;aACtCiN,IAAc5J,MAAMD,WACjBX,IAAI,GAAGC,IAAIU,UAAaX,IAAIC,GAAGD,KAAK;UACrC0F,IAAYkE,YAChB7H,GACApB,EAAKX,IACLrD,GACAY,GACA6E,QACaxC,MAAbqK,IAAyBA,EAASjK,UAAKJ;eAEvBA,MAAd8F,KAA4ByE;QAG9BK,EAAQxK,UAAmBJ,MAAd8F,IAA0BA,IAAY;;;;;;;EAKlD,gBAAI/E,IACF,OAGAmI,cACL/G,GACApB,GACAyB,QACaxC,MAAbqK,IAAyB1L,aAAa0L;GAKtCG,uBAAe3H;SACN,wBACC,wBAA6C;GC9jBvDgI,2BAAmBC,GAAeC;+BACnCD;IACHE,+BACKF;MACHG,4BACKH;QACHI,cAAcH;;;;GAMdI,wBAAgBL;+BACjBA;IACHrG,OAAO2G,oBAAeN;;GAIlBO,4BAAoBP;;GAGpBQ,4BAAoBR;SACH,YAArBA;GAOIS,4BAAoBT;0BACAA,MAAgC,mBAAzBO,iBAAiBP;GAS5CU,2BACJtI,GACAuI;+BAEGvI;IACH8H,+BACK9H;qBACHuI;;;;;eAwMSX;0BAAuBA;;;eAS1BY;yBAAuBA,aAAeA;;;eADnCA;SAAuB,WAAhBA;;;eAQPA;SAAuB,WAAhBA;;;eAkCDZ;UAAOS,iBAAiBT;;;ICxN9Ba,sCACXtF,GACA5B;MAEMmH,IAA+C,IAC/CC,IAAyC,IACzCC,IAAW,IAAIC,iBAAS1F;gBAG5B5B,GACAuH,0BAAkBF,GAAU;IAC1BG,gBAAOnQ;UACDA,gBAAmB;YACfW,IAAOyP,YAAYJ;eACP;UAChBK,MAAM7P;UACNF,eAAe;YACb+P,MAAM7P;YACN0K,MAAMoF,SAAS3P;;UAEjBuK,MAAMoF,SAAY3P;UAClB4P,cAAcvQ;;;;IAIpBwQ,6BAAoBxQ;MAClB8P,OAAwB9P;;;SAKvB,EAAC8P,GAAoBC;;;iBAmClBU;SAAoB,eAAf1Q,QAAQ0Q;;;iBA8DUC,GAAKC;EACJ,yBAApBA,UACFD,MAAQC;;;;AA/FtB,IAAaC,+BACXrG,GACA5B,GACAkI,GACAC;aAkCoDC,GAAGC;UACvCC,IAAYJ,EAAoBG;;;SADqB,IAMlD1M,IAAI,GAAGC,IAAI0M,UAAkB3M,IAAIC,GAAGD,KAAK;mBAC3B2M,EAAU3M,aACzB4M,IAAenR,QAAQ+I,IACvBqI,IAAgBC,iBAAiBtI,IAG9BuI,IAAI,GAAG9M,IAAI4M,UAAsBE,IAAI9M,GAAG8M,KAAK;YAC9CnG,IAAOiG,EAAcE;cACQnG,OACjCoG,EAAsBpG,KAAQ4F,EAAc5F;;MAKhDqG,EAAoBL,KAAgBpI;aAE7B;QACLuH,MAAM7P;QACN0K,MAAMoF,SAASY;;;;;MAxDvBlB,IAAW,IAAIC,iBAAS1F,IAExB+G,IAGFzO,YAEE0O,IAGF1O,YAGE2O,IAAyC,IAAI1Q;uBAGjD6H,GACAuH,0BAAkBF,GAAU;IAC1BG,OAAO;MACLsB,gBAAOzR;YACAA;cAIC4M,IAAa5M;cAGf4M,aAAsB5M;gBAKpB0R,IADgBC,SAASpH,GAAQyF,aA8BpC,KAEG4B,IAAqBzR,gBAAgBH;gBAGY,MAArD4R,WAA4BF,WACxBA,SAAsBE,KACtB,EACE;cACEvB,MAAM7P;cACN0K,MAAMoF,SAAS;;yCAKpBtQ;0BACH4M;cACA2D,cAAc;gBACZF,MAAM7P;4BACNqR;;;;;;;IAKRC,UAAU;MACRL,gBAAOzR;QACLA,8BAKGwR;;MAELO,gBAAO/R;YAEMuC,GADLyP,IAAc,UAAIhS;aACbuC;UACTyP,OAAiBT,EAAoBhP;;aAClCzC,IAAMyC;UACTyP,OAAiBV,EAAsB/O;;qCAC7BvC;uBAAMgS;;;;;GAOtB1B,oBAAYrM;SAA6B;IAC7CoM,MAAM7P;WACNyD;;GAII0N,oBAAYpH,GAAuByF;EACjCrP,IAAOD,WAAWsP;iCACHrP,KAQdsR,uBAAetR,KAAQ4J,mBAAwB5J,KAAQ,EAACA,gDAP7D2F,KACE,kFACA;EAEK;GAOL8J,uBAAeJ;aACbrP,IAAOD,WAAWsP,kBAEbiC,uBAAetR,4CACxB,iFACA;;GAOEyQ,4BAAoBpR;MAClB4I,IAAkB;gBAElB5I,GAAM;IACVkS,yBAAgBC;MACdvJ,OAAW7I,QAAQoS;;;;;;;;iCD9MKC;kBAAwC/R;eA6M7DuP;0CAEOyC,IAAS9C,iBAAiBnI;UACA;QAC9BA,WAAW2H,gBAAgB3H,GAAW6H;QACtC1L,MAAMqM;QACNnO,OAAOmO;QACP0C,YAAY1C;;UAID,wBAAXyC,KACY,kBAAXA,KAAwC,cAAZpD;QAE7B3H,WAAe,GACfiL,qBACE7C,gBAAgBtI,GAAW;;;;;UAzN5BgL,IAAO;QAEZnL,IAAQ,IAAIgD,MAChBmI,WAAc,IAAIrG,iBAAiBqG,iBAAelO,GAClDkO,aACAA,WACAA,cACAA;QAIEA,WAAc;UACVnM,IAAUmM;UAChBI,IAAYvM,0BAAoBC;QAC9BF,YAAYiB,QAAYhB,GAASC;;;QAI/BuM,IAA+B,IAAIrP,KACnCsP,IAAoB,IAAItP,KACxBuP,IAA4B9P,YAE5B+P,aACJC,GACAtL;WAEqBrD,MAAjBqD,KAEFA,sBAAqBuL;YACbzP,IAAOsP,EAAKG;iBACL5O,MAATb,GAAoB;UACtBsP,EAAKG,KAAO;cACC;eAAR/I,IAAWxF,IAAIlB,UAAaiB,IAAIC,GAAGD;YACtCuO,MAAsBxP,EAAKiB;;;;OAO/ByO,aACJ3L,GACAyL;MAGAA,oBAA0BtQ;YACpBA,MAAQ6E,OAAe;cACnB4H,IAAK0D,MAAQnQ;qBACfyM,MACF0D,SAAWnQ,IACXgQ,qBAA0B7C,gBAAgBV,GAAI;;;OAOhDgE,aAAoB5L;mBAtFE4H;wBATDA;iBACN,eAArBA;UAS2BA,MAAgC,mBAAzBO,iBAAiBP;OAsF7CiE,CAAqB7L,IAAY;2BAEVc,gBAAgBjB,GAAOG,GAAW7E;cACvDgF,WACFkL,MAAiClQ,GAAKgF,IAChCsL,IAAoB,IAAI/R,KAC9B8R,EAAyBC,GAAmBtL,IAC5CwL,EAAyB3L,GAAWyL;;OAMpCrN,aAAsBwJ,GAAezH;MACzCA,oBAAqBuL;SACNH,EAAKG,OAASH,EAAKG,KAAO,UAC7B9D;cAEGA,UACX0D,MACE1D,OACyB,mBAAzBO,iBAAiBP,KACbU,gBAAgBV,GAAI,uBACpBA;;OAQNkE,aACJ9L;cAEwCuB,MAAM1B,GAAOG;;eAGjD7D,IACF6L,IAAe,UAEf5J,EAAmB4B,GAAWG,IAC9B6H,IACGlC,KAA2C,iBAAhCqC,iBAAiBnI,KAEzB,YADA;aAID;QACL6H,SAASG;mBACThI;cACA7D;;OAKE4P,aAAyB7L;0DAEvBoG,IAAU8B,iBAAiBpI,2BAK3ByL,IAAoB,IAAI/R;QAE5B+R,GACAJ,MAAiClQ;eAECA;gBT2MRgB,GAAoBC;eAE3CD,UAAaC;6BACCD,WAAcC;6BACdD,SAAYC;QS9MfyD,QAAY1E;UAIf,QAATgB,GAAqC;QACvC6P,IAAAA,IAAoBpM,MAAMC,GAAOG,GAAW7D;YAExCmK,GAAS;eACL2F,IAAc1K,MAAM1B,GAAOG;cAEjCkM,IAAoBD;;cAEb1K,MAAM1B,GAAOG,GAAW7D;;;MAKnCqP,EAAyBC,GAAmBO;WAE1CR,EAAyBC,GAAmBS;QAIrBhM,aAAkBuL;gBAGN3O,MAAtBoP,KACb9N,EAAmB8B,aAAkBgM;aAGhC;cAAE/P;eAAM9B;oBAAO6Q;mBAAYlL;;;oBAG7BmM;MACCC,IAAwBC,YAANF;UAIlBG,IAAelB,IAKfmB,eAASC,gBAATD,CADAE,WAAK,EAALA,CADAC,aAAOC,kBAAYvB,GAAnBsB,CADAN,OAKDQ;UAMHP,YADAQ,UAAIjB,EAAJiB,CADAlQ,UAAIsL,aAAJtL,CADAmQ,aAAO,EAACR,GAAcF;UAOlBW,IAIJV,YADA1P,UAAImP,EAAJnP,CADAqQ,mBAAAA,CADAC;UAUAtQ,gBAAAA,CADAqQ,mBAAAA,CADAD;UAUApQ,YAAAA,CADAqQ,mBAAAA,CADAD;UAwCApQ,UAAIoP,EAAJpP,CATAuQ,EACEC,YAAM,EAGFH,mBAAAA,CADAC,IAGFG;yBAMO,EAACC,GAASC;;;;;;;;;oCCvUMrU;;kBAEMA;eAmByBsU;mBACnCA;;uBAnBrBpK,IAASyB,0BAAkB4I,IAE3BC,IAAmB,IAAI/T,KAEvBgU,IAAmB,IAAIhU,KAEvBgQ,IAAiCjO,YAEjCgO,IAAuChO,YAGvCkS,aAA0B/F;UACL,eAArBA;;;UAKO9D,GADL8J,IAAoCnS;WAC/BqI;QACT8J,EAAiB9J,KAAQ2F,EAAoB3F;;mCAM1C8D;QACHrG,OAAOiI,oBACLrG,GACAyE,SACAgG,GACAlE;;OAMAmE,aAAuB5U;;UACL,gCAItByU,MAAqBvS,KACjBsS,MAAqBtS;QAIzBsS,MAAqBtS;iBAEsBsN,2BACzCtF,GACA5B;;aAdmE,IAiB5DrE,IAAI,GAAGC,IAAIuL,UAA2BxL,IAAIC,GAAGD,KAAK;cACnDwE,IAAWgH,EAAmBxL;YACtBvE,QAAQ+I,MAAaA;;QAG5BxE,IAAI;aAAGC,IAAIwL,UAAqBzL,IAAIC,GAAGD;UAExC3D,IAAOZ,SADP+I,IAAWiH,EAAazL,oBAExB4Q,IACJrE,EAAoBlQ,OAAUkQ,EAAoBlQ,KAAQ,KAE3DmI,gBAA+BoM;UAChCA,OAAa;iBAAE3S;sBAAKuG;;;;OAIlBqM,aAA0B9U;wCAE5ByU;;oBAIGvB;eAKHxP,UAAIgR,EAAJhR,CADAkQ,UAAIkB,EAAJlB,CADAA,UAAIgB,EAAJhB,CADAV;;;;;;;;;;;;;"}