{"version":3,"file":"urql-exchange-graphcache.es.js","sources":["../src/ast/node.ts","../src/helpers/help.ts","../src/store/keys.ts","../src/store/timing.ts","../src/store/data.ts","../src/operations/shared.ts","../src/ast/traversal.ts","../src/operations/write.ts","../src/store/store.ts","../src/operations/invalidate.ts","../src/operations/query.ts","../src/ast/variables.ts","../src/ast/schemaPredicates.ts","../src/cacheExchange.ts","../src/populateExchange.ts"],"sourcesContent":["import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  GraphQLOutputType,\n  Kind,\n  isWrappingType,\n} from 'graphql';\n\nimport { SelectionSet, GraphQLFlatType } from '../types';\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias !== undefined ? node.alias.value : getName(node);\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet =>\n  node.selectionSet !== undefined ? node.selectionSet.selections : [];\n\nexport const getTypeCondition = ({\n  typeCondition,\n}: {\n  typeCondition?: NamedTypeNode;\n}): string | null =>\n  typeCondition !== undefined ? getName(typeCondition) : null;\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n\nexport const unwrapType = (\n  type: null | undefined | GraphQLOutputType\n): GraphQLFlatType | null => {\n  if (isWrappingType(type)) {\n    return unwrapType(type.ofType);\n  }\n\n  return type || null;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/help.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\nimport { ErrorCode } from '../types';\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\nconst helpUrl =\n  '\\nhttps://github.com/FormidableLabs/urql-exchange-graphcache/blob/master/docs/help.md#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import { stringifyVariables } from 'urql/core';\nimport { Variables, FieldInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: null | Variables) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\n/** Prefix key with its owner type Link / Record */\nexport const prefixKey = (owner: 'l' | 'r', key: string) => `${owner}|${key}`;\n","export const defer: (fn: () => void) => void =\n  process.env.NODE_ENV === 'production' && typeof Promise !== 'undefined'\n    ? Promise.prototype.then.bind(Promise.resolve())\n    : fn => setTimeout(fn, 0);\n","import {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n} from '../types';\nimport { invariant, currentDebugStack } from '../helpers/help';\nimport { fieldInfoOfKey, joinKeys, prefixKey } from './keys';\nimport { defer } from './timing';\n\nimport { observable } from 'mobx';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OptimisticMap<T> = Record<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OptimisticMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n  keys: number[];\n}\n\nexport interface InMemoryData {\n  persistenceScheduled: boolean;\n  persistenceBatch: SerializedEntries;\n  gcScheduled: boolean;\n  gcBatch: Set<string>;\n  queryRootKey: string;\n  refCount: Dict<number>;\n  refLock: OptimisticMap<Dict<number>>;\n  records: NodeMap<EntityField>;\n  links: NodeMap<Link>;\n  storage: StorageAdapter | null;\n}\n\nexport const makeDict = (): any => observable({});\n\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Set<string> = null;\nlet currentOptimisticKey: null | number = null;\n\nconst makeNodeMap = <T>(): NodeMap<T> => ({\n  optimistic: makeDict(),\n  base: new Map(),\n  keys: [],\n});\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  data: InMemoryData,\n  optimisticKey: number | null\n) => {\n  //@ts-ignore\n  window.currentData = currentData = data;\n  currentDependencies = new Set();\n  currentOptimisticKey = optimisticKey;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  const data = currentData!;\n\n  if (!data.gcScheduled && data.gcBatch.size > 0) {\n    data.gcScheduled = true;\n    defer(() => {\n      gc(data);\n    });\n  }\n\n  if (data.storage && !data.persistenceScheduled) {\n    data.persistenceScheduled = true;\n    defer(() => {\n      data.storage!.write(data.persistenceBatch);\n      data.persistenceScheduled = false;\n      data.persistenceBatch = makeDict();\n    });\n  }\n\n  currentData = null;\n  currentDependencies = null;\n  currentOptimisticKey = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Set<string> => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  persistenceScheduled: false,\n  persistenceBatch: makeDict(),\n  gcScheduled: false,\n  queryRootKey,\n  gcBatch: new Set(),\n  refCount: makeDict(),\n  refLock: makeDict(),\n  links: makeNodeMap(),\n  records: makeNodeMap(),\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  let keymap: KeyMap<Dict<T | undefined>>;\n  if (currentOptimisticKey) {\n    // If the optimistic map doesn't exist yet, it' created, and\n    // the optimistic key is stored (in order of priority)\n    if (map.optimistic[currentOptimisticKey] === undefined) {\n      map.optimistic[currentOptimisticKey] = new Map();\n      map.keys.unshift(currentOptimisticKey);\n    }\n\n    keymap = map.optimistic[currentOptimisticKey];\n  } else {\n    keymap = map.base;\n  }\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    const node = optimistic.get(entityKey);\n    // If the node and node value exists it is returned, including undefined\n    if (node !== undefined && fieldKey in node) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  const node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNodeParent = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): Record<string, T | undefined> | undefined => {\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    const node = optimistic.get(entityKey);\n    // If the node and node value exists it is returned, including undefined\n    if (node !== undefined && fieldKey in node) {\n      return node;\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  const node = map.base.get(entityKey);\n  return node !== undefined ? node : undefined;\n};\n\n/** Clears an optimistic layers from a NodeMap */\nconst clearOptimisticNodes = <T>(map: NodeMap<T>, optimisticKey: number) => {\n  // Check whether the optimistic layer exists on the NodeMap\n  const index = map.keys.indexOf(optimisticKey);\n  if (index > -1) {\n    // Then delete it and splice out the optimisticKey\n    delete map.optimistic[optimisticKey];\n    map.keys.splice(index, 1);\n  }\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gcBatch */\nconst updateRCForEntity = (\n  gcBatch: void | Set<string>,\n  refCount: Dict<number>,\n  entityKey: string,\n  by: number\n) => {\n  // Retrieve the reference count\n  const count = refCount[entityKey] !== undefined ? refCount[entityKey] : 0;\n  // Adjust it by the \"by\" value\n  const newCount = (refCount[entityKey] = (count + by) | 0);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gcBatch !== undefined) {\n    if (newCount <= 0) gcBatch.add(entityKey);\n    else if (count <= 0 && newCount > 0) gcBatch.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gcBatch */\nconst updateRCForLink = (\n  gcBatch: void | Set<string>,\n  refCount: Dict<number>,\n  link: Link | undefined,\n  by: number\n) => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gcBatch, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      const entityKey = link[i];\n      if (entityKey) {\n        updateRCForEntity(gcBatch, refCount, entityKey, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n) => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = map.keys.length; i < l; i++) {\n    const optimistic = map.optimistic[map.keys[i]];\n    extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = (data: InMemoryData) => {\n  // Reset gcScheduled flag\n  data.gcScheduled = false;\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  data.gcBatch.forEach(entityKey => {\n    // Check first whether the reference count is still 0\n    const rc = data.refCount[entityKey] || 0;\n    if (rc <= 0) {\n      // Each optimistic layer may also still contain some references to marked entities\n      for (const optimisticKey in data.refLock) {\n        const refCount = data.refLock[optimisticKey];\n        const locks = refCount[entityKey] || 0;\n        // If the optimistic layer has any references to the entity, don't GC it,\n        // otherwise delete the reference count from the optimistic layer\n        if (locks > 0) return;\n        delete refCount[entityKey];\n      }\n\n      // All conditions are met: The entity can be deleted\n\n      // Delete the reference count, and delete the entity from the GC batch\n      delete data.refCount[entityKey];\n      data.gcBatch.delete(entityKey);\n\n      // Delete the record and for each of its fields, delete them on the persistence\n      // layer if one is present\n      // No optimistic data needs to be deleted, as the entity is not being referenced by\n      // anything and optimistic layers will eventually be deleted anyway\n      const recordsNode = data.records.base.get(entityKey);\n      if (recordsNode !== undefined) {\n        data.records.base.delete(entityKey);\n        if (data.storage) {\n          for (const fieldKey in recordsNode) {\n            const key = prefixKey('r', joinKeys(entityKey, fieldKey));\n            data.persistenceBatch[key] = undefined;\n          }\n        }\n      }\n\n      // Delete all the entity's links, but also update the reference count\n      // for those links (which can lead to an unrolled recursive GC of the children)\n      const linkNode = data.links.base.get(entityKey);\n      if (linkNode !== undefined) {\n        data.links.base.delete(entityKey);\n        for (const fieldKey in linkNode) {\n          // Delete all links from the persistence layer if one is present\n          if (data.storage) {\n            const key = prefixKey('l', joinKeys(entityKey, fieldKey));\n            data.persistenceBatch[key] = undefined;\n          }\n\n          updateRCForLink(data.gcBatch, data.refCount, linkNode[fieldKey], -1);\n        }\n      }\n    } else {\n      data.gcBatch.delete(entityKey);\n    }\n  });\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies!.add(entityKey);\n    } else if (fieldKey !== undefined) {\n      currentDependencies!.add(joinKeys(entityKey, fieldKey));\n    }\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\nexport const readParent = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNodeParent(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n  if (currentData!.storage && !currentOptimisticKey) {\n    const key = prefixKey('r', joinKeys(entityKey, fieldKey));\n    currentData!.persistenceBatch[key] = value;\n  }\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: Dict<number>;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gcBatch: void | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount =\n      data.refLock[currentOptimisticKey] ||\n      (data.refLock[currentOptimisticKey] = makeDict());\n    links = data.links.optimistic[currentOptimisticKey];\n  } else {\n    if (data.storage) {\n      const key = prefixKey('l', joinKeys(entityKey, fieldKey));\n      data.persistenceBatch[key] = link;\n    }\n    refCount = data.refCount;\n    links = data.links.base;\n    gcBatch = data.gcBatch;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links !== undefined ? links.get(entityKey) : undefined;\n  const prevLink = prevLinkNode !== undefined ? prevLinkNode[fieldKey] : null;\n\n  // Update dependencies\n  updateDependencies(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gcBatch, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gcBatch, refCount, link, 1);\n};\n\n/** Removes an optimistic layer of links and records */\nexport const clearOptimistic = (data: InMemoryData, optimisticKey: number) => {\n  // We also delete the optimistic reference locks\n  delete data.refLock[optimisticKey];\n  clearOptimisticNodes(data.records, optimisticKey);\n  clearOptimisticNodes(data.links, optimisticKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState(data, 0);\n  for (const key in entries) {\n    const dotIndex = key.indexOf('.');\n    const entityKey = key.slice(2, dotIndex);\n    const fieldKey = key.slice(dotIndex + 1);\n    switch (key.charCodeAt(0)) {\n      case 108:\n        writeLink(entityKey, fieldKey, entries[key] as Link);\n        break;\n      case 114:\n        writeRecord(entityKey, fieldKey, entries[key]);\n        break;\n    }\n  }\n  clearDataState();\n  data.storage = storage;\n};\n","import { FieldNode, InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport { warn, pushDebugNode } from '../helpers/help';\nimport { hasField } from '../store/data';\nimport { Store, keyOfField } from '../store';\n\nimport {\n  Fragments,\n  Variables,\n  SelectionSet,\n  DataField,\n  NullArray,\n  Data,\n} from '../types';\n\nimport {\n  SchemaPredicates,\n  getTypeCondition,\n  getFieldArguments,\n  shouldInclude,\n  isFieldNode,\n  isInlineFragment,\n  getSelectionSet,\n  getName,\n} from '../ast';\n\ninterface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  ctx: Context\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return !getSelectionSet(node).some(node => {\n    if (!isFieldNode(node)) return false;\n    const fieldKey = keyOfField(\n      getName(node),\n      getFieldArguments(node, ctx.variables)\n    );\n    return !hasField(entityKey, fieldKey);\n  });\n};\n\nexport class SelectionIterator {\n  typename: void | string;\n  entityKey: string;\n  indexStack: number[];\n  context: Context;\n  selectionStack: SelectionSet[];\n\n  constructor(\n    typename: void | string,\n    entityKey: string,\n    select: SelectionSet,\n    ctx: Context\n  ) {\n    this.typename = typename;\n    this.entityKey = entityKey;\n    this.context = ctx;\n    this.indexStack = [0];\n    this.selectionStack = [select];\n  }\n\n  next(): void | FieldNode {\n    while (this.indexStack.length !== 0) {\n      const index = this.indexStack[this.indexStack.length - 1]++;\n      const select = this.selectionStack[this.selectionStack.length - 1];\n      if (index >= select.length) {\n        this.indexStack.pop();\n        this.selectionStack.pop();\n        continue;\n      } else {\n        const node = select[index];\n        if (!shouldInclude(node, this.context.variables)) {\n          continue;\n        } else if (!isFieldNode(node)) {\n          // A fragment is either referred to by FragmentSpread or inline\n          const fragmentNode = !isInlineFragment(node)\n            ? this.context.fragments[getName(node)]\n            : node;\n\n          if (fragmentNode !== undefined) {\n            if (process.env.NODE_ENV !== 'production') {\n              pushDebugNode(this.typename, fragmentNode);\n            }\n\n            const isMatching =\n              this.context.schemaPredicates !== undefined\n                ? this.context.schemaPredicates.isInterfaceOfType(\n                    getTypeCondition(fragmentNode),\n                    this.typename\n                  )\n                : isFragmentHeuristicallyMatching(\n                    fragmentNode,\n                    this.typename,\n                    this.entityKey,\n                    this.context\n                  );\n\n            if (isMatching) {\n              this.indexStack.push(0);\n              this.selectionStack.push(getSelectionSet(fragmentNode));\n            }\n          }\n\n          continue;\n        } else if (getName(node) === '__typename') {\n          continue;\n        } else {\n          return node;\n        }\n      }\n    }\n\n    return undefined;\n  }\n}\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x === undefined ? null : (x as Data | NullArray<Data>);\n","import {\n  SelectionNode,\n  DefinitionNode,\n  DocumentNode,\n  FragmentDefinitionNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { invariant } from '../helpers/help';\nimport { getName } from './node';\nimport { Fragments, Variables } from '../types';\n\nconst isFragmentNode = (node: DefinitionNode): node is FragmentDefinitionNode =>\n  node.kind === Kind.FRAGMENT_DEFINITION;\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  const operation = doc.definitions.find(\n    node => node.kind === Kind.OPERATION_DEFINITION\n  ) as OperationDefinitionNode;\n\n  invariant(\n    !!operation,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n\n  return operation;\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments =>\n  doc.definitions.filter(isFragmentNode).reduce((map: Fragments, node) => {\n    map[getName(node)] = node;\n    return map;\n  }, {});\n\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  const { directives } = node;\n  if (directives === undefined) {\n    return true;\n  }\n\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0, l = directives.length; i < l; i++) {\n    const directive = directives[i];\n    const name = getName(directive);\n\n    // Ignore other directives\n    const isInclude = name === 'include';\n    if (!isInclude && name !== 'skip') continue;\n\n    // Get the first argument and expect it to be named \"if\"\n    const arg = directive.arguments ? directive.arguments[0] : null;\n    if (!arg || getName(arg) !== 'if') continue;\n\n    const value = valueFromASTUntyped(arg.value, vars);\n    if (typeof value !== 'boolean' && value !== null) continue;\n\n    // Return whether this directive forces us to skip\n    // `@include(if: false)` or `@skip(if: true)`\n    return isInclude ? !!value : !value;\n  }\n\n  return true;\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\n\nimport {\n  getFieldAlias,\n  getFragments,\n  getMainOperation,\n  getSelectionSet,\n  normalizeVariables,\n  getFragmentTypeName,\n  getName,\n  getFieldArguments,\n  SchemaPredicates,\n} from '../ast';\n\nimport {\n  NullArray,\n  Fragments,\n  Variables,\n  Data,\n  Link,\n  SelectionSet,\n  OperationRequest,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  makeDict,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { invariant, warn, pushDebugNode } from '../helpers/help';\nimport { SelectionIterator, ensureData } from './shared';\n\nexport interface WriteResult {\n  dependencies: Set<string>;\n}\n\ninterface Context {\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  fieldName: string;\n  result: WriteResult;\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  optimistic?: boolean;\n  schemaPredicates?: SchemaPredicates;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data\n): WriteResult => {\n  initDataState(store.data, 0);\n  const result = startWrite(store, request, data);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { dependencies: getCurrentDependencies() };\n\n  const select = getSelectionSet(operation);\n  const operationName = store.getRootKey(operation.operation);\n\n  const ctx: Context = {\n    parentTypeName: operationName,\n    parentKey: operationName,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    result,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  if (operationName === ctx.store.getRootKey('query')) {\n    writeSelection(ctx, operationName, select, data);\n  } else {\n    writeRoot(ctx, operationName, select, data);\n  }\n\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  optimisticKey: number\n): WriteResult => {\n  initDataState(store.data, optimisticKey);\n\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { dependencies: getCurrentDependencies() };\n\n  const mutationRootKey = store.getRootKey('mutation');\n  const operationName = store.getRootKey(operation.operation);\n  invariant(\n    operationName === mutationRootKey,\n    'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n      'This case is unsupported and should never occur.',\n    10\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  const ctx: Context = {\n    parentTypeName: mutationRootKey,\n    parentKey: mutationRootKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    result,\n    store,\n    schemaPredicates: store.schemaPredicates,\n    optimistic: true,\n  };\n\n  const data = makeDict();\n  const iter = new SelectionIterator(\n    operationName,\n    operationName,\n    getSelectionSet(operation),\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    if (node.selectionSet !== undefined) {\n      const fieldName = getName(node);\n      const resolver = ctx.store.optimisticMutations[fieldName];\n\n      if (resolver !== undefined) {\n        // We have to update the context to reflect up-to-date ResolveInfo\n        ctx.fieldName = fieldName;\n\n        const fieldArgs = getFieldArguments(node, ctx.variables);\n        const resolverValue = resolver(fieldArgs || makeDict(), ctx.store, ctx);\n        const resolverData = ensureData(resolverValue);\n        writeRootField(ctx, resolverData, getSelectionSet(node));\n        data[fieldName] = resolverValue;\n        const updater = ctx.store.updates[mutationRootKey][fieldName];\n        if (updater !== undefined) {\n          updater(data, fieldArgs || makeDict(), ctx.store, ctx);\n        }\n      }\n    }\n  }\n\n  clearDataState();\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Data,\n  variables?: Variables\n) => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    return warn(\n      'writeFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      11\n    );\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const writeData = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(writeData);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx: Context = {\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: variables || {},\n    fragments,\n    result: { dependencies: getCurrentDependencies() },\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), writeData);\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.getRootKey('query');\n  const typename = isQuery ? entityKey : data.__typename;\n  if (typeof typename !== 'string') return;\n\n  InMemoryData.writeRecord(entityKey, '__typename', typename);\n\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldValue = data[getFieldAlias(node)];\n    const key = joinKeys(entityKey, fieldKey);\n\n    if (process.env.NODE_ENV !== 'production') {\n      if (fieldValue === undefined) {\n        const advice = ctx.optimistic\n          ? '\\nYour optimistic result may be missing a field!'\n          : '';\n\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.' +\n            advice,\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.schemaPredicates && typename) {\n        ctx.schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n      }\n    }\n\n    if (node.selectionSet === undefined) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(entityKey, fieldKey, fieldValue);\n    } else {\n      // Process the field and write links for the child entities that have been written\n      const fieldData = ensureData(fieldValue);\n      const link = writeField(ctx, key, getSelectionSet(node), fieldData);\n      InMemoryData.writeLink(entityKey, fieldKey, link);\n    }\n  }\n};\n\nconst writeField = (\n  ctx: Context,\n  parentFieldKey: string,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>\n): Link => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      const item = data[i];\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = joinKeys(parentFieldKey, `${i}`);\n      // Recursively write array data\n      const links = writeField(ctx, indexKey, select, item);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n    }\n\n    return newData;\n  } else if (data === null) {\n    return null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const key = entityKey !== null ? entityKey : parentFieldKey;\n  const typename = data.__typename;\n\n  if (\n    ctx.store.keys[data.__typename] === undefined &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !typename.endsWith('Connection') &&\n    !typename.endsWith('Edge') &&\n    typename !== 'PageInfo'\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  writeSelection(ctx, key, select, data);\n  return key;\n};\n\n// This is like writeSelection but assumes no parent entity exists\nconst writeRoot = (\n  ctx: Context,\n  typename: string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isRootField =\n    typename === ctx.store.getRootKey('mutation') ||\n    typename === ctx.store.getRootKey('subscription');\n\n  const iter = new SelectionIterator(typename, typename, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = joinKeys(typename, keyOfField(fieldName, fieldArgs));\n    if (node.selectionSet !== undefined) {\n      const fieldValue = ensureData(data[getFieldAlias(node)]);\n      writeRootField(ctx, fieldValue, getSelectionSet(node));\n    }\n\n    if (isRootField) {\n      // We have to update the context to reflect up-to-date ResolveInfo\n      ctx.parentTypeName = typename;\n      ctx.parentKey = typename;\n      ctx.parentFieldKey = fieldKey;\n      ctx.fieldName = fieldName;\n\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater !== undefined) {\n        updater(data, fieldArgs || makeDict(), ctx.store, ctx);\n      }\n    }\n  }\n};\n\n// This is like writeField but doesn't fall back to a generated key\nconst writeRootField = (\n  ctx: Context,\n  data: null | Data | NullArray<Data>,\n  select: SelectionSet\n) => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++)\n      newData[i] = writeRootField(ctx, data[i], select);\n    return newData;\n  } else if (data === null) {\n    return;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(data);\n  if (entityKey !== null) {\n    writeSelection(ctx, entityKey, select, data);\n  } else {\n    const typename = data.__typename;\n    writeRoot(ctx, typename, select, data);\n  }\n};\n","import { DocumentNode } from 'graphql';\nimport { createRequest } from 'urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  Data,\n  QueryInput,\n  UpdatesConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n} from '../types';\n\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidate } from '../operations/invalidate';\nimport { SchemaPredicates } from '../ast';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport class Store implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: UpdatesConfig;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schemaPredicates?: SchemaPredicates;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(\n    schemaPredicates?: SchemaPredicates,\n    resolvers?: ResolverConfig,\n    updates?: Partial<UpdatesConfig>,\n    optimisticMutations?: OptimisticMutationConfig,\n    keys?: KeyingConfig\n  ) {\n    this.resolvers = resolvers || {};\n    this.optimisticMutations = optimisticMutations || {};\n    this.keys = keys || {};\n    this.schemaPredicates = schemaPredicates;\n\n    this.updates = {\n      Mutation: (updates && updates.Mutation) || {},\n      Subscription: (updates && updates.Subscription) || {},\n    } as UpdatesConfig;\n\n    if (schemaPredicates) {\n      const { schema } = schemaPredicates;\n      const queryType = schema.getQueryType();\n      const mutationType = schema.getMutationType();\n      const subscriptionType = schema.getSubscriptionType();\n\n      const queryName = queryType ? queryType.name : 'Query';\n      const mutationName = mutationType ? mutationType.name : 'Mutation';\n      const subscriptionName = subscriptionType\n        ? subscriptionType.name\n        : 'Subscription';\n\n      this.rootFields = {\n        query: queryName,\n        mutation: mutationName,\n        subscription: subscriptionName,\n      };\n\n      this.rootNames = {\n        [queryName]: 'query',\n        [mutationName]: 'mutation',\n        [subscriptionName]: 'subscription',\n      };\n    } else {\n      this.rootFields = {\n        query: 'Query',\n        mutation: 'Mutation',\n        subscription: 'Subscription',\n      };\n\n      this.rootNames = {\n        Query: 'query',\n        Mutation: 'mutation',\n        Subscription: 'subscription',\n      };\n    }\n\n    this.data = InMemoryData.make(this.getRootKey('query'));\n  }\n\n  gcScheduled = false;\n  gc = () => {\n    InMemoryData.gc(this.data);\n    this.gcScheduled = false;\n  };\n\n  keyOfField = keyOfField;\n\n  getRootKey(name: RootField) {\n    return this.rootFields[name];\n  }\n\n  keyOfEntity(data: Data) {\n    const { __typename: typename, id, _id } = data;\n    if (!typename) {\n      return null;\n    } else if (this.rootNames[typename] !== undefined) {\n      return typename;\n    }\n\n    let key: string | null | void;\n    if (this.keys[typename]) {\n      key = this.keys[typename](data);\n    } else if (id !== undefined && id !== null) {\n      key = `${id}`;\n    } else if (_id !== undefined && _id !== null) {\n      key = `${_id}`;\n    }\n\n    return key ? `${typename}:${key}` : null;\n  }\n\n  resolveFieldByKey(entity: Data | string | null, fieldKey: string): DataField {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n    if (entityKey === null) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link ? link : null;\n  }\n\n  resolve(\n    entity: Data | string | null,\n    field: string,\n    args?: Variables\n  ): DataField {\n    return this.resolveFieldByKey(entity, keyOfField(field, args));\n  }\n\n  invalidateQuery(query: string | DocumentNode, variables?: Variables) {\n    invalidate(this, createRequest(query, variables));\n  }\n\n  inspectFields(entity: Data | string | null): FieldInfo[] {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n    return entityKey !== null ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery(\n    input: QueryInput,\n    updater: (data: Data | null) => Data | null\n  ): void {\n    const request = createRequest(input.query, input.variables);\n    const output = updater(this.readQuery(request as QueryInput));\n    if (output !== null) {\n      startWrite(this, request, output);\n    }\n  }\n\n  readQuery(input: QueryInput): Data | null {\n    return read(this, createRequest(input.query, input.variables)).data;\n  }\n\n  readFragment(\n    dataFragment: DocumentNode,\n    entity: string | Data,\n    variables?: Variables\n  ): Data | null {\n    return readFragment(this, dataFragment, entity, variables);\n  }\n\n  writeFragment(\n    dataFragment: DocumentNode,\n    data: Data,\n    variables?: Variables\n  ): void {\n    writeFragment(this, dataFragment, data, variables);\n  }\n}\n","import { FieldNode } from 'graphql';\n\nimport {\n  getMainOperation,\n  normalizeVariables,\n  getFragments,\n  getSelectionSet,\n  getName,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  EntityField,\n  OperationRequest,\n  Variables,\n  Fragments,\n  SelectionSet,\n} from '../types';\n\nimport * as InMemoryData from '../store/data';\nimport { Store, keyOfField } from '../store';\nimport { SchemaPredicates } from '../ast';\nimport { SelectionIterator } from './shared';\n\ninterface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nexport const invalidate = (store: Store, request: OperationRequest) => {\n  const operation = getMainOperation(request.query);\n\n  const ctx: Context = {\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  invalidateSelection(\n    ctx,\n    ctx.store.getRootKey('query'),\n    getSelectionSet(operation)\n  );\n};\n\nexport const invalidateSelection = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet\n) => {\n  const isQuery = entityKey === 'Query';\n\n  let typename: EntityField;\n  if (!isQuery) {\n    typename = InMemoryData.readRecord(entityKey, '__typename');\n    if (typeof typename !== 'string') {\n      return;\n    } else {\n      InMemoryData.writeRecord(entityKey, '__typename', undefined);\n    }\n  } else {\n    typename = entityKey;\n  }\n\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldName = getName(node);\n    const fieldKey = keyOfField(\n      fieldName,\n      getFieldArguments(node, ctx.variables)\n    );\n\n    if (\n      process.env.NODE_ENV !== 'production' &&\n      ctx.schemaPredicates &&\n      typename\n    ) {\n      ctx.schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    if (node.selectionSet === undefined) {\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n    } else {\n      const fieldSelect = getSelectionSet(node);\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      InMemoryData.writeLink(entityKey, fieldKey, undefined);\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n\n      if (Array.isArray(link)) {\n        for (let i = 0, l = link.length; i < l; i++) {\n          const childLink = link[i];\n          if (childLink !== null) {\n            invalidateSelection(ctx, childLink, fieldSelect);\n          }\n        }\n      } else if (link) {\n        invalidateSelection(ctx, link, fieldSelect);\n      }\n    }\n  }\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { action } from 'mobx';\nimport {\n  getFragments,\n  getMainOperation,\n  getSelectionSet,\n  normalizeVariables,\n  getName,\n  getFieldArguments,\n  getFieldAlias,\n  getFragmentTypeName,\n} from '../ast';\n\nimport {\n  Fragments,\n  Variables,\n  Data,\n  DataField,\n  Link,\n  SelectionSet,\n  OperationRequest,\n  NullArray,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  // clearDataState,\n  makeDict,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode } from '../helpers/help';\nimport { SelectionIterator, ensureData } from './shared';\nimport { SchemaPredicates } from '../ast';\n\nexport interface QueryResult {\n  dependencies: Set<string>;\n  partial: boolean;\n  data: null | Data;\n}\n\ninterface Context {\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  fieldName: string;\n  partial: boolean;\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  schemaPredicates?: SchemaPredicates;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data\n): QueryResult => {\n  initDataState(store.data, 0);\n  const result = read(store, request, data);\n  // clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.getRootKey(operation.operation);\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx: Context = {\n    parentTypeName: rootKey,\n    parentKey: rootKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: normalizeVariables(operation, request.variables),\n    fragments: getFragments(request.query),\n    partial: false,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  let data = input || makeDict();\n  data =\n    rootKey !== ctx.store.getRootKey('query')\n      ? readRoot(ctx, rootKey, rootSelect, data)\n      : readSelection(ctx, rootKey, rootSelect, data);\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: data === undefined ? false : ctx.partial,\n    data: data === undefined ? null : data,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  originalData: Data\n): Data => {\n  if (typeof originalData.__typename !== 'string') {\n    return originalData;\n  }\n\n  const iter = new SelectionIterator(entityKey, entityKey, select, ctx);\n  const data = makeDict();\n  data.__typename = originalData.__typename;\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = originalData[fieldAlias];\n    if (node.selectionSet !== undefined && fieldValue !== null) {\n      const fieldData = ensureData(fieldValue);\n      data[fieldAlias] = readRootField(ctx, getSelectionSet(node), fieldData);\n    } else {\n      data[fieldAlias] = fieldValue;\n    }\n  }\n\n  return data;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: null | Data | NullArray<Data>\n): Data | NullArray<Data> | null => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    for (let i = 0, l = originalData.length; i < l; i++)\n      newData[i] = readRootField(ctx, select, originalData[i]);\n    return newData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    const fieldValue = readSelection(ctx, entityKey, select, makeDict());\n    return fieldValue === undefined ? null : fieldValue;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Data | string,\n  variables?: Variables\n): Data | null => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    warn(\n      'readFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      6\n    );\n\n    return null;\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename) {\n    entity.__typename = typename;\n  }\n\n  const entityKey =\n    typeof entity !== 'string'\n      ? store.keyOfEntity({ __typename: typename, ...entity } as Data)\n      : entity;\n\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx: Context = {\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    variables: variables || {},\n    fragments,\n    partial: false,\n    store,\n    schemaPredicates: store.schemaPredicates,\n  };\n\n  return (\n    readSelection(ctx, entityKey, getSelectionSet(fragment), makeDict()) || null\n  );\n};\n\nconst readSelection = action(\n  (\n    ctx: Context,\n    entityKey: string,\n    select: SelectionSet,\n    data: Data\n  ): Data | undefined => {\n    const { store, schemaPredicates } = ctx;\n    const isQuery = entityKey === store.getRootKey('query');\n\n    // Get the __typename field for a given entity to check that it exists\n    const typename = !isQuery\n      ? InMemoryData.readRecord(entityKey, '__typename')\n      : entityKey;\n    if (typeof typename !== 'string') {\n      return undefined;\n    }\n\n    data.__typename = typename;\n    const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n    let node: FieldNode | void;\n    let hasFields = false;\n    let hasPartials = false;\n    while ((node = iter.next()) !== undefined) {\n      // Derive the needed data from our node.\n      const fieldName = getName(node);\n      const fieldArgs = getFieldArguments(node, ctx.variables);\n      const fieldAlias = getFieldAlias(node);\n      const fieldKey = keyOfField(fieldName, fieldArgs);\n      const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n      const fieldParent = InMemoryData.readParent(entityKey, fieldKey);\n      const key = joinKeys(entityKey, fieldKey);\n      let pleaseDontAssign = false;\n\n      if (\n        process.env.NODE_ENV !== 'production' &&\n        schemaPredicates &&\n        typename\n      ) {\n        schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n      }\n\n      // We temporarily store the data field in here, but undefined\n      // means that the value is missing from the cache\n      let dataFieldValue: void | DataField;\n\n      const resolvers = store.resolvers[typename];\n      if (\n        resolvers !== undefined &&\n        typeof resolvers[fieldName] === 'function'\n      ) {\n        // We have to update the information in context to reflect the info\n        // that the resolver will receive\n        ctx.parentTypeName = typename;\n        ctx.parentKey = entityKey;\n        ctx.parentFieldKey = key;\n        ctx.fieldName = fieldName;\n\n        // We have a resolver for this field.\n        // Prepare the actual fieldValue, so that the resolver can use it\n        if (fieldValue !== undefined) {\n          data[fieldAlias] = fieldValue;\n        }\n\n        dataFieldValue = resolvers[fieldName](\n          data,\n          fieldArgs || makeDict(),\n          store,\n          ctx\n        );\n\n        if (node.selectionSet !== undefined) {\n          // When it has a selection set we are resolving an entity with a\n          // subselection. This can either be a list or an object.\n          dataFieldValue = resolveResolverResult(\n            ctx,\n            typename,\n            fieldName,\n            key,\n            getSelectionSet(node),\n            (data[fieldAlias] as Data) || makeDict(),\n            dataFieldValue\n          );\n        }\n\n        if (\n          schemaPredicates !== undefined &&\n          dataFieldValue === null &&\n          !schemaPredicates.isFieldNullable(typename, fieldName)\n        ) {\n          // Special case for when null is not a valid value for the\n          // current field\n          return undefined;\n        }\n      } else if (node.selectionSet === undefined) {\n        // The field is a scalar and can be retrieved directly\n        dataFieldValue = fieldValue;\n        pleaseDontAssign = true;\n        if (data[fieldAlias] === undefined) {\n          Object.defineProperty(data, fieldAlias, {\n            get: () => {\n              return fieldParent !== null && fieldParent !== undefined\n                ? fieldParent[fieldAlias]\n                : undefined;\n            },\n          });\n        }\n      } else {\n        // We have a selection set which means that we'll be checking for links\n        const link = InMemoryData.readLink(entityKey, fieldKey);\n        if (link !== undefined) {\n          dataFieldValue = resolveLink(\n            ctx,\n            link,\n            typename,\n            fieldName,\n            getSelectionSet(node),\n            data[fieldAlias] as Data\n          );\n\n          if (data[fieldAlias] === undefined) {\n            pleaseDontAssign = true;\n            const localNode = node;\n            const localTypeName = typename;\n            Object.defineProperty(data, fieldAlias, {\n              get: () => {\n                const localLink = InMemoryData.readLink(entityKey, fieldKey);\n                if (!localLink) {\n                  return undefined;\n                }\n\n                const linkedEntity = resolveLink(\n                  ctx,\n                  localLink,\n                  localTypeName,\n                  fieldName,\n                  getSelectionSet(localNode),\n                  undefined\n                );\n                return linkedEntity !== null && linkedEntity !== undefined\n                  ? linkedEntity\n                  : undefined;\n              },\n            });\n          }\n        } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n          // The entity on the field was invalid but can still be recovered\n          dataFieldValue = fieldValue;\n        }\n      }\n\n      // Now that dataFieldValue has been retrieved it'll be set on data\n      // If it's uncached (undefined) but nullable we can continue assembling\n      // a partial query result\n      if (\n        dataFieldValue === undefined &&\n        schemaPredicates !== undefined &&\n        schemaPredicates.isFieldNullable(typename, fieldName)\n      ) {\n        // The field is uncached but we have a schema that says it's nullable\n        // Set the field to null and continue\n        hasPartials = true;\n        data[fieldAlias] = null;\n      } else if (dataFieldValue === undefined) {\n        // The field is uncached and not nullable; return undefined\n        return undefined;\n      } else {\n        // Otherwise continue as usual\n        hasFields = true;\n        if (pleaseDontAssign === false) {\n          data[fieldAlias] = dataFieldValue;\n        }\n      }\n    }\n\n    if (hasPartials) ctx.partial = true;\n    return isQuery && hasPartials && !hasFields ? undefined : data;\n  }\n);\n\nconst readResolverResult = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  data: Data,\n  result: Data\n): Data | undefined => {\n  const { store, schemaPredicates } = ctx;\n  const entityKey = store.keyOfEntity(result) || key;\n  const resolvedTypename = result.__typename;\n  const typename =\n    InMemoryData.readRecord(entityKey, '__typename') || resolvedTypename;\n\n  if (\n    typeof typename !== 'string' ||\n    (resolvedTypename && typename !== resolvedTypename)\n  ) {\n    // TODO: This may be an invalid error for resolvers that return interfaces\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return undefined;\n  }\n\n  // The following closely mirrors readSelection, but differs only slightly for the\n  // sake of resolving from an existing resolver result\n  data.__typename = typename;\n  const iter = new SelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasFields = false;\n  let hasPartials = false;\n  while ((node = iter.next()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(\n      fieldName,\n      getFieldArguments(node, ctx.variables)\n    );\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result[fieldName];\n\n    if (process.env.NODE_ENV !== 'production' && schemaPredicates && typename) {\n      schemaPredicates.isFieldAvailableOnType(typename, fieldName);\n    }\n\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n    if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (node.selectionSet === undefined) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        data[fieldAlias] as Data,\n        resultValue\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          data[fieldAlias] as Data\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (\n      dataFieldValue === undefined &&\n      schemaPredicates !== undefined &&\n      schemaPredicates.isFieldNullable(typename, fieldName)\n    ) {\n      // The field is uncached but we have a schema that says it's nullable\n      // Set the field to null and continue\n      hasPartials = true;\n      data[fieldAlias] = null;\n    } else if (dataFieldValue === undefined) {\n      // The field is uncached and not nullable; return undefined\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = true;\n      data[fieldAlias] = dataFieldValue;\n    }\n  }\n\n  if (hasPartials) ctx.partial = true;\n  return !hasFields ? undefined : data;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | Data | Data[],\n  result: void | DataField\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { schemaPredicates } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const isListNullable =\n      schemaPredicates === undefined ||\n      schemaPredicates.isListNullable(typename, fieldName);\n    const data = new Array(result.length);\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        // Get the inner previous data from prevData\n        prevData !== undefined ? prevData[i] : undefined,\n        result[i]\n      );\n\n      if (childResult === undefined && !isListNullable) {\n        return undefined;\n      } else {\n        data[i] = childResult !== undefined ? childResult : null;\n      }\n    }\n\n    return data;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (isDataOrKey(result)) {\n    const data = prevData === undefined ? makeDict() : prevData;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readResolverResult(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | Data | Data[]\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { schemaPredicates } = ctx;\n    const isListNullable =\n      schemaPredicates !== undefined &&\n      schemaPredicates.isListNullable(typename, fieldName);\n    const newLink = new Array(link.length);\n    for (let i = 0, l = link.length; i < l; i++) {\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData !== undefined ? prevData[i] : undefined\n      );\n      if (childLink === undefined && !isListNullable) {\n        return undefined;\n      } else {\n        newLink[i] = childLink !== undefined ? childLink : null;\n      }\n    }\n\n    return newLink;\n  } else if (link === null) {\n    return null;\n  } else {\n    // console.log({link, prevData})\n    return readSelection(\n      ctx,\n      link,\n      select,\n      prevData === undefined ? makeDict() : prevData\n    );\n  }\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\nimport { makeDict } from '../store';\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  if (node.arguments === undefined || node.arguments.length === 0) {\n    return null;\n  }\n\n  const args = makeDict();\n  let argsSize = 0;\n\n  for (let i = 0, l = node.arguments.length; i < l; i++) {\n    const arg = node.arguments[i];\n    const value = valueFromASTUntyped(arg.value, vars);\n    if (value !== undefined && value !== null) {\n      args[getName(arg)] = value;\n      argsSize++;\n    }\n  }\n\n  return argsSize > 0 ? args : null;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n): Variables => {\n  if (node.variableDefinitions === undefined) {\n    return {};\n  }\n\n  const args: Variables = (input as Variables) || {};\n\n  return node.variableDefinitions.reduce((vars, def) => {\n    const name = getName(def.variable);\n    let value = args[name];\n    if (value === undefined) {\n      if (def.defaultValue !== undefined) {\n        value = valueFromASTUntyped(def.defaultValue, args);\n      } else {\n        return vars;\n      }\n    }\n\n    vars[name] = value;\n    return vars;\n  }, makeDict());\n};\n","import {\n  buildClientSchema,\n  isNullableType,\n  isListType,\n  isNonNullType,\n  GraphQLSchema,\n  GraphQLAbstractType,\n  GraphQLObjectType,\n  GraphQLInterfaceType,\n  GraphQLUnionType,\n} from 'graphql';\n\nimport { invariant, warn } from '../helpers/help';\n\nexport class SchemaPredicates {\n  schema: GraphQLSchema;\n\n  constructor(schema: object) {\n    this.schema = buildClientSchema(schema as any);\n  }\n\n  isFieldNullable(typename: string, fieldName: string): boolean {\n    const field = getField(this.schema, typename, fieldName);\n    if (field === undefined) return false;\n    return isNullableType(field.type);\n  }\n\n  isListNullable(typename: string, fieldName: string): boolean {\n    const field = getField(this.schema, typename, fieldName);\n    if (field === undefined) return false;\n    const ofType = isNonNullType(field.type) ? field.type.ofType : field.type;\n    return isListType(ofType) && isNullableType(ofType.ofType);\n  }\n\n  isFieldAvailableOnType(typename: string, fieldname: string): boolean {\n    return !!getField(this.schema, typename, fieldname);\n  }\n\n  isInterfaceOfType(\n    typeCondition: null | string,\n    typename: string | void\n  ): boolean {\n    if (!typename || !typeCondition) return false;\n    if (typename === typeCondition) return true;\n\n    const abstractType = this.schema.getType(typeCondition);\n    const objectType = this.schema.getType(typename);\n\n    if (abstractType instanceof GraphQLObjectType) {\n      return abstractType === objectType;\n    }\n\n    expectAbstractType(abstractType, typeCondition);\n    expectObjectType(objectType, typename);\n    return this.schema.isPossibleType(abstractType, objectType);\n  }\n}\n\nconst getField = (\n  schema: GraphQLSchema,\n  typename: string,\n  fieldName: string\n) => {\n  const object = schema.getType(typename);\n  expectObjectType(object, typename);\n\n  const field = object.getFields()[fieldName];\n  if (field === undefined) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n\n    return undefined;\n  }\n\n  return field;\n};\n\nfunction expectObjectType(x: any, typename: string): asserts x is GraphQLObjectType {\n  invariant(\n    x instanceof GraphQLObjectType,\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(x: any, typename: string): asserts x is GraphQLAbstractType {\n  invariant(\n    x instanceof GraphQLInterfaceType || x instanceof GraphQLUnionType,\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n","import { IntrospectionQuery } from 'graphql';\n\nimport {\n  Exchange,\n  formatDocument,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from 'urql/core';\n\nimport {\n  filter,\n  map,\n  merge,\n  pipe,\n  share,\n  tap,\n  fromPromise,\n  fromArray,\n  buffer,\n  take,\n  mergeMap,\n  concat,\n  empty,\n  Source,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { SchemaPredicates } from './ast';\nimport { hydrateData } from './store/data';\nimport { makeDict, Store, clearOptimistic } from './store';\n\nimport {\n  UpdatesConfig,\n  ResolverConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  StorageAdapter,\n} from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n};\n\ntype OperationMap = Map<number, Operation>;\n\ninterface DependentOperations {\n  [key: string]: number[];\n}\n\n// Returns the given operation result with added cacheOutcome meta field\nconst addCacheOutcome = (op: Operation, outcome: CacheOutcome): Operation => ({\n  ...op,\n  context: {\n    ...op.context,\n    meta: {\n      ...op.context.meta,\n      cacheOutcome: outcome,\n    },\n  },\n});\n\n// Returns the given operation with added __typename fields on its query\nconst addTypeNames = (op: Operation): Operation => ({\n  ...op,\n  query: formatDocument(op.query),\n});\n\n// Retrieves the requestPolicy from an operation\nconst getRequestPolicy = (op: Operation) => op.context.requestPolicy;\n\n// Returns whether an operation is a query\nconst isQueryOperation = (op: Operation): boolean =>\n  op.operationName === 'query';\n\n// Returns whether an operation is a mutation\nconst isMutationOperation = (op: Operation): boolean =>\n  op.operationName === 'mutation';\n\n// Returns whether an operation can potentially be read from cache\nconst isCacheableQuery = (op: Operation): boolean => {\n  return isQueryOperation(op) && getRequestPolicy(op) !== 'network-only';\n};\n\n// Returns whether an operation potentially triggers an optimistic update\nconst isOptimisticMutation = (op: Operation): boolean => {\n  return isMutationOperation(op) && getRequestPolicy(op) !== 'network-only';\n};\n\n// Copy an operation and change the requestPolicy to skip the cache\nconst toRequestPolicy = (\n  operation: Operation,\n  requestPolicy: RequestPolicy\n): Operation => ({\n  ...operation,\n  context: {\n    ...operation.context,\n    requestPolicy,\n  },\n});\n\nexport interface CacheExchangeOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionQuery;\n  storage?: StorageAdapter;\n}\n\nexport const cacheExchange = (opts?: CacheExchangeOpts): Exchange => ({\n  forward,\n  client,\n}) => {\n  if (!opts) opts = {};\n\n  const store = new Store(\n    opts.schema ? new SchemaPredicates(opts.schema) : undefined,\n    opts.resolvers,\n    opts.updates,\n    opts.optimistic,\n    opts.keys\n  );\n\n  let hydration: void | Promise<void>;\n  if (opts.storage) {\n    const storage = opts.storage;\n    hydration = storage.read().then(entries => {\n      hydrateData(store.data, storage, entries);\n    });\n  }\n\n  const optimisticKeysToDependencies = new Map<number, Set<string>>();\n  const ops: OperationMap = new Map();\n  const deps: DependentOperations = makeDict();\n\n  const collectPendingOperations = (\n    pendingOperations: Set<number>,\n    dependencies: void | Set<string>\n  ) => {\n    if (dependencies !== undefined) {\n      // Collect operations that will be updated due to cache changes\n      dependencies.forEach(dep => {\n        const keys = deps[dep];\n        if (keys !== undefined) {\n          deps[dep] = [];\n          for (let i = 0, l = keys.length; i < l; i++) {\n            pendingOperations.add(keys[i]);\n          }\n        }\n      });\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Set<number>\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    pendingOperations.forEach(key => {\n      if (key !== operation.key) {\n        const op = ops.get(key);\n        if (op !== undefined) {\n          ops.delete(key);\n          client.reexecuteOperation(toRequestPolicy(op, 'cache-first'));\n        }\n      }\n    });\n  };\n\n  // This executes an optimistic update for mutations and registers it if necessary\n  const optimisticUpdate = (operation: Operation) => {\n    if (isOptimisticMutation(operation)) {\n      const { key } = operation;\n      const { dependencies } = writeOptimistic(store, operation, key);\n      if (dependencies.size !== 0) {\n        optimisticKeysToDependencies.set(key, dependencies);\n        const pendingOperations = new Set<number>();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Set<string>) => {\n    dependencies.forEach(dep => {\n      const keys = deps[dep] || (deps[dep] = []);\n      keys.push(op.key);\n\n      if (!ops.has(op.key)) {\n        ops.set(\n          op.key,\n          getRequestPolicy(op) === 'network-only'\n            ? toRequestPolicy(op, 'cache-and-network')\n            : op\n        );\n      }\n    });\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const { data, dependencies, partial } = query(store, operation);\n    let cacheOutcome: CacheOutcome;\n\n    if (data === null) {\n      cacheOutcome = 'miss';\n    } else {\n      updateDependencies(operation, dependencies);\n      cacheOutcome =\n        !partial || getRequestPolicy(operation) === 'cache-only'\n          ? 'hit'\n          : 'partial';\n    }\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (result: OperationResult): OperationResult => {\n    const { operation, error, extensions } = result;\n    const isQuery = isQueryOperation(operation);\n    let { data } = result;\n\n    // Clear old optimistic values from the store\n    const { key } = operation;\n    const pendingOperations = new Set<number>();\n    collectPendingOperations(\n      pendingOperations,\n      optimisticKeysToDependencies.get(key)\n    );\n    optimisticKeysToDependencies.delete(key);\n    clearOptimistic(store.data, key);\n\n    let writeDependencies: Set<string> | void;\n    let queryDependencies: Set<string> | void;\n    if (data !== null && data !== undefined) {\n      writeDependencies = write(store, operation, data).dependencies;\n\n      if (isQuery) {\n        const queryResult = query(store, operation);\n        data = queryResult.data;\n        queryDependencies = queryResult.dependencies;\n      } else {\n        data = query(store, operation, data).data;\n      }\n    }\n\n    // Collect all write dependencies and query dependencies for queries\n    collectPendingOperations(pendingOperations, writeDependencies);\n    if (isQuery) {\n      collectPendingOperations(pendingOperations, queryDependencies);\n    }\n\n    // Execute all pending operations related to changed dependencies\n    executePendingOperations(result.operation, pendingOperations);\n\n    // Update this operation's dependencies if it's a query\n    if (isQuery && queryDependencies !== undefined) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Buffer operations while waiting on hydration to finish\n    // If no hydration takes place we replace this stream with an empty one\n    const bufferedOps$ = hydration\n      ? pipe(\n          sharedOps$,\n          buffer(fromPromise(hydration)),\n          take(1),\n          mergeMap(fromArray)\n        )\n      : (empty as Source<Operation>);\n\n    const inputOps$ = pipe(\n      concat([bufferedOps$, sharedOps$]),\n      map(addTypeNames),\n      tap(optimisticUpdate),\n      share\n    );\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cache$ = pipe(\n      inputOps$,\n      filter(op => isCacheableQuery(op)),\n      map(operationResultFromCache),\n      share\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheOps$ = pipe(\n      cache$,\n      filter(res => res.outcome === 'miss'),\n      map(res => addCacheOutcome(res.operation, res.outcome))\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cache$,\n      filter(res => res.outcome !== 'miss'),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome } = res;\n          const policy = getRequestPolicy(operation);\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            policy === 'cache-and-network' ||\n            (policy === 'cache-first' && outcome === 'partial')\n          ) {\n            result.stale = true;\n            client.reexecuteOperation(\n              toRequestPolicy(operation, 'network-only')\n            );\n          }\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      forward(\n        merge([\n          pipe(\n            inputOps$,\n            filter(op => !isCacheableQuery(op))\n          ),\n          cacheOps$,\n        ])\n      ),\n      map(updateCacheWithResult)\n    );\n\n    return merge([result$, cacheResult$]);\n  };\n};\n","import {\n  DocumentNode,\n  buildClientSchema,\n  visitWithTypeInfo,\n  TypeInfo,\n  FragmentDefinitionNode,\n  GraphQLSchema,\n  IntrospectionQuery,\n  FragmentSpreadNode,\n  NameNode,\n  ASTNode,\n  isCompositeType,\n  isAbstractType,\n  Kind,\n  visit,\n} from 'graphql';\n\nimport { pipe, tap, map } from 'wonka';\nimport { Exchange, Operation } from 'urql/core';\n\nimport { getName, getSelectionSet, unwrapType } from './ast';\nimport { makeDict } from './store';\nimport { invariant, warn } from './helpers/help';\n\ninterface PopulateExchangeOpts {\n  schema: IntrospectionQuery;\n}\n\n/** An exchange for auto-populating mutations with a required response body. */\nexport const populateExchange = ({\n  schema: ogSchema,\n}: PopulateExchangeOpts): Exchange => ({ forward }) => {\n  const schema = buildClientSchema(ogSchema);\n  /** List of operation keys that have already been parsed. */\n  const parsedOperations = new Set<number>();\n  /** List of operation keys that have not been torn down. */\n  const activeOperations = new Set<number>();\n  /** Collection of fragments used by the user. */\n  const userFragments: UserFragmentMap = makeDict();\n  /** Collection of actively in use type fragments. */\n  const activeTypeFragments: TypeFragmentMap = makeDict();\n\n  /** Handle mutation and inject selections + fragments. */\n  const handleIncomingMutation = (op: Operation) => {\n    if (op.operationName !== 'mutation') {\n      return op;\n    }\n\n    const activeSelections: TypeFragmentMap = makeDict();\n    for (const name in activeTypeFragments) {\n      activeSelections[name] = activeTypeFragments[name].filter(s =>\n        activeOperations.has(s.key)\n      );\n    }\n\n    return {\n      ...op,\n      query: addFragmentsToQuery(\n        schema,\n        op.query,\n        activeSelections,\n        userFragments\n      ),\n    };\n  };\n\n  /** Handle query and extract fragments. */\n  const handleIncomingQuery = ({ key, operationName, query }: Operation) => {\n    if (operationName !== 'query') {\n      return;\n    }\n\n    activeOperations.add(key);\n    if (parsedOperations.has(key)) {\n      return;\n    }\n\n    parsedOperations.add(key);\n\n    const [extractedFragments, newFragments] = extractSelectionsFromQuery(\n      schema,\n      query\n    );\n\n    for (let i = 0, l = extractedFragments.length; i < l; i++) {\n      const fragment = extractedFragments[i];\n      userFragments[getName(fragment)] = fragment;\n    }\n\n    for (let i = 0, l = newFragments.length; i < l; i++) {\n      const fragment = newFragments[i];\n      const type = getName(fragment.typeCondition);\n      const current =\n        activeTypeFragments[type] || (activeTypeFragments[type] = []);\n\n      (fragment as any).name.value += current.length;\n      current.push({ key, fragment });\n    }\n  };\n\n  const handleIncomingTeardown = ({ key, operationName }: Operation) => {\n    if (operationName === 'teardown') {\n      activeOperations.delete(key);\n    }\n  };\n\n  return ops$ => {\n    return pipe(\n      ops$,\n      tap(handleIncomingQuery),\n      tap(handleIncomingTeardown),\n      map(handleIncomingMutation),\n      forward\n    );\n  };\n};\n\ntype UserFragmentMap<T extends string = string> = Record<\n  T,\n  FragmentDefinitionNode\n>;\n\ntype TypeFragmentMap<T extends string = string> = Record<T, TypeFragment[]>;\n\ninterface TypeFragment {\n  /** Operation key where selection set is being used. */\n  key: number;\n  /** Selection set. */\n  fragment: FragmentDefinitionNode;\n}\n\n/** Gets typed selection sets and fragments from query */\nexport const extractSelectionsFromQuery = (\n  schema: GraphQLSchema,\n  query: DocumentNode\n) => {\n  const extractedFragments: FragmentDefinitionNode[] = [];\n  const newFragments: FragmentDefinitionNode[] = [];\n  const typeInfo = new TypeInfo(schema);\n\n  visit(\n    query,\n    visitWithTypeInfo(typeInfo, {\n      Field: node => {\n        if (node.selectionSet) {\n          const type = getTypeName(typeInfo);\n          newFragments.push({\n            kind: Kind.FRAGMENT_DEFINITION,\n            typeCondition: {\n              kind: Kind.NAMED_TYPE,\n              name: nameNode(type),\n            },\n            name: nameNode(`${type}_PopulateFragment_`),\n            selectionSet: node.selectionSet,\n          });\n        }\n      },\n      FragmentDefinition: node => {\n        extractedFragments.push(node);\n      },\n    })\n  );\n\n  return [extractedFragments, newFragments];\n};\n\n/** Replaces populate decorator with fragment spreads + fragments. */\nexport const addFragmentsToQuery = (\n  schema: GraphQLSchema,\n  query: DocumentNode,\n  activeTypeFragments: TypeFragmentMap,\n  userFragments: UserFragmentMap\n) => {\n  const typeInfo = new TypeInfo(schema);\n\n  const requiredUserFragments: Record<\n    string,\n    FragmentDefinitionNode\n  > = makeDict();\n\n  const additionalFragments: Record<\n    string,\n    FragmentDefinitionNode\n  > = makeDict();\n\n  /** Fragments provided and used by the current query */\n  const existingFragmentsForQuery: Set<string> = new Set();\n\n  return visit(\n    query,\n    visitWithTypeInfo(typeInfo, {\n      Field: {\n        enter: node => {\n          if (!node.directives) {\n            return;\n          }\n\n          const directives = node.directives.filter(\n            d => getName(d) !== 'populate'\n          );\n          if (directives.length === node.directives.length) {\n            return;\n          }\n\n          const possibleTypes = getTypes(schema, typeInfo);\n          const newSelections = possibleTypes.reduce((p, possibleType) => {\n            const typeFrags = activeTypeFragments[possibleType.name];\n            if (!typeFrags) {\n              return p;\n            }\n\n            for (let i = 0, l = typeFrags.length; i < l; i++) {\n              const { fragment } = typeFrags[i];\n              const fragmentName = getName(fragment);\n              const usedFragments = getUsedFragments(fragment);\n\n              // Add used fragment for insertion at Document node\n              for (let j = 0, l = usedFragments.length; j < l; j++) {\n                const name = usedFragments[j];\n                if (!existingFragmentsForQuery.has(name)) {\n                  requiredUserFragments[name] = userFragments[name];\n                }\n              }\n\n              // Add fragment for insertion at Document node\n              additionalFragments[fragmentName] = fragment;\n\n              p.push({\n                kind: Kind.FRAGMENT_SPREAD,\n                name: nameNode(fragmentName),\n              });\n            }\n\n            return p;\n          }, [] as FragmentSpreadNode[]);\n\n          const existingSelections = getSelectionSet(node);\n\n          const selections =\n            existingSelections.length + newSelections.length !== 0\n              ? [...newSelections, ...existingSelections]\n              : [\n                  {\n                    kind: Kind.FIELD,\n                    name: nameNode('__typename'),\n                  },\n                ];\n\n          return {\n            ...node,\n            directives,\n            selectionSet: {\n              kind: Kind.SELECTION_SET,\n              selections,\n            },\n          };\n        },\n      },\n      Document: {\n        enter: node => {\n          node.definitions.reduce((set, definition) => {\n            if (definition.kind === 'FragmentDefinition') {\n              set.add(definition.name.value);\n            }\n            return set;\n          }, existingFragmentsForQuery);\n        },\n        leave: node => {\n          const definitions = [...node.definitions];\n          for (const key in additionalFragments)\n            definitions.push(additionalFragments[key]);\n          for (const key in requiredUserFragments)\n            definitions.push(requiredUserFragments[key]);\n          return { ...node, definitions };\n        },\n      },\n    })\n  );\n};\n\nconst nameNode = (value: string): NameNode => ({\n  kind: Kind.NAME,\n  value,\n});\n\n/** Get all possible types for node with TypeInfo. */\nconst getTypes = (schema: GraphQLSchema, typeInfo: TypeInfo) => {\n  const type = unwrapType(typeInfo.getType());\n  if (!isCompositeType(type)) {\n    warn(\n      'Invalid type: The type ` + type + ` is used with @populate but does not exist.',\n      17\n    );\n    return [];\n  }\n\n  return isAbstractType(type) ? schema.getPossibleTypes(type) : [type];\n};\n\n/** Get name of non-abstract type for adding to 'activeTypeFragments'. */\nconst getTypeName = (typeInfo: TypeInfo) => {\n  const type = unwrapType(typeInfo.getType());\n  invariant(\n    type && !isAbstractType(type),\n    'Invalid TypeInfo state: Found no flat schema type when one was expected.',\n    18\n  );\n\n  return type.toString();\n};\n\n/** Get fragment names referenced by node. */\nconst getUsedFragments = (node: ASTNode) => {\n  const names: string[] = [];\n\n  visit(node, {\n    FragmentSpread: f => {\n      names.push(getName(f));\n    },\n  });\n\n  return names;\n};\n"],"names":["node","type","unwrapType","const","cache","Set","currentDebugStack","typename","identifier","Kind","invariant","condition","message","code","process","errorMessage","getDebugOutput","error","Error","helpUrl","console","fieldName","args","stringifyVariables","fieldKey","parenIndex","arguments","JSON","defer","Promise","fn","let","currentData","currentDependencies","currentOptimisticKey","optimistic","makeDict","base","Map","keys","data","optimisticKey","window","gc","map","entityKey","value","undefined","keymap","entity","i","l","index","gcBatch","refCount","by","count","newCount","link","updateRCForEntity","Array","fieldInfos","seenFieldKeys","fieldInfoOfKey","extractNodeFields","owner","updateRCForLink","linkNode","updateDependencies","links","prevLinkNode","prevLink","ctx","typeCondition","getTypeCondition","warn","getSelectionSet","keyOfField","getName","getFieldArguments","readRecord","readLink","SelectionIterator","select","this","variables","directives","directive","name","isInclude","arg","valueFromASTUntyped","vars","shouldInclude","fragmentNode","isFragmentHeuristicallyMatching","x","store","request","initDataState","startWrite","operation","getMainOperation","result","dependencies","getCurrentDependencies","operationName","parentTypeName","parentKey","parentFieldKey","normalizeVariables","fragments","getFragments","schemaPredicates","pushDebugNode","writeSelection","writeRoot","mutationRootKey","iter","resolver","ensureData","resolverValue","updater","fieldArgs","clearDataState","query","names","Object","fragment","_extends","__typename","writeData","InMemoryData","fieldValue","getFieldAlias","key","advice","expected","fieldData","writeField","newData","isRootField","writeRootField","Store","resolvers","updates","optimisticMutations","queryType","mutationType","schema","subscriptionType","queryName","mutationName","subscriptionName","queryRootKey","persistenceScheduled","persistenceBatch","gcScheduled","refLock","makeNodeMap","records","storage","id","_id","field","fieldSelect","childLink","invalidateSelection","createRequest","input","output","dataFragment","readSelection","partial","argsSize","def","SchemaPredicates","buildClientSchema","getField","isNullableType","isNonNullType","ofType","fieldname","abstractType","objectType","object","expectObjectType","doc","isFragmentNode","rootKey","rootSelect","readRoot","originalData","fieldAlias","readRootField","action","isQuery","hasFields","hasPartials","fieldParent","pleaseDontAssign","dataFieldValue","prevData","childResult","resolveResolverResult","isListNullable","readResolverResult","resolvedTypename","resultValue","resolveLink","get","localNode","localTypeName","localLink","linkedEntity","newLink","op","outcome","context","meta","cacheOutcome","formatDocument","requestPolicy","res","isCacheableQuery","d","set","definition","activeTypeFragments","userFragments","p","possibleType","typeFrags","fragmentName","usedFragments","getUsedFragments","j","requiredUserFragments","additionalFragments","kind","nameNode","typeInfo","TypeInfo","existingFragmentsForQuery","visitWithTypeInfo","Field","enter","isAbstractType","possibleTypes","existingSelections","newSelections","selectionSet","selections","Document","leave","definitions","FragmentSpread","f","opts","ref","collectPendingOperations","optimisticKeysToDependencies","clearOptimisticNodes","write","Pa","writeDependencies","queryDependencies","executePendingOperations","pendingOperations","dep","deps","ops","toRequestPolicy","writeOptimistic","client","policy","addCacheOutcome","extensions","hydration","entries","dotIndex","writeLink","writeRecord","ops$","sharedOps$","share","bufferedOps$","mergeMap","fromArray","take","buffer","fromPromise","empty","tap","optimisticUpdate","addTypeNames","concat","cache$","operationResultFromCache","filter","inputOps$","updateCacheWithResult","forward","merge","cacheOps$","result$","cacheResult$","activeOperations","parsedOperations","extractedFragments","newFragments","FragmentDefinition","extractSelectionsFromQuery","c","current","activeSelections","addFragmentsToQuery","s","ogSchema","handleIncomingMutation","handleIncomingTeardown","handleIncomingQuery"],"mappings":";;;;uFAuBEA,eAA2BA,uEAMOA,2EAMlB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;YAWhBC;wBAEmBA,KACVC,GAAWD,YAGbA,KAAQ;;;ACxCjBE,IAAMC,KAAQ,IAAIC,KAELC,IAA8B;;WAEbC,GAAyBP;MACjDQ,IAAa;aACCC,uBAChBD,IAAaD,6BACcA,UACvB,oBACKP,WAAcS,4BAEvBD,KADaR,eAAgBA,qBAAqB,mBAC1BA,cACfA,WAAcS,6BACvBD,IAAa,MAAIR;OAIjBM,OAAuBE;;;;oBAMrB,mBAAmBF,OAAuB,QAAQ,MAClD;;;AAENI,WACEC,GACAC,GACAC;OAEKF;cACgBC,KAAW,oBAAoBC,IAAO,MAC5B,iBAAzBC,yBACFC,KAAgBC;KAGZC,IAAYC,MAAMH,IAvC1BI,2FAuCmDN,WACpC;IACPI;;;;WAIWL,GAAiBC;EAC/BT,OAAUQ,OACbQ,aAAaR,IAAUI,OA/CzBG,2FA+CsDN;EACpDT,OAAUQ;;;WCxDaS,GAAmBC;aAClCD,UAAaE,mBAAmBD,WAAWD;;;YAExBG;MACvBC,IAAaD,UAAiB;cAChCC,IACK;cACLD;IACAH,WAAWG,QAAe,GAAGC;IAC7BC,WAAWC,WAAWH,QAAeC,IAAa;MAG7C;cACLD;IACAH,WAAWG;IACXE,WAAW;;;;AClBVvB,IAAMyB,KACc,iBAAzBd,wBAA4D,gCACxDe,4BAA4BA,8BAC5BC;oBAAiBA,GAAI;;;;oBCiCmB;;;AAE9CC,IAAIC,IAAmC,MACnCC,IAA0C,MAC1CC,IAAsC;;;SAEA;IACxCC,YAAYC;IACZC,MAAM,IAAIC;IACVC,MAAM;;;;WAKNC,GACAC;EAGAC,qBAAqBV,IAAcQ;MACb,IAAInC;MACHoC;mBACnB3B,yBACFR,WAA2B;;;;MAMvBkC,IAAOR;GAERQ,iBAAwC,IAApBA,mBACvBA,iBAAmB,GACnBZ;IACEe,GAAGH;;gBAIcA,2BACnBA,0BAA4B,GAC5BZ;IACEY,gBAAoBA;8BACQ;yBACJJ;;MAK5BH,IADAD,IAAc;mBAGVlB,yBACFR,WAA2B;;;;EAM7BI,EAC0B,SAAxBuB,2CACA,0KAGA;;;;YAqBFW,GACAC,GACArB,GACAsB;EAKIZ,UAG2Ca,MAAzCH,aAAeV,OACjBU,aAAeV,KAAwB,IAAII,KAC3CM,eAAiBV;EAGnBc,IAASJ,aAAeV,MAExBc,IAASJ;MAIPK,IAASD,MAAWH;aACpBI,KACFD,MAAWH,GAAYI,IAASb;aAM9BU,KAAwBZ,IAG1Be,EAAOzB,KAAYsB,WAFZG,EAAOzB;;;YAQhBoB,GACAC,GACArB;OACe,IAEN0B,IAAI,GAAGC,IAAIP,eAAiBM,IAAIC,GAAGD,KAAK;QAEzClD,IADa4C,aAAeA,OAASM,QACfL;aAEfE,MAAT/C,KAAsBwB;eACZA;;;qBAKVxB,IAAO4C,WAAaC,MACE7C,EAAKwB,UAAYuB;;;YAyBdH,GAAiBH;MAE1CW,IAAQR,eAAiBH;OAC3BW,aAEKR,aAAeH,IACtBG,cAAgBQ,GAAO;;;YAMzBC,GACAC,GACAT,GACAU;MAGMC,SAAgCT,MAAxBO,EAAST,KAA2BS,EAAST,KAAa;MAEtDS,EAAST,KAAcW,IAAQD,IAAM;aAGnDF,MACc,KAAZI,IAAeJ,MAAYR,KACb,KAATW,KAAyB,IAAXC,KAAcJ,SAAeR;;;YAMtDQ,GACAC,GACAI,GACAH;MAEoB;IAClBI,GAAkBN,GAASC,GAAUI,GAAMH;aAClCK,cAAcF;SAAO,IACrBR,IAAI,GAAGC,IAAIO,UAAaR,IAAIC,GAAGD,KAAK;UACrCL,IAAYa,EAAKR;WAErBS,GAAkBN,GAASC,GAAUT,GAAWU;;;;;YAQtDM,GACAC,GACA9D;WAEa+C,MAAT/C;SACGG,IAAMqB;MACJsC,MAAkBtC,OAGrBqC,OAAgBE,GAAevC,KAC/BsC,MAAkBtC;;;;;YAQxBqC,GACAC,GACAjB,GACAD;EAGAoB,GAAkBH,GAAYC,GAAelB,WAAaC;OAF1D,IAKSK,IAAI,GAAGC,IAAIP,eAAiBM,IAAIC,GAAGD;IAE1Cc,GAAkBH,GAAYC,GADXlB,aAAeA,OAASM,QACiBL;;;;YAK7CL;EAEjBA,iBAAmB;8BAIEK;QAGT,MADCL,WAAcK,MAAc,IAC1B;WAEN1C,IAAMsC,gBAA+B;YAClCa,IAAWd,UAAaC;YAIlB,KAHEa,EAAST,MAAc;;;eAI9BS,EAAST;;aAMXL,WAAcK;uBACDA;eAOAE,WADAP,mBAAsBK,QAExCL,sBAAyBK,IACrBL;aACGrC,IAAMqB;UAETgB,mBFnSmDyB,OEkSfpB,UAAWrB,UAClBuB;;;eAQlBA,WADAP,iBAAoBK,KACT;QAC1BL,oBAAuBK;aAClB1C,IAAMqB;UAELgB,cAEFA,mBFjTmDyB,OEgTfpB,UAAWrB,UAClBuB,IAG/BmB,GAAgB1B,WAAcA,YAAe2B,EAAS3C;;;;uBAItCqB;;;;;WAKEA,GAAmBrB;EAC5B,iBAAbA,MACEqB,MAAcb,iBAChBC,MAAyBY,UACHE,MAAbvB,KACTS,MAAkCY,UAAWrB;;;WAOjDqB,GACArB;EAEA4C,EAAmBvB,GAAWrB;YACfQ,WAAsBa,GAAWrB;;;WAahDqB,GACArB;EAEA4C,EAAmBvB,GAAWrB;YACfQ,SAAoBa,GAAWrB;;;WAK9CqB,GACArB,GACAsB;EAEAsB,EAAmBvB,GAAWrB;KACtBQ,WAAsBa,GAAWrB,GAAUsB;gBACtBZ,MAE3BF,mBF3W2DiC,OE0WvBpB,UAAWrB,KACVsB;;;YAUvCD,GACArB,GACAkC;MAEMlB,IAAOR;MAOTE,GAAsB;IAGxBoB,IAAAA,IACEd,UAAaN,OACZM,UAAaN,KAAwBE;QACxCiC,IAAQ7B,mBAAsBN;SACzB;IACDM,cAEFA,mBF1YyDyB,OEyYrBpB,UAAWrB,KAClBkC;QAEpBlB;QACHA;QACRa,IAAUb;;WAKsBO,OAD5BuB,SAAyBvB,MAAVsB,IAAsBA,MAAUxB,UAAaE,KACpBuB,EAAa9C,KAAY;IAGpDqB,GAAWrB;KAEtBgB,SAAYK,GAAWrB,GAAUkC;KAEzBL,GAASC,GAAUiB;KAEnBlB,GAASC,GAAUI,GAAM;;;YCrZzC1D,GACAO,GACAsC,GACA2B;OAEKjE;YAAiB;;MAChBkE,IAAgBC,GAAiB1E;MACnCO,MAAakE;YAAsB;;2CAEvCE,EACE,6EACEpE,IACA,wCAEAkE,IACA,6CACAA,IACA,iJAGF;UAGMG,EAAgB5E,kBAAWA;QAChBA,WLnBLS;cKmBmB;;QACdoE,EACfC,EAAQ9E,IACR+E,EAAkB/E,GAAMwE;kBD8UQzB,MAApCiC,EC5UmBnC,GAAWrB,WD6UIuB,MAAlCkC,EC7UmBpC,GAAWrB;;;;AAW9B0D,WACE3E,GACAsC,GACAsC,GACAX;kBAEgBjE;mBACCsC;iBACF2B;oBACG,EAAC;wBACG,EAACW;;;;QAIW,MAA3BC,0BAA8B;QAC7BhC,IAAQgC,gBAAgBA,yBAAyB,MACjDD,IAASC,oBAAoBA,6BAA6B;QAC5DhC,KAAS+B;;WAIN;MACCnF,IAAOmF,EAAO/B;SCnDjB;QDoDsBiC,IAAAA;gBAANrF;iBClDN+C,MAAfuC;eAFK,IAOApC,IAAI,GAAGC,IAAImC,UAAmBpC,IAAIC,GAAGD,KAAK;gBAC3CqC,IAAYD,EAAWpC,IACvBsC,IAAOV,EAAQS,IAGfE,IAAqB,cAATD;iBACbC,KAAsB,WAATD,OAGZE,IAAMH,cAAsBA,YAAoB,KAAK,SAC9B,SAAjBT,EAAQY,OAGC,qBADf5C,IAAQ6C,oBAAoBD,SAAWE,OACD,SAAV9C;kBAI3B2C,MAAc3C,KAASA;;;;;aArBvB;;UDiDE+C;QAEE,IAAiB7F,WL5DhBS;mBKkEesC,OAJf+C,IAAiC9F,WL1DFS,uBK2DjC2E,uBAAuBN,EAAQ9E,MAC/BA,OAG2B,iBAAzBc,0BACYsE,eAAeU;eAIK/C,MAAlCqC,gCACIA,gDACEV,GAAiBoB,IACjBV,iBAEFW,GACED,GACAV,eACAA,gBACAA;iCAIe,6BACIR,EAAgBkB;;mBAKlB,iBAAlBhB,EAAQ9E;;;;;;;;WAYAgG;oBACzBA,IAAkB,OAAQA;;;YEtF1BC,GACAC,GACA1D;EAEA2D,EAAcF,QAAY;MACXG,GAAWH,GAAOC,GAAS1D;;;;;YAM1CyD,GACAC,GACA1D;MAEM6D,IAAYC,GAAiBJ,UAC7BK,IAAsB;IAAEC,cAAcC;KAEtCtB,IAASP,EAAgByB,IACzBK,IAAgBT,aAAiBI;MAElB;IACnBM,gBAAgBD;IAChBE,WAAWF;IACXG,gBAAgB;IAChBxF,WAAW;IACXgE,WAAWyB,GAAmBT,GAAWH;IACzCa,WAAWC,EAAad;YACxBK;WACAN;IACAgB,kBAAkBhB;;mBAGhBnF,wBACFoG,EAAcR,GAAeL;QAGT7B,mBAAqB,WACzC2C,GAAe3C,GAAKkC,GAAevB,GAAQ3C,KAE3C4E,GAAU5C,GAAKkC,GAAevB,GAAQ3C;;;;YAOxCyD,GACAC,GACAzD;EAEA0D,EAAcF,QAAYxD;MAEpB4D,IAAYC,GAAiBJ;MACP;IAAEM,cAAcC;;MAEtCY,IAAkBpB,aAAiB,aACnCS,IAAgBT,aAAiBI;IAErCK,MAAkBW,2CAClB,oIAEA;mBAGEvG,wBACFoG,EAAcR,GAAeL;MAGV;IACnBM,gBAAgBU;IAChBT,WAAWS;IACXR,gBAAgB;IAChBxF,WAAW;IACXgE,WAAWyB,GAAmBT,GAAWH;IACzCa,WAAWC,EAAad;YACxBK;WACAN;IACAgB,kBAAkBhB;IAClB9D,aAAY;;MAGDC;MACA,IAAI8C,EACfwB,GACAA,GACA9B,EAAgByB,IAChB7B;WAGExE,QAC4B+C,OAAxB/C,IAAOsH;aACavE,MAAtB/C,gBAAiC;UAC7BqB,IAAYyD,EAAQ9E,IACpBuH,IAAW/C,4BAA8BnD;eAE9B0B,MAAbwE,GAAwB;QAE1B/C,cAAgBnD;WAKDmD,GADMgD,MADCD,OADJxC,EAAkB/E,GAAMwE,iBACEpC,KAAYoC,SAAWA,KAEjCI,EAAgB5E;UAC7CqB,KAAaoG;wBACFjD,gBAAkB6C,GAAiBhG,OAEjDqG,EAAQlF,GAAMmF,KAAavF,KAAYoC,SAAWA;;;;EAM1DoD;;;;YAKA3B,GACA4B,GACArF,GACA6C;EAEM0B,IAAYC,EAAaa;MACzBC,IAAQC,YAAYhB;WAEThE,WADAgE,EAAUe,EAAM;mDAExBnD,EACL,mIAEA;;MAIEpE,IAA+ByH;MACnBC;IAAEC,YAAY3H;KAAaiC;MACvCK,IAAYoD,cAAkBkC;OAC/BtF;mDACI8B,EACL,sIAEEpE,IACA,MACF;;mBAIAO,wBACFoG,EAAc3G,GAAUyH;SAGL;IACnBrB,gBAAgBpG;IAChBqG,WAAW/D;IACXgE,gBAAgB;IAChBxF,WAAW;IACXgE,WAAWA,KAAa;eACxB0B;IACAR,QAAQ;MAAEC,cAAcC;;WACxBR;IACAgB,kBAAkBhB;KAGApD,GAAW+B,EAAgBoD,IAAWG;;;YAI1D3D,GACA3B,GACAsC,GACA3C;MAGMjC,IADUsC,MAAc2B,mBAAqB,WACxB3B,IAAYL;MACf;IAExB4F,EAAyBvF,GAAW,cAActC;QAErC,IAAI2E,EAAkB3E,GAAUsC,GAAWsC,GAAQX;aAE5DxE,QAC4B+C,OAAxB/C,IAAOsH,aAA4B;UACnCjG,IAAYyD,EAAQ9E,IACpB2H,IAAY5C,EAAkB/E,GAAMwE;UACzBK,EAAWxD,GAAWsG;UACjCU,IAAa7F,EAAK8F,EAActI,KAChCuI,IAAe1F,UAAWrB;UAEH,iBAAzBV;iBACiBiC,MAAfsF,GAA0B;UACtBG,IAAShE,eACX,qDACA;mBAGoBzB,MAAtB/C,iBACI,kCACA;mDAEN2E,EACE,sCACEnD,IACA,uDACAiH,IACA,qBACAD,GACF;;;gCAI+BjI,KACjCiE,0CAA4CjE,GAAUc;;;iBAItDrB,iBAEFoI,EAAyBvF,GAAWrB,GAAU6G,MAGxCK,IAAYlB,EAAWa,IAE7BD,GAAuBvF,GAAWrB,GAD5BkC,IAAOiF,GAAWnE,GAAK+D,GAAK3D,EAAgB5E,IAAO0I;;;;;YAO7DlE,GACAqC,GACA1B,GACA3C;MAEIoB,cAAcpB,IAAO;aACjBoG,IAAchF,MAAMpB,WACjBU,IAAI,GAAGC,IAAIX,UAAaU,IAAIC,GAAGD,KAAK;UAKrCmB,IAAQsE,GAAWnE,GAFCqC,UAAmB3D,GAELiC,GAJ3B3C,EAAKU;QAMVA,KAAKmB;;;;EAIV,IAAa,SAAT7B;;;MAKe,cADRgC,oBAAsBhC,MACPK,IAAYgE;MAC5BrE;aAGfgC,aAAehC,iBACD,SAAdK,KACoB,wBACnBtC,WAAkB,iBAClBA,WAAkB,WACN,eAAbA,8CAEAoE,EACE,qDACEkC,IACA,6LAIAtG,IACA,mIAGAA,IACA,+BACF;KAIWiE,GAAK+D,GAAKpD,GAAQ3C;;;;YAMjCgC,GACAjE,GACA4E,GACA3C;MAEMqG,IACJtI,MAAaiE,mBAAqB,eAClCjE,MAAaiE,mBAAqB;MAEvB,IAAIU,EAAkB3E,GAAUA,GAAU4E,GAAQX;WAE3DxE,QAC4B+C,OAAxB/C,IAAOsH,aAA4B;QACnCjG,IAAYyD,EAAQ9E,IACpB2H,IAAY5C,EAAkB/E,GAAMwE;QACNK,IAAAA,EAAWxD,GAAWsG;QAAhCpH,UL7UZgI;aK8UYxF,MAAtB/C,gBAAiC;SAEpBwE,GADIgD,EAAWhF,EAAK8F,EAActI,MACjB4E,EAAgB5E;;IAG9C6I,MAEFrE,mBAAqBjE,GACrBiE,cAAgBjE,GAChBiE,mBAAqBhD,GACrBgD,cAAgBnD;SAKA0B,OADV2E,IAAUlD,gBAAkBjE,GAAUc,OAE1CqG,EAAQlF,GAAMmF,KAAavF,KAAYoC,SAAWA;;;;YAQxDA,GACAhC,GACA2C;MAEIvB,cAAcpB,IAAO;aACjBoG,IAAchF,MAAMpB,WACjBU,IAAI,GAAGC,IAAIX,UAAaU,IAAIC,GAAGD;MACtC0F,EAAQ1F,KAAK4F,GAAetE,GAAKhC,EAAKU,IAAIiC;;;;EAE1B,SAAT3C,MAMO,UADZK,IAAY2B,oBAAsBhC,MAEtC2E,GAAe3C,GAAK3B,GAAWsC,GAAQ3C,KAGvC4E,GAAU5C,GADOhC,cACQ2C,GAAQ3C;;;AC5WnCuG,WACE9B,GACA+B,GACAC,GACAC,GACA3G;;sBAoDY;;OAEI6C;qBACG;;oBAGRP;mBAxDMmE,KAAa;6BACHE,KAAuB;cACtC3G,KAAQ;0BACI0E;iBAET;cACFgC,KAAWA,cAAqB;kBAC5BA,KAAWA,kBAAyB;;OAK7CE,mCACAC,IAAeC,qBACfC,IAAmBD;oBAQP;WANZE,IAAYJ,IAAYA,SAAiB;cACzCK,IAAeJ,IAAeA,SAAoB;kBAClDK,IAAmBH,IACrBA,SACA;4BAQa,IACdC,KAAY,WACZC,KAAe,cACfC,KAAmB;2BAGJ;WACT;cACG;kBACI;sBAGC;WACR;cACG;kBACI;;0BJgBDC;WAAwC;MAC3DC,uBAAsB;MACtBC,kBAAkBxH;MAClByH,cAAa;oBACbH;MACArG,SAAS,IAAIhD;MACbiD,UAAUlB;MACV0H,SAAS1H;MACTiC,OAAO0F;MACPC,SAASD;MACTE,SAAS;;GItBK7B,CAAkBhD,gBAAgB;;;kCAWrCI;yBACcA;;;mCAGbhD;;OAELjC;;;WAEmCwC,MAA7BqC,eAAe7E;;;MAItBgI;YACUhI,SACN6E,UAAU7E,GAAUiC,KACVO,QAAPmH,IACT3B,IAAM,KAAG2B,IACQnH,QAARoH,MACT5B,IAAM,KAAG4B;aAGK5J,UAAYgI,IAAQ;;;yCAGpBtF,GAA8BzB;MAK5B,UAJZqB,IACO,SAAXI,KAAqC,uBACjCmC,iBAAiBnC,KACjBA;;;MAEAoF,IAAaD,EAAwBvF,GAAWrB;oBAClD6G,IAAiCA,KAC/B3E,IAAO0E,EAAsBvF,GAAWrB,MAChCkC,IAAO;;;+BAIrBT,GACAmH,GACA9I;gCAE8B2B,GAAQ4B,EAAWuF,GAAO9I;;;uCAG1CuG,GAA8BxC;eCjG9Cb,GACA3B,GACAsC;QAE8B,YAAdtC,GAGF;MACZtC,IAAAA,IAAW6H,EAAwBvF,GAAW;UACtB;;;QAGGA,GAAW,mBAAcE;;UAGzCF;;QAGA,IAAIqC,EAAkB3E,GAAUsC,GAAWsC,GAAQX;aAE5DxE,QAC4B+C,OAAxB/C,IAAOsH,aAA4B;UACnCjG,IAAYyD,EAAQ9E,IACpBwB,IAAWqD,EACfxD,GACA0D,EAAkB/E,GAAMwE;uBAIxB1D,wBACA0D,sBACAjE,KAEAiE,0CAA4CjE,GAAUc;eAG9B0B,MAAtB/C;QACFoI,EAAyBvF,GAAWrB,QAAUuB;iBAExCsH,IAAczF,EAAgB5E,IAC9B0D,IAAO0E,EAAsBvF,GAAWrB,IAE9C4G,GAAuBvF,GAAWrB,QAAUuB,IAC5CqF,EAAyBvF,GAAWrB,QAAUuB,IAE1Ca,cAAcF,IAAO;QACdR,IAAI;aAARnB,IAAWoB,IAAIO,UAAaR,IAAIC,GAAGD,KAAK;cACrCoH,IAAY5G,EAAKR;mBACnBoH,KACFC,GAAoB/F,GAAK8F,GAAWD;;;aAIxCE,GAAoB/F,GAAKd,GAAM2G;;;QApEhB;IACnBhF,WAAWyB,OAHKR,IDmHCkE,IAAAA,cAAc3C,GAAOxC,YChHGa;IACzCa,WAAWC,EAAad;WD+GbD;IC7GXgB,kBD6GWhB;KCxGXzB,mBAAqB,UACrBI,EAAgByB;;;qCD0GJpD;MAKS,UAJfJ,IACO,SAAXI,KAAqC,uBACjCmC,iBAAiBnC,KACjBA,IACCJ;oCJ0SHgB,IAA0B,IAC1BC,IAA6B,IAAIzD;MI3ST+H;OJgTTvE,GAAYC,GIhTHsE,GJgT6B/D;OACtCR,GAAYC,GIjTHsE,GJiT6B4B;QACpDnG;;QIlT+D;;;;;mCAIpE4G,GACA/C;EAEMxB,IAAUsE,cAAcC,SAAaA;gBAC5B/C,EAAQtC,eAAec,WAEzBd,MAAMc,GAASwE;;;iCAIpBD;YACIrF,MAAMoF,cAAcC,SAAaA;;;oCAI7CE,GACA1H,GACAoC;EETI0B,IAAYC,EFWU2D;MEVtB7C,IAAQC,YAAYhB;WAEThE,WADAgE,EAAUe,EAAM;6CAE/BnD,EACE,kIAEA;QAGK;;QAGHpE,IAA+ByH;4BFFK/E,iBAAAA,eEIpB1C;KAGhBsC,IACc,uBFREoD,iBESEgC;MAAEC,YAAY3H;OFTI0C,MAAAA,MEwBb,iBAAzBnC,wBACFoG,EAAc3G,GAAUyH,QAgBxB4C,EAbmBpG;MACnBmC,gBAAgBpG;MAChBqG,WAAW/D;MACXgE,gBAAgB;MAChBxF,WAAW;MACXgE,WFjCgDA,KEiCxB;iBACxB0B;MACA8D,UAAS;aFnCW5E;MEqCpBgB,kBFrCoBhB;OEyCDpD,GAAW+B,EAAgBoD,IAAW5F,QAAe,kDA5BxEuC,EACE,gIAEEpE,IACA,MACF;QAGK;;;;;qCFjBPoK,GACAnI,GACA6C;KAEcD,MAAMuF,GAAcnI,GAAM6C;;;WG9K1CrF,GACA4F;WAEuB7C,MAAnB/C,eAA0D,MAA1BA;;;WAI9BsB,IAAOc,KACT0I,IAAW,GAEN5H,IAAI,GAAGC,IAAInD,oBAAuBkD,IAAIC,GAAGD,KAAK;QAC/CwC,IAAM1F,YAAekD,IACrBJ,IAAQ6C,oBAAoBD,SAAWE;YACzC9C,MACFxB,EAAKwD,EAAQY,MAAQ5C,GACrBgI;;aAIGA,IAAexJ,IAAO;;;YAK7BtB,GACAyK;WAEiC1H,MAA7B/C;WACK;;MAGHsB,IAAmBmJ,KAAuB;gDAER7E,GAAMmF;QACtCvF,IAAOV,EAAQiG,aACjBjI,IAAQxB,EAAKkE;aACHzC,MAAVD;eACuBC,MAArBgI;QACFjI,IAAQ6C,oBAAoBoF,gBAAkBzJ;;;;;MAM7CkE,KAAQ1C;;MAEZV;;;ACzCH4I,WAAY3B;gBACI4B,kBAAkB5B;;;uCAGlB9I,GAAkBc;qBAC1B+I,IAAQc,GAAS9F,aAAa7E,GAAUc,OACd,IACzB8J,eAAef;;;sCAGT7J,GAAkBc;WAEjB0B,OADRqH,IAAQc,GAAS9F,aAAa7E,GAAUc;YACd;;MACjB+J,cAAchB,UAAcA,gBAAoBA;oBAC7CiB,MAAWF,eAAeE;;;8CAGvB9K,GAAkB+K;WAC9BJ,GAAS9F,aAAa7E,GAAU+K;;;yCAIzC7G,GACAlE;OAEKA,MAAakE;YAAsB;;MACpClE,MAAakE;YAAsB;;MAEjC8G,IAAenG,oBAAoBX,IACnC+G,IAAapG,oBAAoB7E;MAEnCgL;iBACsBC;;IAGPD,qCAAAA,uEA+CnB,sCA/CiC9G,IAiD/B,2IAEF;KAlDiB+G,GAAYjL;oCACKgL,GAAcC;;;YAKlDnC,GACA9I,GACAc;KAEMoK,IAASpC,UAAe9I,IACLA;WAGXwC,WADA0I,cAAmBpK;6CAE/BsD,EACE,+BACEtD,IACA,0BACAd,IACA,2HAGF;;;;;;AASNmL,YAA0B1F,GAAQzF;EAChCG,EACEsF,wEACA,oCACEzF,IACA,6FAEF;;;YN9EoBP;oBACRS;;;YAOZT;oBAAsBS;;;YAHxBkL;OAEMtF,IAAYsF,iEAMhB,wIAEA;;;;YAQ6C/I,GAAgB5C;EAC7D4C,EAAIkC,EAAQ9E,MAASA;;;;WAFI2L;8BACJC,eAGpB;;;YIkBH3F,GACAC,GACA1D;EAEA2D,EAAcF,QAAY;YACNA,GAAOC,GAAS1D;;;YAMpCyD,GACAC,GACAuE;MAEMpE,IAAYC,GAAiBJ,UAC7B2F,IAAU5F,aAAiBI,cAC3ByF,IAAalH,EAAgByB;MAEd;IACnBM,gBAAgBkF;IAChBjF,WAAWiF;IACXhF,gBAAgB;IAChBxF,WAAW;IACXgE,WAAWyB,GAAmBT,GAAWH;IACzCa,WAAWC,EAAad;IACxB2E,UAAS;WACT5E;IACAgB,kBAAkBhB;;mBAGhBnF,wBACFoG,EAAc2E,GAASxF;MAGdoE,KAASrI;MAElByJ,MAAYrH,mBAAqB,WAC7BuH,GAASvH,GAAKqH,GAASC,GAAYtJ,KACnCoI,EAAcpG,GAAKqH,GAASC,GAAYtJ;SAEvC;IACLgE,cAAcC;IACdoE,cAAkB9H,MAATP,KAAqB,IAAQgC;IACtChC,WAAeO,MAATP,IAAqB,OAAOA;;;;YAKpCgC,GACA3B,GACAsC,GACA6G;MAEuC;;;MAI1B,IAAI9G,EAAkBrC,GAAWA,GAAWsC,GAAQX;OACpDpC,kBACK4J;WAEdhM,QAC4B+C,OAAxB/C,IAAOsH,aAA4B;QACnC2E,IAAa3D,EAActI,IAC3BqI,IAAa2D,EAAaC;eAC5BjM,kBAAkD,SAAfqI,KAC/BK,IAAYlB,EAAWa,IAC7B7F,EAAKyJ,KAAcC,GAAc1H,GAAKI,EAAgB5E,IAAO0I,MAE7DlG,EAAKyJ,KAAc5D;;;;;YAQvB7D,GACAW,GACA6G;MAEIpI,cAAcoI,IAAe;aACzBpD,IAAchF,MAAMoI,WACjB9I,IAAI,GAAGC,IAAI6I,UAAqB9I,IAAIC,GAAGD;MAC9C0F,EAAQ1F,KAAKgJ,GAAc1H,GAAKW,GAAQ6G,EAAa9I;;;;EAElD,IAAqB,SAAjB8I;;;uBAKOxH,oBAAsBwH,WAKhBjJ,OADhBsF,IAAauC,EAAcpG,GAAK3B,GAAWsC,GAAQ/C,QACvB,OAAOiG,IAElC0D,GAASvH,GAAKwH,cAAyB7G,GAAQ6G;;;AAkE1D7L,IAAMyK,IAAgBuB,iBAElB3H,GACA3B,GACAsC,GACA3C;2CAGM4J,IAAUvJ,MAAcoD,aAAiB,UAGzC1F,IAAY6L,IAEdvJ,IADAuF,EAAwBvF,GAAW;MAEf;IAIxBL,eAAkBjC;QACL,IAAI2E,EAAkB3E,GAAUsC,GAAWsC,GAAQX;aAE5DxE,GACAqM,KAAY,GACZC,KAAc;UAGVjL,IAAYyD,EAAQ9E,IACpB2H,IAAY5C,EAAkB/E,GAAMwE,cACpCyH,IAAa3D,EAActI,IAC3BwB,IAAWqD,EAAWxD,GAAWsG,IACjCU,IAAaD,EAAwBvF,GAAWrB,IAChD+K,iBN8GV1J,GACArB;QAEA4C,EAAmBvB,GAAWrB;WA1La;eA2LtBQ,IAAAA,IAAAA,WAzLZkB,IAAI,GAAGC,IAAIP,eAAiBM,IAAIC,GAAGD,KAAK;gBAEzClD,IADa4C,aAAeA,OAASM,QAwLFL;qBArL5BE,MAAT/C,KAqLgDwB,QArLR;kBACnCxB;;;;mBAMK+C,OADV/C,IAAO4C,WA+K8BC,MA9Kf7C,SAAO+C;;;OM4DXqF,CAAwBvF,GAAWrB,IACjD+G,KAAe1F,UAAWrB,GAC5BgL,MAAmB;uBAGrB1L,wBACAmG,KACA1G,KAEA0G,yBAAwC1G,GAAUc;UAKhDoL,YAEEzD,KAAY/C,YAAgB1F;eAElBwC,MAAdiG,MACgC,wBAAf3H;YAIjBmD,mBAAqBjE,GACrBiE,cAAgB3B,GAChB2B,mBAAqB+D,IACrB/D,cAAgBnD;aAIG0B,MAAfsF,MACF7F,EAAKyJ,KAAc5D,IAGrBoE,IAAiBzD,GAAU3H,GACzBmB,GACAmF,KAAavF,KACb6D,GACAzB,SAGwBzB,MAAtB/C,mBAGFyM,gBAiORjI,GACAjE,GACAc,GACAkH,GACApD,GACAuH,GACAnG;cAEI3C,cAAc2C,IAAS;;qBAKFxD,MAArBkE,KACAA,iBAAgC1G,GAAUc;qBACtCmB,IAAWoB,MAAM2C,WACdrD,IAAI,GAAGC,IAAIoD,UAAerD,IAAIC,GAAGD,KAAK;kBAEvCyJ,IAAcC,GAClBpI,GACAjE,GACAc,GACSkH,UAAQrF,GACjBiC,QAEapC,MAAb2J,IAAyBA,EAASxJ,UAAKH,GACvCwD,EAAOrD;uBAGWH,MAAhB4J,KAA8BE;gBAGhCrK,EAAKU,UAAqBH,MAAhB4J,IAA4BA,IAAc;;;;;;;UAKnD,IAAe,QAAXpG;;;cAiEE,wBACC,wBAA6C,iCAhE3B;YACxB/D,SAAoBO,MAAb2J,IAAyBtK,MAAasK;gBAC1B;cACrB9B,IAAAA,EAAAA,GAAAA,GAAAA,GAAAA;;yBACAkC,oBA1JAjK,IA0JAiK,oBAAAA,MAAAA,GAzJAC,IAyJAD;cAxJAvM,IACJ6H,EAAwBvF,GAAW,iBAAiBkK,GAGhC,wBACnBA,KAAoBxM,MAAawM;yDAGlCpI,EACE,6CACE9B,IACA,+EAEF;yBAGKE;;gBAwIH+J,eAnIYvM;oBACL,IAAI2E,EAAkB3E,GAAUsC,GAkIvCiK,GAAAA;qBA9HFR,IADAD,KAAY,QAEgBtJ,OAAxB/C,IAAOsH,aAA4B;kBAEnCjG,IAAYyD,EAAQ9E;sBACPsI,EAActI;sBAC3BwB,IAAWqD,EACfxD,GACA0D,EAAkB/E,GAuHhB8M,eArHEvE,IAAe1F,UAAWrB,GAC1B6G,IAAaD,EAAwBvF,GAAWrB,IAChDwL,IAmHFF,EAnHuBzL;mCAEvBP,wBAAyCmG,KAAoB1G,KAC/D0G,yBAAwC1G,GAAUc;sBAKhDoL;6BACAO,UAAmDjK,MAAtB/C,iBAE/ByM,IAAiBO,SACcjK,MAAtB/C,iBAETyM,IAAiBpE,SACQtF,MAAhBiK,IAETP,IAAiBG,GAkGfE,GAhGAvM,GACAc,GACAkH,GACA3D,EAAgB5E,IA6FhB8M,EA5FKb,IACLe,UAMWjK,OAFPW,IAAO0E,EAAsBvF,GAAWrB,MAG5CiL,IAAiBQ,GAoFjBH,GAlFEpJ,GACAnD,GACAc,GACAuD,EAAgB5E,IA+ElB8M,EA9EOb,MAEwB,wBAA2B,SAAf5D,MAE3CoE,IAAiBpE;2BAQAtF,MAAnB0J,UACqB1J,MAArBkE,KACAA,kBAAiC1G,GAAUc;oBAI3CiL,KAAc,GA4DZQ,EA3DGb,KAAc;kCACSlJ,MAAnB0J,GAA8B;6BAEhC1J;;;yBAGK,GAqDV+J,EApDGb,KAAcQ;;;gBAInBH,MAgDEQ,aAhDyB;oBACvBT,IA+CFS,SA/Cc/J;;;;;mDAiDlB4B,EACE,2CACE4D,IACA,uGAEF;SAnRqBqE,CACfpI,GACAjE,GACAc,GACAkH,IACA3D,EAAgB5E,IACfwC,EAAKyJ,MAAwB7J,KAC9BqK,UAKmB1J,MAArBkE,KACmB,SAAnBwF,MACCxF,kBAAiC1G,GAAUc;;oBAIrC0B;;;sBAEsBA,MAAtB/C;QAETyM,IAAiBpE,GACjBmE,MAAmB,QACMzJ,MAArBP,EAAKyJ,MACPlE,sBAAsBvF,GAAMyJ,GAAY;UACtCiB;2BACSX,KACHA,GAAYN,UACZlJ;;;sBAOGA,OADPW,IAAO0E,EAAsBvF,GAAWrB;YAE5CiL,IAAiBQ,GACfzI,GACAd,GACAnD,GACAc,GACAuD,EAAgB5E,IAChBwC,EAAKyJ,UAGkBlJ,MAArBP,EAAKyJ,IAA2B;UAClCO,MAAmB;cACbW,KAAYnN,GACZoN,KAAgB7M;gCACAiC,GAAMyJ,GAAY;YACtCiB;kBACQG,IAAYjF,EAAsBvF,GAAWrB;kBAC9C6L;uBAYmB,aARHJ,GACnBzI,GACA6I,GACAD,IACA/L,GACAuD,EAAgBuI,UAChBpK,MAGEuK,SACAvK;;;;;;QAIqB,wBAA2B,SAAfsF,MAE3CoE,IAAiBpE;;eAQAtF,MAAnB0J,UACqB1J,MAArBkE,KACAA,kBAAiC1G,GAAUc;QAI3CiL,KAAc,GACd9J,EAAKyJ,KAAc;aACd;QAAA,SAAuBlJ,MAAnB0J;;oBAEF1J;;;aAGK;eACRyJ,OACFhK,EAAKyJ,KAAcQ;;YAnJO1J,OAAxB/C,IAAOsH;;;;;;IAwJXgF,MAAa9H,aAAc;gBACb8H,MAAgBD,SAAYtJ,IAAYP;;;;YAqL5DgC,GACAd,GACAnD,GACAc,GACA8D,GACAuH;MAEI9I,cAAcF,IAAO;;aAGAX,MAArBkE,KACAA,iBAAgC1G,GAAUc;aACtCkM,IAAc3J,MAAMF,WACjBR,IAAI,GAAGC,IAAIO,UAAaR,IAAIC,GAAGD,KAAK;UACrCoH,IAAY2C,GAChBzI,GACAd,EAAKR,IACL3C,GACAc,GACA8D,QACapC,MAAb2J,IAAyBA,EAASxJ,UAAKH;eAEvBA,MAAduH,KAA4BuC;QAG9BU,EAAQrK,UAAmBH,MAAduH,IAA0BA,IAAY;;;;;;;EAKlD,gBAAI5G,IACF,OAGAkH,EACLpG,GACAd,GACAyB,QACapC,MAAb2J,IAAyBtK,MAAasK;;;YGvjBnBc,GAAeC;iBACnCD;IACHE,iBACKF;MACHG,cACKH;QACHI,cAAcH;;;;;;YAMED;iBACjBA;IACH3F,OAAOgG,eAAeL;;;;YAeEA;SAPH,YAQGA,mBAAgC,mBAARA;;;YAUhDnH,GACAyH;iBAEGzH;IACHqH,iBACKrH;qBACHyH;;;;;YAwMSN;YAAuBA;;;YAS1BO;YAAuBA,aAAeA;;;YADnCA;SAAuB,WAAhBA;;;YAQPA;SAAuB,WAAhBA;;;YAkCDP;UAAOQ,GAAiBR;;;YCtJ/BS;SAAoB,eAAfnJ,EAAQmJ;;;YA8DUC,GAAKC;EACJ,yBAApBA,UACFD,MAAQC;;;;YA9FpB9E,GACAxB,GACAuG,GACAC;aAkCoDC,GAAGC;UACvCC,IAAYJ,EAAoBG;;;SADqB,IAMlDrL,IAAI,GAAGC,IAAIqL,UAAkBtL,IAAIC,GAAGD,KAAK;mBAC3BsL,EAAUtL,aACzBuL,IAAe3J,EAAQkD,IACvB0G,IAAgBC,GAAiB3G,IAG9B4G,IAAI,GAAGzL,IAAIuL,UAAsBE,IAAIzL,GAAGyL,KAAK;YAC9CpJ,IAAOkJ,EAAcE;cACQpJ,OACjCqJ,EAAsBrJ,KAAQ6I,EAAc7I;;MAKhDsJ,EAAoBL,KAAgBzG;aAE7B;QACL+G,MAAMtO;QACN+E,MAAMwJ,GAASP;;;;;MAxDvBQ,IAAW,IAAIC,SAAS7F,IAExBwF,IAGFzM,KAEE0M,IAGF1M,KAGE+M,IAAyC,IAAI9O;eAGjDwH,GACAuH,kBAAkBH,GAAU;IAC1BI,OAAO;MACLC,gBAAOtP;YACAA;cAICsF,IAAatF;cAGfsF,aAAsBtF;YAuF5BC,IAAAA,IAAOC,GAnFkC+O;4BAoF1BhP,SAQdsP,eAAetP,KA5FiBoJ,mBA4FepJ,KAAQ,EAACA,gDAP7D0E,EACE,kFACA;gBAEK;gBAxFqB6K,YA6BnB;gBAEGC,IAAqB7K,EAAgB5E;gBAGY,MAArDyP,WAA4BC,WACxBA,SAAsBD,KACtB,EACE;cACEV,MAAMtO;cACN+E,MAAMwJ,GAAS;;2BAKpBhP;0BACHsF;cACAqK,cAAc;gBACZZ,MAAMtO;4BACNmP;;;;;;;IAKRC,UAAU;MACRP,gBAAOtP;QACLA,yBAKGmP;;MAELW,gBAAO9P;YAEMuI,GADLwH,IAAc,UAAI/P;aACbuI;UACTwH,OAAiBjB,EAAoBvG;;aAClCpI,IAAMoI;UACTwH,OAAiBlB,EAAsBtG;;uBAC7BvI;uBAAM+P;;;;;;;YAOVjN;SAA6B;IAC7CiM,MAAMtO;WACNqC;;;;YA8BwB9C;MAClB8H,IAAkB;QAElB9H,GAAM;IACVgQ,yBAAgBC;MACdnI,OAAWhD,EAAQmL;;;;;;;;;ED9MKC;IAAwCC;MAqHrC5J,oEAEvB6F;QAMNgE,GAAAA,EAAAA;MAEEC,EAEFA;MAAoC9H;;MACRA,KT6M9B+H;MS7M8B/H,KT8M9B+H;MS9M8B/H;QAIP/F,WACC+N,GAAAA,GAAAA,GAAwB/N;;UAGT6D,KAD/B+F,IAASoE,GACS3I,GAAAA,IACpBrF;;;cAGAA,GAAOqF,GAAAA,GAAAA,GAAwBrF;;;QARMW,GAazCiN;MAA4CK,UAE1CL;MAA4CM,EAI9CC,EAAAA;MAA2CC,qBAG5BF,IACbtM;MAAqCsM;;;;;;;IAGLrK;MAlElCA,QAEwCwB,GAAM5B,GAAOI;;eAGjD7D,IACFoL,IAAe,UAEfxJ,EAAmBiC,GAAWG,IAC9BoH,IACG/C,KAA2C,iBAAfxE,0BAEzB,YADA;aAID;QACLoH,SAASG;mBACTvH;cACA7D;;;eArCwBgL,GAAehH;MACzCA,oBAAqBqK;SACNC,EAAKD,OAASC,EAAKD,KAAO,UAC7BrD;cAEGA,UACXuD,MACEvD,OACyB,mBAARA,0BACbwD,GAAgBxD,GAAI,uBACpBA;;;eAxBcnH;UA9FL,eA+FMA,mBAtFgC,mBAsFhCA,yBAAY;2BAEV4K,GAAgBhL,GAAOI,GAAWkC;cACvD/B,WACF6J,MAAiC9H,GAAK/B,IAEtC4J,EADMQ,IAAoB,IAAIvQ,KACcmG,IAC5CmK,EAAyBtK,GAAWuK;;;eAxBxCvK,GACAuK;MAGAA,oBAA0BrI;YACpBA,MAAQlC,OAAe;cACnBmH,IAAKuD,MAAQxI;qBACfiF,MACFuD,SAAWxI,IACX2I,qBAA0BF,GAAgBxD,GAAI;;;;eA3BpDoD,GACApK;WAEqBzD,MAAjByD,KAEFA,sBAAqBqK;YACbtO,IAAOuO,EAAKD;iBACL9N,MAATR,GAAoB;UACtBuO,EAAKD,KAAO;cACC;eAAR9O,IAAWoB,IAAIZ,UAAaW,IAAIC,GAAGD;YACtC0N,MAAsBrO,EAAKW;;;;;eAwK9B6K;0CAEOoD,IAA0B9K;UACA;QAC9BA,WAAW+K,GAAgB/K,GAAWoH;QACtCjL,MAAMuL;QACN9M,OAAO8M;QACPsD,YAAYtD;;UAID,wBAAXoD,KACY,kBAAXA,KAAwC,cAAZ1D;QAE7BlH,WAAe,GACf2K,qBACEF,GAAgB3K,GAAW;;;;;UAzN5B6J,IAAO;QAEZjK,IAAQ,IAAI8C,EAChBmH,WAAc,IAAIlF,EAAiBkF,iBAAenN,GAClDmN,aACAA,WACAA,cACAA;QAIEA,WAAc;UACVjG,IAAUiG;UAChBoB,IAAYrH,0BAAoBsH;QAClBtL,IAAAA,IAAAA,QAAYgE,IAAAA;UToVdzH,GAAM;aACfrC,IAAMoI,QAAgB;cACnBiJ,IAAWjJ,UAAY,MACvB1F,IAAY0F,QAAU,GAAGiJ;cACdjJ,QAAUiJ,IAAW;kBAC9BjJ,aAAe;gBAChB;YACHkJ,GAAU5O,GAAWrB,GS3VU+P,ET2VQhJ;;;gBAEpC;YACHmJ,EAAY7O,GAAWrB,GS9VQ+P,ET8VUhJ;;;QAI/CX;oBACeqC;;;QS/VToG,IAA+B,IAAI/N,KACnCyO,IAAoB,IAAIzO,KACxBwO,IAA4B1O;oBA2I3BuP;MACCC,IAAwBC,MAANF;UAIlBG,IAAeR,IAKfS,SAASC,UAATD,CADAE,KAAK,EAALA,CADAC,OAAOC,YAAYb,GAAnBY,CADAN,OAKDQ;UAMHP,MADAQ,IAAIC,EAAJD,CADAzP,IAAI2P,GAAJ3P,CADA4P,OAAO,EAACV,GAAcF;UAOlBa,IAIJZ,MADAjP,IAAI8P,EAAJ9P,CADA+P,UAAAA,CADAC;UAUAhQ,OAAAA,CADA+P,UAAAA,CADAF;UAUA7P,MAAAA,CADA+P,UAAAA,CADAF;UAwCA7P,IAAIiQ,EAAJjQ,CATAkQ,EACEC,MAAM,EAGFJ,UAAAA,CADAC,IAGFI;mBAMO,EAACC,GAASC;;;;;;;;;;ECvUM/C;;IAEMA;;;IAuET5H;MAnCC4H;qDAK3BgD,EACIC,SAAAA;;QAIiB7K,gBAwDvBc,GACAxB;cAEMwL,IAA+C,IAC/CC,IAAyC,IACzCrE,IAAW,IAAIC,SAAS7F;gBAG5BxB,GACAuH,kBAAkBH,GAAU;YAC1BI,gBAAOrP;kBACDA,gBAAmB;gBA6JvBC,IAAAA,IAAOC,GA5JoB+O;kBA8J/BhP,MAASsP,eAAetP,4CACxB,iFACA;oBAGKA;uBAlKmB;kBAChB8O,MAAMtO;kBACNgE,eAAe;oBACbsK,MAAMtO;oBACN+E,MAAMwJ,GAAS/O;;kBAEjBuF,MAAMwJ,GAAY/O;kBAClB0P,cAAc3P;;;;YAIpBuT,6BAAoBvT;cAClBqT,OAAwBrT;;;iBAKvB,EAACqT,GAAoBC;UApFiBE,GAAAA;QAEzC3L;;4BAdmE4L,sBAiBfvQ;;UAChBA,EACpCmL,EAAAA,MAAcvJ;;QAAduJ,IAFyD;QAKlDnL;UACuBA,IAM9BwQ,GAP8CxQ,MAO9CwQ,IALa5O,oBAKb4O,EAAAA,OAAAA,OAHoDzT;YAGpDyT;;;;;;;IAAAA;MArD4BlG;;;MAErBA,IAIEhI;WAAAA;QACTmO,EAAiBnO,KAAQ4I,EAAoB5I;;qBAM1CgI;QACH3F,OAAO+L,GACLvK,GACAmE,SACAmG,GACAtF;;;eAXwDwF;mBACnCA;;uBAnBrBxK,IAAS4B,kBAAkB6I,IAE3BV,IAAmB,IAAI/S,KAEvB8S,IAAmB,IAAI9S,KAEvBgO,IAAiCjM,KAEjCgM,IAAuChM;oBAkEtCuP;eAKH/O,IAAImR,EAAJnR,CADAyP,IAAI2B,EAAJ3B,CADAA,IAAI4B,EAAJ5B,CADAV;;;;;;;;;;;;;;;"}